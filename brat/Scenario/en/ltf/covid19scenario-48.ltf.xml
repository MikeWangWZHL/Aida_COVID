<?xml version="1.0" encoding="utf-8"?>
<LCTL_TEXT>
	<DOC id="covid19scenario-48">
		<TEXT>
			<SEG id="covid19scenario-48-0" start_char="0" end_char="192">
				<ORIGINAL_TEXT>The Centers for Disease Control and Prevention (CDC) announced that COVID-19, a disease caused by the coronavirus, had been found in toilet paper, and that people should use wet cloths instead.</ORIGINAL_TEXT>
				<TOKEN id="token-0-0" start_char="0" end_char="2">The</TOKEN>
				<TOKEN id="token-0-1" start_char="4" end_char="10">Centers</TOKEN>
				<TOKEN id="token-0-2" start_char="12" end_char="14">for</TOKEN>
				<TOKEN id="token-0-3" start_char="16" end_char="22">Disease</TOKEN>
				<TOKEN id="token-0-4" start_char="24" end_char="30">Control</TOKEN>
				<TOKEN id="token-0-5" start_char="32" end_char="34">and</TOKEN>
				<TOKEN id="token-0-6" start_char="36" end_char="45">Prevention</TOKEN>
				<TOKEN id="token-0-7" start_char="47" end_char="47">(</TOKEN>
				<TOKEN id="token-0-8" start_char="48" end_char="50">CDC</TOKEN>
				<TOKEN id="token-0-9" start_char="51" end_char="51">)</TOKEN>
				<TOKEN id="token-0-10" start_char="53" end_char="61">announced</TOKEN>
				<TOKEN id="token-0-11" start_char="63" end_char="66">that</TOKEN>
				<TOKEN id="token-0-12" start_char="68" end_char="72">COVID</TOKEN>
				<TOKEN id="token-0-13" start_char="73" end_char="73">-</TOKEN>
				<TOKEN id="token-0-14" start_char="74" end_char="75">19</TOKEN>
				<TOKEN id="token-0-15" start_char="76" end_char="76">,</TOKEN>
				<TOKEN id="token-0-16" start_char="78" end_char="78">a</TOKEN>
				<TOKEN id="token-0-17" start_char="80" end_char="86">disease</TOKEN>
				<TOKEN id="token-0-18" start_char="88" end_char="93">caused</TOKEN>
				<TOKEN id="token-0-19" start_char="95" end_char="96">by</TOKEN>
				<TOKEN id="token-0-20" start_char="98" end_char="100">the</TOKEN>
				<TOKEN id="token-0-21" start_char="102" end_char="112">coronavirus</TOKEN>
				<TOKEN id="token-0-22" start_char="113" end_char="113">,</TOKEN>
				<TOKEN id="token-0-23" start_char="115" end_char="117">had</TOKEN>
				<TOKEN id="token-0-24" start_char="119" end_char="122">been</TOKEN>
				<TOKEN id="token-0-25" start_char="124" end_char="128">found</TOKEN>
				<TOKEN id="token-0-26" start_char="130" end_char="131">in</TOKEN>
				<TOKEN id="token-0-27" start_char="133" end_char="138">toilet</TOKEN>
				<TOKEN id="token-0-28" start_char="140" end_char="144">paper</TOKEN>
				<TOKEN id="token-0-29" start_char="145" end_char="145">,</TOKEN>
				<TOKEN id="token-0-30" start_char="147" end_char="149">and</TOKEN>
				<TOKEN id="token-0-31" start_char="151" end_char="154">that</TOKEN>
				<TOKEN id="token-0-32" start_char="156" end_char="161">people</TOKEN>
				<TOKEN id="token-0-33" start_char="163" end_char="168">should</TOKEN>
				<TOKEN id="token-0-34" start_char="170" end_char="172">use</TOKEN>
				<TOKEN id="token-0-35" start_char="174" end_char="176">wet</TOKEN>
				<TOKEN id="token-0-36" start_char="178" end_char="183">cloths</TOKEN>
				<TOKEN id="token-0-37" start_char="185" end_char="191">instead</TOKEN>
				<TOKEN id="token-0-38" start_char="192" end_char="192">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-1" start_char="194" end_char="498">
				<ORIGINAL_TEXT>On March 10, 2020, Now8News published an article positing that the Centers for Disease Control and Prevention (CDC) announced that COVID-19, a disease caused by the new coronavirus, had been found in packages of toilet paper, and that people should start using a wet washcloth to clean themselves instead:</ORIGINAL_TEXT>
				<TOKEN id="token-1-0" start_char="194" end_char="195">On</TOKEN>
				<TOKEN id="token-1-1" start_char="197" end_char="201">March</TOKEN>
				<TOKEN id="token-1-2" start_char="203" end_char="204">10</TOKEN>
				<TOKEN id="token-1-3" start_char="205" end_char="205">,</TOKEN>
				<TOKEN id="token-1-4" start_char="207" end_char="210">2020</TOKEN>
				<TOKEN id="token-1-5" start_char="211" end_char="211">,</TOKEN>
				<TOKEN id="token-1-6" start_char="213" end_char="220">Now8News</TOKEN>
				<TOKEN id="token-1-7" start_char="222" end_char="230">published</TOKEN>
				<TOKEN id="token-1-8" start_char="232" end_char="233">an</TOKEN>
				<TOKEN id="token-1-9" start_char="235" end_char="241">article</TOKEN>
				<TOKEN id="token-1-10" start_char="243" end_char="250">positing</TOKEN>
				<TOKEN id="token-1-11" start_char="252" end_char="255">that</TOKEN>
				<TOKEN id="token-1-12" start_char="257" end_char="259">the</TOKEN>
				<TOKEN id="token-1-13" start_char="261" end_char="267">Centers</TOKEN>
				<TOKEN id="token-1-14" start_char="269" end_char="271">for</TOKEN>
				<TOKEN id="token-1-15" start_char="273" end_char="279">Disease</TOKEN>
				<TOKEN id="token-1-16" start_char="281" end_char="287">Control</TOKEN>
				<TOKEN id="token-1-17" start_char="289" end_char="291">and</TOKEN>
				<TOKEN id="token-1-18" start_char="293" end_char="302">Prevention</TOKEN>
				<TOKEN id="token-1-19" start_char="304" end_char="304">(</TOKEN>
				<TOKEN id="token-1-20" start_char="305" end_char="307">CDC</TOKEN>
				<TOKEN id="token-1-21" start_char="308" end_char="308">)</TOKEN>
				<TOKEN id="token-1-22" start_char="310" end_char="318">announced</TOKEN>
				<TOKEN id="token-1-23" start_char="320" end_char="323">that</TOKEN>
				<TOKEN id="token-1-24" start_char="325" end_char="329">COVID</TOKEN>
				<TOKEN id="token-1-25" start_char="330" end_char="330">-</TOKEN>
				<TOKEN id="token-1-26" start_char="331" end_char="332">19</TOKEN>
				<TOKEN id="token-1-27" start_char="333" end_char="333">,</TOKEN>
				<TOKEN id="token-1-28" start_char="335" end_char="335">a</TOKEN>
				<TOKEN id="token-1-29" start_char="337" end_char="343">disease</TOKEN>
				<TOKEN id="token-1-30" start_char="345" end_char="350">caused</TOKEN>
				<TOKEN id="token-1-31" start_char="352" end_char="353">by</TOKEN>
				<TOKEN id="token-1-32" start_char="355" end_char="357">the</TOKEN>
				<TOKEN id="token-1-33" start_char="359" end_char="361">new</TOKEN>
				<TOKEN id="token-1-34" start_char="363" end_char="373">coronavirus</TOKEN>
				<TOKEN id="token-1-35" start_char="374" end_char="374">,</TOKEN>
				<TOKEN id="token-1-36" start_char="376" end_char="378">had</TOKEN>
				<TOKEN id="token-1-37" start_char="380" end_char="383">been</TOKEN>
				<TOKEN id="token-1-38" start_char="385" end_char="389">found</TOKEN>
				<TOKEN id="token-1-39" start_char="391" end_char="392">in</TOKEN>
				<TOKEN id="token-1-40" start_char="394" end_char="401">packages</TOKEN>
				<TOKEN id="token-1-41" start_char="403" end_char="404">of</TOKEN>
				<TOKEN id="token-1-42" start_char="406" end_char="411">toilet</TOKEN>
				<TOKEN id="token-1-43" start_char="413" end_char="417">paper</TOKEN>
				<TOKEN id="token-1-44" start_char="418" end_char="418">,</TOKEN>
				<TOKEN id="token-1-45" start_char="420" end_char="422">and</TOKEN>
				<TOKEN id="token-1-46" start_char="424" end_char="427">that</TOKEN>
				<TOKEN id="token-1-47" start_char="429" end_char="434">people</TOKEN>
				<TOKEN id="token-1-48" start_char="436" end_char="441">should</TOKEN>
				<TOKEN id="token-1-49" start_char="443" end_char="447">start</TOKEN>
				<TOKEN id="token-1-50" start_char="449" end_char="453">using</TOKEN>
				<TOKEN id="token-1-51" start_char="455" end_char="455">a</TOKEN>
				<TOKEN id="token-1-52" start_char="457" end_char="459">wet</TOKEN>
				<TOKEN id="token-1-53" start_char="461" end_char="469">washcloth</TOKEN>
				<TOKEN id="token-1-54" start_char="471" end_char="472">to</TOKEN>
				<TOKEN id="token-1-55" start_char="474" end_char="478">clean</TOKEN>
				<TOKEN id="token-1-56" start_char="480" end_char="489">themselves</TOKEN>
				<TOKEN id="token-1-57" start_char="491" end_char="497">instead</TOKEN>
				<TOKEN id="token-1-58" start_char="498" end_char="498">:</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-2" start_char="500" end_char="618">
				<ORIGINAL_TEXT>COVID-19 Found in Toilet Paper […] The coronavirus has been found in the one place people never expected, toilet paper.</ORIGINAL_TEXT>
				<TOKEN id="token-2-0" start_char="500" end_char="504">COVID</TOKEN>
				<TOKEN id="token-2-1" start_char="505" end_char="505">-</TOKEN>
				<TOKEN id="token-2-2" start_char="506" end_char="507">19</TOKEN>
				<TOKEN id="token-2-3" start_char="509" end_char="513">Found</TOKEN>
				<TOKEN id="token-2-4" start_char="515" end_char="516">in</TOKEN>
				<TOKEN id="token-2-5" start_char="518" end_char="523">Toilet</TOKEN>
				<TOKEN id="token-2-6" start_char="525" end_char="529">Paper</TOKEN>
				<TOKEN id="token-2-7" start_char="531" end_char="533">[…]</TOKEN>
				<TOKEN id="token-2-8" start_char="535" end_char="537">The</TOKEN>
				<TOKEN id="token-2-9" start_char="539" end_char="549">coronavirus</TOKEN>
				<TOKEN id="token-2-10" start_char="551" end_char="553">has</TOKEN>
				<TOKEN id="token-2-11" start_char="555" end_char="558">been</TOKEN>
				<TOKEN id="token-2-12" start_char="560" end_char="564">found</TOKEN>
				<TOKEN id="token-2-13" start_char="566" end_char="567">in</TOKEN>
				<TOKEN id="token-2-14" start_char="569" end_char="571">the</TOKEN>
				<TOKEN id="token-2-15" start_char="573" end_char="575">one</TOKEN>
				<TOKEN id="token-2-16" start_char="577" end_char="581">place</TOKEN>
				<TOKEN id="token-2-17" start_char="583" end_char="588">people</TOKEN>
				<TOKEN id="token-2-18" start_char="590" end_char="594">never</TOKEN>
				<TOKEN id="token-2-19" start_char="596" end_char="603">expected</TOKEN>
				<TOKEN id="token-2-20" start_char="604" end_char="604">,</TOKEN>
				<TOKEN id="token-2-21" start_char="606" end_char="611">toilet</TOKEN>
				<TOKEN id="token-2-22" start_char="613" end_char="617">paper</TOKEN>
				<TOKEN id="token-2-23" start_char="618" end_char="618">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-3" start_char="620" end_char="736">
				<ORIGINAL_TEXT>An estimated 6000 more people have contracted the virus in the United States tied to a contamination of toilet paper.</ORIGINAL_TEXT>
				<TOKEN id="token-3-0" start_char="620" end_char="621">An</TOKEN>
				<TOKEN id="token-3-1" start_char="623" end_char="631">estimated</TOKEN>
				<TOKEN id="token-3-2" start_char="633" end_char="636">6000</TOKEN>
				<TOKEN id="token-3-3" start_char="638" end_char="641">more</TOKEN>
				<TOKEN id="token-3-4" start_char="643" end_char="648">people</TOKEN>
				<TOKEN id="token-3-5" start_char="650" end_char="653">have</TOKEN>
				<TOKEN id="token-3-6" start_char="655" end_char="664">contracted</TOKEN>
				<TOKEN id="token-3-7" start_char="666" end_char="668">the</TOKEN>
				<TOKEN id="token-3-8" start_char="670" end_char="674">virus</TOKEN>
				<TOKEN id="token-3-9" start_char="676" end_char="677">in</TOKEN>
				<TOKEN id="token-3-10" start_char="679" end_char="681">the</TOKEN>
				<TOKEN id="token-3-11" start_char="683" end_char="688">United</TOKEN>
				<TOKEN id="token-3-12" start_char="690" end_char="695">States</TOKEN>
				<TOKEN id="token-3-13" start_char="697" end_char="700">tied</TOKEN>
				<TOKEN id="token-3-14" start_char="702" end_char="703">to</TOKEN>
				<TOKEN id="token-3-15" start_char="705" end_char="705">a</TOKEN>
				<TOKEN id="token-3-16" start_char="707" end_char="719">contamination</TOKEN>
				<TOKEN id="token-3-17" start_char="721" end_char="722">of</TOKEN>
				<TOKEN id="token-3-18" start_char="724" end_char="729">toilet</TOKEN>
				<TOKEN id="token-3-19" start_char="731" end_char="735">paper</TOKEN>
				<TOKEN id="token-3-20" start_char="736" end_char="736">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-4" start_char="738" end_char="924">
				<ORIGINAL_TEXT>Testing by Washington State Health department confirmed the COVID-19 virus particles were present in the samples they took from five separate packages of toilet paper from Big Box stores.</ORIGINAL_TEXT>
				<TOKEN id="token-4-0" start_char="738" end_char="744">Testing</TOKEN>
				<TOKEN id="token-4-1" start_char="746" end_char="747">by</TOKEN>
				<TOKEN id="token-4-2" start_char="749" end_char="758">Washington</TOKEN>
				<TOKEN id="token-4-3" start_char="760" end_char="764">State</TOKEN>
				<TOKEN id="token-4-4" start_char="766" end_char="771">Health</TOKEN>
				<TOKEN id="token-4-5" start_char="773" end_char="782">department</TOKEN>
				<TOKEN id="token-4-6" start_char="784" end_char="792">confirmed</TOKEN>
				<TOKEN id="token-4-7" start_char="794" end_char="796">the</TOKEN>
				<TOKEN id="token-4-8" start_char="798" end_char="802">COVID</TOKEN>
				<TOKEN id="token-4-9" start_char="803" end_char="803">-</TOKEN>
				<TOKEN id="token-4-10" start_char="804" end_char="805">19</TOKEN>
				<TOKEN id="token-4-11" start_char="807" end_char="811">virus</TOKEN>
				<TOKEN id="token-4-12" start_char="813" end_char="821">particles</TOKEN>
				<TOKEN id="token-4-13" start_char="823" end_char="826">were</TOKEN>
				<TOKEN id="token-4-14" start_char="828" end_char="834">present</TOKEN>
				<TOKEN id="token-4-15" start_char="836" end_char="837">in</TOKEN>
				<TOKEN id="token-4-16" start_char="839" end_char="841">the</TOKEN>
				<TOKEN id="token-4-17" start_char="843" end_char="849">samples</TOKEN>
				<TOKEN id="token-4-18" start_char="851" end_char="854">they</TOKEN>
				<TOKEN id="token-4-19" start_char="856" end_char="859">took</TOKEN>
				<TOKEN id="token-4-20" start_char="861" end_char="864">from</TOKEN>
				<TOKEN id="token-4-21" start_char="866" end_char="869">five</TOKEN>
				<TOKEN id="token-4-22" start_char="871" end_char="878">separate</TOKEN>
				<TOKEN id="token-4-23" start_char="880" end_char="887">packages</TOKEN>
				<TOKEN id="token-4-24" start_char="889" end_char="890">of</TOKEN>
				<TOKEN id="token-4-25" start_char="892" end_char="897">toilet</TOKEN>
				<TOKEN id="token-4-26" start_char="899" end_char="903">paper</TOKEN>
				<TOKEN id="token-4-27" start_char="905" end_char="908">from</TOKEN>
				<TOKEN id="token-4-28" start_char="910" end_char="912">Big</TOKEN>
				<TOKEN id="token-4-29" start_char="914" end_char="916">Box</TOKEN>
				<TOKEN id="token-4-30" start_char="918" end_char="923">stores</TOKEN>
				<TOKEN id="token-4-31" start_char="924" end_char="924">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-5" start_char="926" end_char="1125">
				<ORIGINAL_TEXT>The CDC said this strain of deadly virus “breeds rapidly in tissue fibers.” The CDC is urging people to using a wet washcloth when cleaning themselves after using the bathroom instead of toilet paper.</ORIGINAL_TEXT>
				<TOKEN id="token-5-0" start_char="926" end_char="928">The</TOKEN>
				<TOKEN id="token-5-1" start_char="930" end_char="932">CDC</TOKEN>
				<TOKEN id="token-5-2" start_char="934" end_char="937">said</TOKEN>
				<TOKEN id="token-5-3" start_char="939" end_char="942">this</TOKEN>
				<TOKEN id="token-5-4" start_char="944" end_char="949">strain</TOKEN>
				<TOKEN id="token-5-5" start_char="951" end_char="952">of</TOKEN>
				<TOKEN id="token-5-6" start_char="954" end_char="959">deadly</TOKEN>
				<TOKEN id="token-5-7" start_char="961" end_char="965">virus</TOKEN>
				<TOKEN id="token-5-8" start_char="967" end_char="967">“</TOKEN>
				<TOKEN id="token-5-9" start_char="968" end_char="973">breeds</TOKEN>
				<TOKEN id="token-5-10" start_char="975" end_char="981">rapidly</TOKEN>
				<TOKEN id="token-5-11" start_char="983" end_char="984">in</TOKEN>
				<TOKEN id="token-5-12" start_char="986" end_char="991">tissue</TOKEN>
				<TOKEN id="token-5-13" start_char="993" end_char="998">fibers</TOKEN>
				<TOKEN id="token-5-14" start_char="999" end_char="1000">.”</TOKEN>
				<TOKEN id="token-5-15" start_char="1002" end_char="1004">The</TOKEN>
				<TOKEN id="token-5-16" start_char="1006" end_char="1008">CDC</TOKEN>
				<TOKEN id="token-5-17" start_char="1010" end_char="1011">is</TOKEN>
				<TOKEN id="token-5-18" start_char="1013" end_char="1018">urging</TOKEN>
				<TOKEN id="token-5-19" start_char="1020" end_char="1025">people</TOKEN>
				<TOKEN id="token-5-20" start_char="1027" end_char="1028">to</TOKEN>
				<TOKEN id="token-5-21" start_char="1030" end_char="1034">using</TOKEN>
				<TOKEN id="token-5-22" start_char="1036" end_char="1036">a</TOKEN>
				<TOKEN id="token-5-23" start_char="1038" end_char="1040">wet</TOKEN>
				<TOKEN id="token-5-24" start_char="1042" end_char="1050">washcloth</TOKEN>
				<TOKEN id="token-5-25" start_char="1052" end_char="1055">when</TOKEN>
				<TOKEN id="token-5-26" start_char="1057" end_char="1064">cleaning</TOKEN>
				<TOKEN id="token-5-27" start_char="1066" end_char="1075">themselves</TOKEN>
				<TOKEN id="token-5-28" start_char="1077" end_char="1081">after</TOKEN>
				<TOKEN id="token-5-29" start_char="1083" end_char="1087">using</TOKEN>
				<TOKEN id="token-5-30" start_char="1089" end_char="1091">the</TOKEN>
				<TOKEN id="token-5-31" start_char="1093" end_char="1100">bathroom</TOKEN>
				<TOKEN id="token-5-32" start_char="1102" end_char="1108">instead</TOKEN>
				<TOKEN id="token-5-33" start_char="1110" end_char="1111">of</TOKEN>
				<TOKEN id="token-5-34" start_char="1113" end_char="1118">toilet</TOKEN>
				<TOKEN id="token-5-35" start_char="1120" end_char="1124">paper</TOKEN>
				<TOKEN id="token-5-36" start_char="1125" end_char="1125">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-6" start_char="1127" end_char="1161">
				<ORIGINAL_TEXT>This is not a genuine news article.</ORIGINAL_TEXT>
				<TOKEN id="token-6-0" start_char="1127" end_char="1130">This</TOKEN>
				<TOKEN id="token-6-1" start_char="1132" end_char="1133">is</TOKEN>
				<TOKEN id="token-6-2" start_char="1135" end_char="1137">not</TOKEN>
				<TOKEN id="token-6-3" start_char="1139" end_char="1139">a</TOKEN>
				<TOKEN id="token-6-4" start_char="1141" end_char="1147">genuine</TOKEN>
				<TOKEN id="token-6-5" start_char="1149" end_char="1152">news</TOKEN>
				<TOKEN id="token-6-6" start_char="1154" end_char="1160">article</TOKEN>
				<TOKEN id="token-6-7" start_char="1161" end_char="1161">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-7" start_char="1163" end_char="1223">
				<ORIGINAL_TEXT>Now8News is a junk news site that traffics in misinformation.</ORIGINAL_TEXT>
				<TOKEN id="token-7-0" start_char="1163" end_char="1170">Now8News</TOKEN>
				<TOKEN id="token-7-1" start_char="1172" end_char="1173">is</TOKEN>
				<TOKEN id="token-7-2" start_char="1175" end_char="1175">a</TOKEN>
				<TOKEN id="token-7-3" start_char="1177" end_char="1180">junk</TOKEN>
				<TOKEN id="token-7-4" start_char="1182" end_char="1185">news</TOKEN>
				<TOKEN id="token-7-5" start_char="1187" end_char="1190">site</TOKEN>
				<TOKEN id="token-7-6" start_char="1192" end_char="1195">that</TOKEN>
				<TOKEN id="token-7-7" start_char="1197" end_char="1204">traffics</TOKEN>
				<TOKEN id="token-7-8" start_char="1206" end_char="1207">in</TOKEN>
				<TOKEN id="token-7-9" start_char="1209" end_char="1222">misinformation</TOKEN>
				<TOKEN id="token-7-10" start_char="1223" end_char="1223">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-8" start_char="1225" end_char="1378">
				<ORIGINAL_TEXT>Now8News does not carry a readily available disclaimer labeling its content as “satire” or “fake news,” but this website’s content is undoubtedly fiction.</ORIGINAL_TEXT>
				<TOKEN id="token-8-0" start_char="1225" end_char="1232">Now8News</TOKEN>
				<TOKEN id="token-8-1" start_char="1234" end_char="1237">does</TOKEN>
				<TOKEN id="token-8-2" start_char="1239" end_char="1241">not</TOKEN>
				<TOKEN id="token-8-3" start_char="1243" end_char="1247">carry</TOKEN>
				<TOKEN id="token-8-4" start_char="1249" end_char="1249">a</TOKEN>
				<TOKEN id="token-8-5" start_char="1251" end_char="1257">readily</TOKEN>
				<TOKEN id="token-8-6" start_char="1259" end_char="1267">available</TOKEN>
				<TOKEN id="token-8-7" start_char="1269" end_char="1278">disclaimer</TOKEN>
				<TOKEN id="token-8-8" start_char="1280" end_char="1287">labeling</TOKEN>
				<TOKEN id="token-8-9" start_char="1289" end_char="1291">its</TOKEN>
				<TOKEN id="token-8-10" start_char="1293" end_char="1299">content</TOKEN>
				<TOKEN id="token-8-11" start_char="1301" end_char="1302">as</TOKEN>
				<TOKEN id="token-8-12" start_char="1304" end_char="1304">“</TOKEN>
				<TOKEN id="token-8-13" start_char="1305" end_char="1310">satire</TOKEN>
				<TOKEN id="token-8-14" start_char="1311" end_char="1311">”</TOKEN>
				<TOKEN id="token-8-15" start_char="1313" end_char="1314">or</TOKEN>
				<TOKEN id="token-8-16" start_char="1316" end_char="1316">“</TOKEN>
				<TOKEN id="token-8-17" start_char="1317" end_char="1320">fake</TOKEN>
				<TOKEN id="token-8-18" start_char="1322" end_char="1325">news</TOKEN>
				<TOKEN id="token-8-19" start_char="1326" end_char="1327">,”</TOKEN>
				<TOKEN id="token-8-20" start_char="1329" end_char="1331">but</TOKEN>
				<TOKEN id="token-8-21" start_char="1333" end_char="1336">this</TOKEN>
				<TOKEN id="token-8-22" start_char="1338" end_char="1344">website</TOKEN>
				<TOKEN id="token-8-23" start_char="1345" end_char="1345">’</TOKEN>
				<TOKEN id="token-8-24" start_char="1346" end_char="1346">s</TOKEN>
				<TOKEN id="token-8-25" start_char="1348" end_char="1354">content</TOKEN>
				<TOKEN id="token-8-26" start_char="1356" end_char="1357">is</TOKEN>
				<TOKEN id="token-8-27" start_char="1359" end_char="1369">undoubtedly</TOKEN>
				<TOKEN id="token-8-28" start_char="1371" end_char="1377">fiction</TOKEN>
				<TOKEN id="token-8-29" start_char="1378" end_char="1378">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-9" start_char="1380" end_char="1647">
				<ORIGINAL_TEXT>We’ve been debunking misinformation from Now8News since at least 2015, including junk news articles about people having sex with pigs at Walmart, a cannibal eating a person at a haunted house in Texas, and a lottery winner who died after dipping his testicles in gold.</ORIGINAL_TEXT>
				<TOKEN id="token-9-0" start_char="1380" end_char="1381">We</TOKEN>
				<TOKEN id="token-9-1" start_char="1382" end_char="1382">’</TOKEN>
				<TOKEN id="token-9-2" start_char="1383" end_char="1384">ve</TOKEN>
				<TOKEN id="token-9-3" start_char="1386" end_char="1389">been</TOKEN>
				<TOKEN id="token-9-4" start_char="1391" end_char="1399">debunking</TOKEN>
				<TOKEN id="token-9-5" start_char="1401" end_char="1414">misinformation</TOKEN>
				<TOKEN id="token-9-6" start_char="1416" end_char="1419">from</TOKEN>
				<TOKEN id="token-9-7" start_char="1421" end_char="1428">Now8News</TOKEN>
				<TOKEN id="token-9-8" start_char="1430" end_char="1434">since</TOKEN>
				<TOKEN id="token-9-9" start_char="1436" end_char="1437">at</TOKEN>
				<TOKEN id="token-9-10" start_char="1439" end_char="1443">least</TOKEN>
				<TOKEN id="token-9-11" start_char="1445" end_char="1448">2015</TOKEN>
				<TOKEN id="token-9-12" start_char="1449" end_char="1449">,</TOKEN>
				<TOKEN id="token-9-13" start_char="1451" end_char="1459">including</TOKEN>
				<TOKEN id="token-9-14" start_char="1461" end_char="1464">junk</TOKEN>
				<TOKEN id="token-9-15" start_char="1466" end_char="1469">news</TOKEN>
				<TOKEN id="token-9-16" start_char="1471" end_char="1478">articles</TOKEN>
				<TOKEN id="token-9-17" start_char="1480" end_char="1484">about</TOKEN>
				<TOKEN id="token-9-18" start_char="1486" end_char="1491">people</TOKEN>
				<TOKEN id="token-9-19" start_char="1493" end_char="1498">having</TOKEN>
				<TOKEN id="token-9-20" start_char="1500" end_char="1502">sex</TOKEN>
				<TOKEN id="token-9-21" start_char="1504" end_char="1507">with</TOKEN>
				<TOKEN id="token-9-22" start_char="1509" end_char="1512">pigs</TOKEN>
				<TOKEN id="token-9-23" start_char="1514" end_char="1515">at</TOKEN>
				<TOKEN id="token-9-24" start_char="1517" end_char="1523">Walmart</TOKEN>
				<TOKEN id="token-9-25" start_char="1524" end_char="1524">,</TOKEN>
				<TOKEN id="token-9-26" start_char="1526" end_char="1526">a</TOKEN>
				<TOKEN id="token-9-27" start_char="1528" end_char="1535">cannibal</TOKEN>
				<TOKEN id="token-9-28" start_char="1537" end_char="1542">eating</TOKEN>
				<TOKEN id="token-9-29" start_char="1544" end_char="1544">a</TOKEN>
				<TOKEN id="token-9-30" start_char="1546" end_char="1551">person</TOKEN>
				<TOKEN id="token-9-31" start_char="1553" end_char="1554">at</TOKEN>
				<TOKEN id="token-9-32" start_char="1556" end_char="1556">a</TOKEN>
				<TOKEN id="token-9-33" start_char="1558" end_char="1564">haunted</TOKEN>
				<TOKEN id="token-9-34" start_char="1566" end_char="1570">house</TOKEN>
				<TOKEN id="token-9-35" start_char="1572" end_char="1573">in</TOKEN>
				<TOKEN id="token-9-36" start_char="1575" end_char="1579">Texas</TOKEN>
				<TOKEN id="token-9-37" start_char="1580" end_char="1580">,</TOKEN>
				<TOKEN id="token-9-38" start_char="1582" end_char="1584">and</TOKEN>
				<TOKEN id="token-9-39" start_char="1586" end_char="1586">a</TOKEN>
				<TOKEN id="token-9-40" start_char="1588" end_char="1594">lottery</TOKEN>
				<TOKEN id="token-9-41" start_char="1596" end_char="1601">winner</TOKEN>
				<TOKEN id="token-9-42" start_char="1603" end_char="1605">who</TOKEN>
				<TOKEN id="token-9-43" start_char="1607" end_char="1610">died</TOKEN>
				<TOKEN id="token-9-44" start_char="1612" end_char="1616">after</TOKEN>
				<TOKEN id="token-9-45" start_char="1618" end_char="1624">dipping</TOKEN>
				<TOKEN id="token-9-46" start_char="1626" end_char="1628">his</TOKEN>
				<TOKEN id="token-9-47" start_char="1630" end_char="1638">testicles</TOKEN>
				<TOKEN id="token-9-48" start_char="1640" end_char="1641">in</TOKEN>
				<TOKEN id="token-9-49" start_char="1643" end_char="1646">gold</TOKEN>
				<TOKEN id="token-9-50" start_char="1647" end_char="1647">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-10" start_char="1649" end_char="1755">
				<ORIGINAL_TEXT>Those unfamiliar with this site, however, were given no indications that this toilet-paper story was false.</ORIGINAL_TEXT>
				<TOKEN id="token-10-0" start_char="1649" end_char="1653">Those</TOKEN>
				<TOKEN id="token-10-1" start_char="1655" end_char="1664">unfamiliar</TOKEN>
				<TOKEN id="token-10-2" start_char="1666" end_char="1669">with</TOKEN>
				<TOKEN id="token-10-3" start_char="1671" end_char="1674">this</TOKEN>
				<TOKEN id="token-10-4" start_char="1676" end_char="1679">site</TOKEN>
				<TOKEN id="token-10-5" start_char="1680" end_char="1680">,</TOKEN>
				<TOKEN id="token-10-6" start_char="1682" end_char="1688">however</TOKEN>
				<TOKEN id="token-10-7" start_char="1689" end_char="1689">,</TOKEN>
				<TOKEN id="token-10-8" start_char="1691" end_char="1694">were</TOKEN>
				<TOKEN id="token-10-9" start_char="1696" end_char="1700">given</TOKEN>
				<TOKEN id="token-10-10" start_char="1702" end_char="1703">no</TOKEN>
				<TOKEN id="token-10-11" start_char="1705" end_char="1715">indications</TOKEN>
				<TOKEN id="token-10-12" start_char="1717" end_char="1720">that</TOKEN>
				<TOKEN id="token-10-13" start_char="1722" end_char="1725">this</TOKEN>
				<TOKEN id="token-10-14" start_char="1727" end_char="1732">toilet</TOKEN>
				<TOKEN id="token-10-15" start_char="1733" end_char="1733">-</TOKEN>
				<TOKEN id="token-10-16" start_char="1734" end_char="1738">paper</TOKEN>
				<TOKEN id="token-10-17" start_char="1740" end_char="1744">story</TOKEN>
				<TOKEN id="token-10-18" start_char="1746" end_char="1748">was</TOKEN>
				<TOKEN id="token-10-19" start_char="1750" end_char="1754">false</TOKEN>
				<TOKEN id="token-10-20" start_char="1755" end_char="1755">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-11" start_char="1757" end_char="1852">
				<ORIGINAL_TEXT>It was presented like a traditional news story and even includes fabricated quotes from the CDC.</ORIGINAL_TEXT>
				<TOKEN id="token-11-0" start_char="1757" end_char="1758">It</TOKEN>
				<TOKEN id="token-11-1" start_char="1760" end_char="1762">was</TOKEN>
				<TOKEN id="token-11-2" start_char="1764" end_char="1772">presented</TOKEN>
				<TOKEN id="token-11-3" start_char="1774" end_char="1777">like</TOKEN>
				<TOKEN id="token-11-4" start_char="1779" end_char="1779">a</TOKEN>
				<TOKEN id="token-11-5" start_char="1781" end_char="1791">traditional</TOKEN>
				<TOKEN id="token-11-6" start_char="1793" end_char="1796">news</TOKEN>
				<TOKEN id="token-11-7" start_char="1798" end_char="1802">story</TOKEN>
				<TOKEN id="token-11-8" start_char="1804" end_char="1806">and</TOKEN>
				<TOKEN id="token-11-9" start_char="1808" end_char="1811">even</TOKEN>
				<TOKEN id="token-11-10" start_char="1813" end_char="1820">includes</TOKEN>
				<TOKEN id="token-11-11" start_char="1822" end_char="1831">fabricated</TOKEN>
				<TOKEN id="token-11-12" start_char="1833" end_char="1838">quotes</TOKEN>
				<TOKEN id="token-11-13" start_char="1840" end_char="1843">from</TOKEN>
				<TOKEN id="token-11-14" start_char="1845" end_char="1847">the</TOKEN>
				<TOKEN id="token-11-15" start_char="1849" end_char="1851">CDC</TOKEN>
				<TOKEN id="token-11-16" start_char="1852" end_char="1852">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-12" start_char="1854" end_char="1871">
				<ORIGINAL_TEXT>Now 8 News writes:</ORIGINAL_TEXT>
				<TOKEN id="token-12-0" start_char="1854" end_char="1856">Now</TOKEN>
				<TOKEN id="token-12-1" start_char="1858" end_char="1858">8</TOKEN>
				<TOKEN id="token-12-2" start_char="1860" end_char="1863">News</TOKEN>
				<TOKEN id="token-12-3" start_char="1865" end_char="1870">writes</TOKEN>
				<TOKEN id="token-12-4" start_char="1871" end_char="1871">:</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-13" start_char="1873" end_char="1996">
				<ORIGINAL_TEXT>The CDC is urging people to using a wet washcloth when cleaning themselves after using the bathroom instead of toilet paper.</ORIGINAL_TEXT>
				<TOKEN id="token-13-0" start_char="1873" end_char="1875">The</TOKEN>
				<TOKEN id="token-13-1" start_char="1877" end_char="1879">CDC</TOKEN>
				<TOKEN id="token-13-2" start_char="1881" end_char="1882">is</TOKEN>
				<TOKEN id="token-13-3" start_char="1884" end_char="1889">urging</TOKEN>
				<TOKEN id="token-13-4" start_char="1891" end_char="1896">people</TOKEN>
				<TOKEN id="token-13-5" start_char="1898" end_char="1899">to</TOKEN>
				<TOKEN id="token-13-6" start_char="1901" end_char="1905">using</TOKEN>
				<TOKEN id="token-13-7" start_char="1907" end_char="1907">a</TOKEN>
				<TOKEN id="token-13-8" start_char="1909" end_char="1911">wet</TOKEN>
				<TOKEN id="token-13-9" start_char="1913" end_char="1921">washcloth</TOKEN>
				<TOKEN id="token-13-10" start_char="1923" end_char="1926">when</TOKEN>
				<TOKEN id="token-13-11" start_char="1928" end_char="1935">cleaning</TOKEN>
				<TOKEN id="token-13-12" start_char="1937" end_char="1946">themselves</TOKEN>
				<TOKEN id="token-13-13" start_char="1948" end_char="1952">after</TOKEN>
				<TOKEN id="token-13-14" start_char="1954" end_char="1958">using</TOKEN>
				<TOKEN id="token-13-15" start_char="1960" end_char="1962">the</TOKEN>
				<TOKEN id="token-13-16" start_char="1964" end_char="1971">bathroom</TOKEN>
				<TOKEN id="token-13-17" start_char="1973" end_char="1979">instead</TOKEN>
				<TOKEN id="token-13-18" start_char="1981" end_char="1982">of</TOKEN>
				<TOKEN id="token-13-19" start_char="1984" end_char="1989">toilet</TOKEN>
				<TOKEN id="token-13-20" start_char="1991" end_char="1995">paper</TOKEN>
				<TOKEN id="token-13-21" start_char="1996" end_char="1996">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-14" start_char="1998" end_char="2205">
				<ORIGINAL_TEXT>“Use a washcloth to clean yourself after you go to the bathroom, it’s not a big deal, that’s what we did back in the old days,” said Peter Lendl, who headed the investigation of the contaminated toilet paper.</ORIGINAL_TEXT>
				<TOKEN id="token-14-0" start_char="1998" end_char="1998">“</TOKEN>
				<TOKEN id="token-14-1" start_char="1999" end_char="2001">Use</TOKEN>
				<TOKEN id="token-14-2" start_char="2003" end_char="2003">a</TOKEN>
				<TOKEN id="token-14-3" start_char="2005" end_char="2013">washcloth</TOKEN>
				<TOKEN id="token-14-4" start_char="2015" end_char="2016">to</TOKEN>
				<TOKEN id="token-14-5" start_char="2018" end_char="2022">clean</TOKEN>
				<TOKEN id="token-14-6" start_char="2024" end_char="2031">yourself</TOKEN>
				<TOKEN id="token-14-7" start_char="2033" end_char="2037">after</TOKEN>
				<TOKEN id="token-14-8" start_char="2039" end_char="2041">you</TOKEN>
				<TOKEN id="token-14-9" start_char="2043" end_char="2044">go</TOKEN>
				<TOKEN id="token-14-10" start_char="2046" end_char="2047">to</TOKEN>
				<TOKEN id="token-14-11" start_char="2049" end_char="2051">the</TOKEN>
				<TOKEN id="token-14-12" start_char="2053" end_char="2060">bathroom</TOKEN>
				<TOKEN id="token-14-13" start_char="2061" end_char="2061">,</TOKEN>
				<TOKEN id="token-14-14" start_char="2063" end_char="2064">it</TOKEN>
				<TOKEN id="token-14-15" start_char="2065" end_char="2065">’</TOKEN>
				<TOKEN id="token-14-16" start_char="2066" end_char="2066">s</TOKEN>
				<TOKEN id="token-14-17" start_char="2068" end_char="2070">not</TOKEN>
				<TOKEN id="token-14-18" start_char="2072" end_char="2072">a</TOKEN>
				<TOKEN id="token-14-19" start_char="2074" end_char="2076">big</TOKEN>
				<TOKEN id="token-14-20" start_char="2078" end_char="2081">deal</TOKEN>
				<TOKEN id="token-14-21" start_char="2082" end_char="2082">,</TOKEN>
				<TOKEN id="token-14-22" start_char="2084" end_char="2087">that</TOKEN>
				<TOKEN id="token-14-23" start_char="2088" end_char="2088">’</TOKEN>
				<TOKEN id="token-14-24" start_char="2089" end_char="2089">s</TOKEN>
				<TOKEN id="token-14-25" start_char="2091" end_char="2094">what</TOKEN>
				<TOKEN id="token-14-26" start_char="2096" end_char="2097">we</TOKEN>
				<TOKEN id="token-14-27" start_char="2099" end_char="2101">did</TOKEN>
				<TOKEN id="token-14-28" start_char="2103" end_char="2106">back</TOKEN>
				<TOKEN id="token-14-29" start_char="2108" end_char="2109">in</TOKEN>
				<TOKEN id="token-14-30" start_char="2111" end_char="2113">the</TOKEN>
				<TOKEN id="token-14-31" start_char="2115" end_char="2117">old</TOKEN>
				<TOKEN id="token-14-32" start_char="2119" end_char="2122">days</TOKEN>
				<TOKEN id="token-14-33" start_char="2123" end_char="2124">,”</TOKEN>
				<TOKEN id="token-14-34" start_char="2126" end_char="2129">said</TOKEN>
				<TOKEN id="token-14-35" start_char="2131" end_char="2135">Peter</TOKEN>
				<TOKEN id="token-14-36" start_char="2137" end_char="2141">Lendl</TOKEN>
				<TOKEN id="token-14-37" start_char="2142" end_char="2142">,</TOKEN>
				<TOKEN id="token-14-38" start_char="2144" end_char="2146">who</TOKEN>
				<TOKEN id="token-14-39" start_char="2148" end_char="2153">headed</TOKEN>
				<TOKEN id="token-14-40" start_char="2155" end_char="2157">the</TOKEN>
				<TOKEN id="token-14-41" start_char="2159" end_char="2171">investigation</TOKEN>
				<TOKEN id="token-14-42" start_char="2173" end_char="2174">of</TOKEN>
				<TOKEN id="token-14-43" start_char="2176" end_char="2178">the</TOKEN>
				<TOKEN id="token-14-44" start_char="2180" end_char="2191">contaminated</TOKEN>
				<TOKEN id="token-14-45" start_char="2193" end_char="2198">toilet</TOKEN>
				<TOKEN id="token-14-46" start_char="2200" end_char="2204">paper</TOKEN>
				<TOKEN id="token-14-47" start_char="2205" end_char="2205">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-15" start_char="2207" end_char="2237">
				<ORIGINAL_TEXT>“Just know which one is yours.”</ORIGINAL_TEXT>
				<TOKEN id="token-15-0" start_char="2207" end_char="2207">“</TOKEN>
				<TOKEN id="token-15-1" start_char="2208" end_char="2211">Just</TOKEN>
				<TOKEN id="token-15-2" start_char="2213" end_char="2216">know</TOKEN>
				<TOKEN id="token-15-3" start_char="2218" end_char="2222">which</TOKEN>
				<TOKEN id="token-15-4" start_char="2224" end_char="2226">one</TOKEN>
				<TOKEN id="token-15-5" start_char="2228" end_char="2229">is</TOKEN>
				<TOKEN id="token-15-6" start_char="2231" end_char="2235">yours</TOKEN>
				<TOKEN id="token-15-7" start_char="2236" end_char="2237">.”</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-16" start_char="2239" end_char="2279">
				<ORIGINAL_TEXT>This is not a genuine quote from the CDC.</ORIGINAL_TEXT>
				<TOKEN id="token-16-0" start_char="2239" end_char="2242">This</TOKEN>
				<TOKEN id="token-16-1" start_char="2244" end_char="2245">is</TOKEN>
				<TOKEN id="token-16-2" start_char="2247" end_char="2249">not</TOKEN>
				<TOKEN id="token-16-3" start_char="2251" end_char="2251">a</TOKEN>
				<TOKEN id="token-16-4" start_char="2253" end_char="2259">genuine</TOKEN>
				<TOKEN id="token-16-5" start_char="2261" end_char="2265">quote</TOKEN>
				<TOKEN id="token-16-6" start_char="2267" end_char="2270">from</TOKEN>
				<TOKEN id="token-16-7" start_char="2272" end_char="2274">the</TOKEN>
				<TOKEN id="token-16-8" start_char="2276" end_char="2278">CDC</TOKEN>
				<TOKEN id="token-16-9" start_char="2279" end_char="2279">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-17" start_char="2281" end_char="2413">
				<ORIGINAL_TEXT>We were unable to find this quote in news articles, social media posts from someone named “Peter Lendl,” or from a CDC press release.</ORIGINAL_TEXT>
				<TOKEN id="token-17-0" start_char="2281" end_char="2282">We</TOKEN>
				<TOKEN id="token-17-1" start_char="2284" end_char="2287">were</TOKEN>
				<TOKEN id="token-17-2" start_char="2289" end_char="2294">unable</TOKEN>
				<TOKEN id="token-17-3" start_char="2296" end_char="2297">to</TOKEN>
				<TOKEN id="token-17-4" start_char="2299" end_char="2302">find</TOKEN>
				<TOKEN id="token-17-5" start_char="2304" end_char="2307">this</TOKEN>
				<TOKEN id="token-17-6" start_char="2309" end_char="2313">quote</TOKEN>
				<TOKEN id="token-17-7" start_char="2315" end_char="2316">in</TOKEN>
				<TOKEN id="token-17-8" start_char="2318" end_char="2321">news</TOKEN>
				<TOKEN id="token-17-9" start_char="2323" end_char="2330">articles</TOKEN>
				<TOKEN id="token-17-10" start_char="2331" end_char="2331">,</TOKEN>
				<TOKEN id="token-17-11" start_char="2333" end_char="2338">social</TOKEN>
				<TOKEN id="token-17-12" start_char="2340" end_char="2344">media</TOKEN>
				<TOKEN id="token-17-13" start_char="2346" end_char="2350">posts</TOKEN>
				<TOKEN id="token-17-14" start_char="2352" end_char="2355">from</TOKEN>
				<TOKEN id="token-17-15" start_char="2357" end_char="2363">someone</TOKEN>
				<TOKEN id="token-17-16" start_char="2365" end_char="2369">named</TOKEN>
				<TOKEN id="token-17-17" start_char="2371" end_char="2371">“</TOKEN>
				<TOKEN id="token-17-18" start_char="2372" end_char="2376">Peter</TOKEN>
				<TOKEN id="token-17-19" start_char="2378" end_char="2382">Lendl</TOKEN>
				<TOKEN id="token-17-20" start_char="2383" end_char="2384">,”</TOKEN>
				<TOKEN id="token-17-21" start_char="2386" end_char="2387">or</TOKEN>
				<TOKEN id="token-17-22" start_char="2389" end_char="2392">from</TOKEN>
				<TOKEN id="token-17-23" start_char="2394" end_char="2394">a</TOKEN>
				<TOKEN id="token-17-24" start_char="2396" end_char="2398">CDC</TOKEN>
				<TOKEN id="token-17-25" start_char="2400" end_char="2404">press</TOKEN>
				<TOKEN id="token-17-26" start_char="2406" end_char="2412">release</TOKEN>
				<TOKEN id="token-17-27" start_char="2413" end_char="2413">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-18" start_char="2415" end_char="2508">
				<ORIGINAL_TEXT>In fact, the only results for this quote pointed back to this junk news article from Now8News.</ORIGINAL_TEXT>
				<TOKEN id="token-18-0" start_char="2415" end_char="2416">In</TOKEN>
				<TOKEN id="token-18-1" start_char="2418" end_char="2421">fact</TOKEN>
				<TOKEN id="token-18-2" start_char="2422" end_char="2422">,</TOKEN>
				<TOKEN id="token-18-3" start_char="2424" end_char="2426">the</TOKEN>
				<TOKEN id="token-18-4" start_char="2428" end_char="2431">only</TOKEN>
				<TOKEN id="token-18-5" start_char="2433" end_char="2439">results</TOKEN>
				<TOKEN id="token-18-6" start_char="2441" end_char="2443">for</TOKEN>
				<TOKEN id="token-18-7" start_char="2445" end_char="2448">this</TOKEN>
				<TOKEN id="token-18-8" start_char="2450" end_char="2454">quote</TOKEN>
				<TOKEN id="token-18-9" start_char="2456" end_char="2462">pointed</TOKEN>
				<TOKEN id="token-18-10" start_char="2464" end_char="2467">back</TOKEN>
				<TOKEN id="token-18-11" start_char="2469" end_char="2470">to</TOKEN>
				<TOKEN id="token-18-12" start_char="2472" end_char="2475">this</TOKEN>
				<TOKEN id="token-18-13" start_char="2477" end_char="2480">junk</TOKEN>
				<TOKEN id="token-18-14" start_char="2482" end_char="2485">news</TOKEN>
				<TOKEN id="token-18-15" start_char="2487" end_char="2493">article</TOKEN>
				<TOKEN id="token-18-16" start_char="2495" end_char="2498">from</TOKEN>
				<TOKEN id="token-18-17" start_char="2500" end_char="2507">Now8News</TOKEN>
				<TOKEN id="token-18-18" start_char="2508" end_char="2508">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-19" start_char="2510" end_char="2681">
				<ORIGINAL_TEXT>In sum, the CDC did not announce in March 2020 that the new strain of coronavirus was “rapidly breeding” on toilet paper and that people should use a wet washcloth instead.</ORIGINAL_TEXT>
				<TOKEN id="token-19-0" start_char="2510" end_char="2511">In</TOKEN>
				<TOKEN id="token-19-1" start_char="2513" end_char="2515">sum</TOKEN>
				<TOKEN id="token-19-2" start_char="2516" end_char="2516">,</TOKEN>
				<TOKEN id="token-19-3" start_char="2518" end_char="2520">the</TOKEN>
				<TOKEN id="token-19-4" start_char="2522" end_char="2524">CDC</TOKEN>
				<TOKEN id="token-19-5" start_char="2526" end_char="2528">did</TOKEN>
				<TOKEN id="token-19-6" start_char="2530" end_char="2532">not</TOKEN>
				<TOKEN id="token-19-7" start_char="2534" end_char="2541">announce</TOKEN>
				<TOKEN id="token-19-8" start_char="2543" end_char="2544">in</TOKEN>
				<TOKEN id="token-19-9" start_char="2546" end_char="2550">March</TOKEN>
				<TOKEN id="token-19-10" start_char="2552" end_char="2555">2020</TOKEN>
				<TOKEN id="token-19-11" start_char="2557" end_char="2560">that</TOKEN>
				<TOKEN id="token-19-12" start_char="2562" end_char="2564">the</TOKEN>
				<TOKEN id="token-19-13" start_char="2566" end_char="2568">new</TOKEN>
				<TOKEN id="token-19-14" start_char="2570" end_char="2575">strain</TOKEN>
				<TOKEN id="token-19-15" start_char="2577" end_char="2578">of</TOKEN>
				<TOKEN id="token-19-16" start_char="2580" end_char="2590">coronavirus</TOKEN>
				<TOKEN id="token-19-17" start_char="2592" end_char="2594">was</TOKEN>
				<TOKEN id="token-19-18" start_char="2596" end_char="2596">“</TOKEN>
				<TOKEN id="token-19-19" start_char="2597" end_char="2603">rapidly</TOKEN>
				<TOKEN id="token-19-20" start_char="2605" end_char="2612">breeding</TOKEN>
				<TOKEN id="token-19-21" start_char="2613" end_char="2613">”</TOKEN>
				<TOKEN id="token-19-22" start_char="2615" end_char="2616">on</TOKEN>
				<TOKEN id="token-19-23" start_char="2618" end_char="2623">toilet</TOKEN>
				<TOKEN id="token-19-24" start_char="2625" end_char="2629">paper</TOKEN>
				<TOKEN id="token-19-25" start_char="2631" end_char="2633">and</TOKEN>
				<TOKEN id="token-19-26" start_char="2635" end_char="2638">that</TOKEN>
				<TOKEN id="token-19-27" start_char="2640" end_char="2645">people</TOKEN>
				<TOKEN id="token-19-28" start_char="2647" end_char="2652">should</TOKEN>
				<TOKEN id="token-19-29" start_char="2654" end_char="2656">use</TOKEN>
				<TOKEN id="token-19-30" start_char="2658" end_char="2658">a</TOKEN>
				<TOKEN id="token-19-31" start_char="2660" end_char="2662">wet</TOKEN>
				<TOKEN id="token-19-32" start_char="2664" end_char="2672">washcloth</TOKEN>
				<TOKEN id="token-19-33" start_char="2674" end_char="2680">instead</TOKEN>
				<TOKEN id="token-19-34" start_char="2681" end_char="2681">.</TOKEN>
			</SEG>
			<SEG id="covid19scenario-48-20" start_char="2683" end_char="2773">
				<ORIGINAL_TEXT>You can find genuine information about how to deal with the coronavirus on the CDC website.</ORIGINAL_TEXT>
				<TOKEN id="token-20-0" start_char="2683" end_char="2685">You</TOKEN>
				<TOKEN id="token-20-1" start_char="2687" end_char="2689">can</TOKEN>
				<TOKEN id="token-20-2" start_char="2691" end_char="2694">find</TOKEN>
				<TOKEN id="token-20-3" start_char="2696" end_char="2702">genuine</TOKEN>
				<TOKEN id="token-20-4" start_char="2704" end_char="2714">information</TOKEN>
				<TOKEN id="token-20-5" start_char="2716" end_char="2720">about</TOKEN>
				<TOKEN id="token-20-6" start_char="2722" end_char="2724">how</TOKEN>
				<TOKEN id="token-20-7" start_char="2726" end_char="2727">to</TOKEN>
				<TOKEN id="token-20-8" start_char="2729" end_char="2732">deal</TOKEN>
				<TOKEN id="token-20-9" start_char="2734" end_char="2737">with</TOKEN>
				<TOKEN id="token-20-10" start_char="2739" end_char="2741">the</TOKEN>
				<TOKEN id="token-20-11" start_char="2743" end_char="2753">coronavirus</TOKEN>
				<TOKEN id="token-20-12" start_char="2755" end_char="2756">on</TOKEN>
				<TOKEN id="token-20-13" start_char="2758" end_char="2760">the</TOKEN>
				<TOKEN id="token-20-14" start_char="2762" end_char="2764">CDC</TOKEN>
				<TOKEN id="token-20-15" start_char="2766" end_char="2772">website</TOKEN>
				<TOKEN id="token-20-16" start_char="2773" end_char="2773">.</TOKEN>
			</SEG>
		</TEXT>
	</DOC>
</LCTL_TEXT>
