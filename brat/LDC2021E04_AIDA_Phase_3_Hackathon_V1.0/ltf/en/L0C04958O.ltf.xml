<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="rus">
<DOC id="L0C04958O" lang="rus" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="12126" raw_text_md5="cef7beb8eec5915cedaea3679cd5eae8">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="52">
<ORIGINAL_TEXT>Experts know the new coronavirus is not a bioweapon.</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Experts</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="12">know</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="14" end_char="16">the</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="18" end_char="20">new</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="22" end_char="32">coronavirus</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="35">is</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="37" end_char="39">not</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="41" end_char="41">a</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="43" end_char="51">bioweapon</TOKEN>
<TOKEN id="token-0-9" pos="punct" morph="none" start_char="52" end_char="52">.</TOKEN>
</SEG>
<SEG id="segment-1" start_char="54" end_char="118">
<ORIGINAL_TEXT>They disagree on whether it could have leaked from a research lab</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="54" end_char="57">They</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="59" end_char="66">disagree</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="68" end_char="69">on</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="71" end_char="77">whether</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="79" end_char="80">it</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="82" end_char="86">could</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="88" end_char="91">have</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="93" end_char="98">leaked</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="100" end_char="103">from</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="105" end_char="105">a</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="107" end_char="114">research</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="116" end_char="118">lab</TOKEN>
</SEG>
<SEG id="segment-2" start_char="122" end_char="206">
<ORIGINAL_TEXT>I agree with Ebright that one shouldn’t rule out a laboratory accident as the source.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="122" end_char="122">I</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="124" end_char="128">agree</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="130" end_char="133">with</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="135" end_char="141">Ebright</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="143" end_char="146">that</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="148" end_char="150">one</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="152" end_char="160">shouldn’t</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="162" end_char="165">rule</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="167" end_char="169">out</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="171" end_char="171">a</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="173" end_char="182">laboratory</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="184" end_char="191">accident</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="193" end_char="194">as</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="196" end_char="198">the</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="200" end_char="205">source</TOKEN>
<TOKEN id="token-2-15" pos="punct" morph="none" start_char="206" end_char="206">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="208" end_char="416">
<ORIGINAL_TEXT>The Nature opinion piece authors are right that the unusual molecular structure of the virus and the randomness in the mutations from the HKU3-1 bat virus suggest no purposeful manipulation for bioweapons use.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="208" end_char="210">The</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="212" end_char="217">Nature</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="219" end_char="225">opinion</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="227" end_char="231">piece</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="233" end_char="239">authors</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="241" end_char="243">are</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="245" end_char="249">right</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="251" end_char="254">that</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="256" end_char="258">the</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="260" end_char="266">unusual</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="268" end_char="276">molecular</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="278" end_char="286">structure</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="288" end_char="289">of</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="291" end_char="293">the</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="295" end_char="299">virus</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="301" end_char="303">and</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="305" end_char="307">the</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="309" end_char="318">randomness</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="320" end_char="321">in</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="323" end_char="325">the</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="327" end_char="335">mutations</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="337" end_char="340">from</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="342" end_char="344">the</TOKEN>
<TOKEN id="token-3-23" pos="unknown" morph="none" start_char="346" end_char="351">HKU3-1</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="353" end_char="355">bat</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="357" end_char="361">virus</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="363" end_char="369">suggest</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="371" end_char="372">no</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="374" end_char="383">purposeful</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="385" end_char="396">manipulation</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="398" end_char="400">for</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="402" end_char="411">bioweapons</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="413" end_char="415">use</TOKEN>
<TOKEN id="token-3-33" pos="punct" morph="none" start_char="416" end_char="416">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="418" end_char="498">
<ORIGINAL_TEXT>However, an accident in a disease transmission experiment shouldn’t be dismissed.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="418" end_char="424">However</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="425" end_char="425">,</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="427" end_char="428">an</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="430" end_char="437">accident</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="439" end_char="440">in</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="442" end_char="442">a</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="444" end_char="450">disease</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="452" end_char="463">transmission</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="465" end_char="474">experiment</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="476" end_char="484">shouldn’t</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="486" end_char="487">be</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="489" end_char="497">dismissed</TOKEN>
<TOKEN id="token-4-12" pos="punct" morph="none" start_char="498" end_char="498">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="500" end_char="670">
<ORIGINAL_TEXT>The Wuhan lab has been conducting experiments for almost 2 decades to understand how the 2003 SARS-CoV jumped to humans and to identify the natural reservoir of the virus.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="500" end_char="502">The</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="504" end_char="508">Wuhan</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="510" end_char="512">lab</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="514" end_char="516">has</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="518" end_char="521">been</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="523" end_char="532">conducting</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="534" end_char="544">experiments</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="546" end_char="548">for</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="550" end_char="555">almost</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="557" end_char="557">2</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="559" end_char="565">decades</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="567" end_char="568">to</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="570" end_char="579">understand</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="581" end_char="583">how</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="585" end_char="587">the</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="589" end_char="592">2003</TOKEN>
<TOKEN id="token-5-16" pos="unknown" morph="none" start_char="594" end_char="601">SARS-CoV</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="603" end_char="608">jumped</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="610" end_char="611">to</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="613" end_char="618">humans</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="620" end_char="622">and</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="624" end_char="625">to</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="627" end_char="634">identify</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="636" end_char="638">the</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="640" end_char="646">natural</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="648" end_char="656">reservoir</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="658" end_char="659">of</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="661" end_char="663">the</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="665" end_char="669">virus</TOKEN>
<TOKEN id="token-5-29" pos="punct" morph="none" start_char="670" end_char="670">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="672" end_char="741">
<ORIGINAL_TEXT>Bat coronaviruses typically bond to bat ACE2 and rarely to human ACE2.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="672" end_char="674">Bat</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="676" end_char="688">coronaviruses</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="690" end_char="698">typically</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="700" end_char="703">bond</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="705" end_char="706">to</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="708" end_char="710">bat</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="712" end_char="715">ACE2</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="717" end_char="719">and</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="721" end_char="726">rarely</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="728" end_char="729">to</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="731" end_char="735">human</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="737" end_char="740">ACE2</TOKEN>
<TOKEN id="token-6-12" pos="punct" morph="none" start_char="741" end_char="741">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="743" end_char="755">
<ORIGINAL_TEXT>… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="743" end_char="743">…</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="745" end_char="748">Read</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="750" end_char="753">more</TOKEN>
<TOKEN id="token-7-3" pos="punct" morph="none" start_char="755" end_char="755">»</TOKEN>
</SEG>
<SEG id="segment-8" start_char="759" end_char="796">
<ORIGINAL_TEXT>Correction: I meant RaTG13 not HKU3-1.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="759" end_char="768">Correction</TOKEN>
<TOKEN id="token-8-1" pos="punct" morph="none" start_char="769" end_char="769">:</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="771" end_char="771">I</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="773" end_char="777">meant</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="779" end_char="784">RaTG13</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="786" end_char="788">not</TOKEN>
<TOKEN id="token-8-6" pos="unknown" morph="none" start_char="790" end_char="795">HKU3-1</TOKEN>
<TOKEN id="token-8-7" pos="punct" morph="none" start_char="796" end_char="796">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="798" end_char="832">
<ORIGINAL_TEXT>I misread the legend on the figure.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="798" end_char="798">I</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="800" end_char="806">misread</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="808" end_char="810">the</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="812" end_char="817">legend</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="819" end_char="820">on</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="822" end_char="824">the</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="826" end_char="831">figure</TOKEN>
<TOKEN id="token-9-7" pos="punct" morph="none" start_char="832" end_char="832">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="836" end_char="936">
<ORIGINAL_TEXT>What advantage might a deliberate leaker have anticipated –therefore why would any sane person do it?</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="836" end_char="839">What</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="841" end_char="849">advantage</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="851" end_char="855">might</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="857" end_char="857">a</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="859" end_char="868">deliberate</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="870" end_char="875">leaker</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="877" end_char="880">have</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="882" end_char="892">anticipated</TOKEN>
<TOKEN id="token-10-8" pos="punct" morph="none" start_char="894" end_char="894">–</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="895" end_char="903">therefore</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="905" end_char="907">why</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="909" end_char="913">would</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="915" end_char="917">any</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="919" end_char="922">sane</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="924" end_char="929">person</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="931" end_char="932">do</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="934" end_char="935">it</TOKEN>
<TOKEN id="token-10-17" pos="punct" morph="none" start_char="936" end_char="936">?</TOKEN>
</SEG>
<SEG id="segment-11" start_char="940" end_char="1045">
<ORIGINAL_TEXT>We know the old dictum: Don’t attribute to malevolence that which can be explained by simple incompetence.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="940" end_char="941">We</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="943" end_char="946">know</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="948" end_char="950">the</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="952" end_char="954">old</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="956" end_char="961">dictum</TOKEN>
<TOKEN id="token-11-5" pos="punct" morph="none" start_char="962" end_char="962">:</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="964" end_char="968">Don’t</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="970" end_char="978">attribute</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="980" end_char="981">to</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="983" end_char="993">malevolence</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="995" end_char="998">that</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1000" end_char="1004">which</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1006" end_char="1008">can</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1010" end_char="1011">be</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1013" end_char="1021">explained</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1023" end_char="1024">by</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1026" end_char="1031">simple</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1033" end_char="1044">incompetence</TOKEN>
<TOKEN id="token-11-18" pos="punct" morph="none" start_char="1045" end_char="1045">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1047" end_char="1171">
<ORIGINAL_TEXT>All it takes is an accidental spill, a torn shoe cover, a configuration error in the HVAC system, and we’re off to the races.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1047" end_char="1049">All</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1051" end_char="1052">it</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1054" end_char="1058">takes</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1060" end_char="1061">is</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1063" end_char="1064">an</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1066" end_char="1075">accidental</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1077" end_char="1081">spill</TOKEN>
<TOKEN id="token-12-7" pos="punct" morph="none" start_char="1082" end_char="1082">,</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1084" end_char="1084">a</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1086" end_char="1089">torn</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1091" end_char="1094">shoe</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1096" end_char="1100">cover</TOKEN>
<TOKEN id="token-12-12" pos="punct" morph="none" start_char="1101" end_char="1101">,</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1103" end_char="1103">a</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1105" end_char="1117">configuration</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1119" end_char="1123">error</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1125" end_char="1126">in</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1128" end_char="1130">the</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1132" end_char="1135">HVAC</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1137" end_char="1142">system</TOKEN>
<TOKEN id="token-12-20" pos="punct" morph="none" start_char="1143" end_char="1143">,</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1145" end_char="1147">and</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1149" end_char="1153">we’re</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1155" end_char="1157">off</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1159" end_char="1160">to</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1162" end_char="1164">the</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1166" end_char="1170">races</TOKEN>
<TOKEN id="token-12-27" pos="punct" morph="none" start_char="1171" end_char="1171">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1173" end_char="1236">
<ORIGINAL_TEXT>Containment, location, and monitoring need a lot more attention.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1173" end_char="1183">Containment</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="1184" end_char="1184">,</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1186" end_char="1193">location</TOKEN>
<TOKEN id="token-13-3" pos="punct" morph="none" start_char="1194" end_char="1194">,</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1196" end_char="1198">and</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1200" end_char="1209">monitoring</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1211" end_char="1214">need</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1216" end_char="1216">a</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1218" end_char="1220">lot</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1222" end_char="1225">more</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1227" end_char="1235">attention</TOKEN>
<TOKEN id="token-13-11" pos="punct" morph="none" start_char="1236" end_char="1236">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1240" end_char="1289">
<ORIGINAL_TEXT>No one said it would be a sane person or group.. .</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1240" end_char="1241">No</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1243" end_char="1245">one</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1247" end_char="1250">said</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1252" end_char="1253">it</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1255" end_char="1259">would</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1261" end_char="1262">be</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1264" end_char="1264">a</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1266" end_char="1269">sane</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1271" end_char="1276">person</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1278" end_char="1279">or</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1281" end_char="1285">group</TOKEN>
<TOKEN id="token-14-11" pos="punct" morph="none" start_char="1286" end_char="1287">..</TOKEN>
<TOKEN id="token-14-12" pos="punct" morph="none" start_char="1289" end_char="1289">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1291" end_char="1820">
<ORIGINAL_TEXT>but some think person/group/whatever would want to kill off a huge population of global humans and initiate economic disaster, globally to take over the world, over something that for U.S. figures is causing at this point 30x less at minimum projections to 60X less at high end projections of the yearly Influenza cases, which this year is projected at 39,000,000 to 56,000,000 in U.S. compared to 532,879 for Covid 19 and at possible 1/3 of the deaths 24,000 to 62,000 of the regular influenza this year, compared to… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1291" end_char="1293">but</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1295" end_char="1298">some</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1300" end_char="1304">think</TOKEN>
<TOKEN id="token-15-3" pos="unknown" morph="none" start_char="1306" end_char="1326">person/group/whatever</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1328" end_char="1332">would</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1334" end_char="1337">want</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1339" end_char="1340">to</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1342" end_char="1345">kill</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1347" end_char="1349">off</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1351" end_char="1351">a</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1353" end_char="1356">huge</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1358" end_char="1367">population</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1369" end_char="1370">of</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1372" end_char="1377">global</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1379" end_char="1384">humans</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1386" end_char="1388">and</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1390" end_char="1397">initiate</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1399" end_char="1406">economic</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1408" end_char="1415">disaster</TOKEN>
<TOKEN id="token-15-19" pos="punct" morph="none" start_char="1416" end_char="1416">,</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1418" end_char="1425">globally</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1427" end_char="1428">to</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1430" end_char="1433">take</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="1435" end_char="1438">over</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1440" end_char="1442">the</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="1444" end_char="1448">world</TOKEN>
<TOKEN id="token-15-26" pos="punct" morph="none" start_char="1449" end_char="1449">,</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="1451" end_char="1454">over</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="1456" end_char="1464">something</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="1466" end_char="1469">that</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="1471" end_char="1473">for</TOKEN>
<TOKEN id="token-15-31" pos="unknown" morph="none" start_char="1475" end_char="1477">U.S</TOKEN>
<TOKEN id="token-15-32" pos="punct" morph="none" start_char="1478" end_char="1478">.</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="1480" end_char="1486">figures</TOKEN>
<TOKEN id="token-15-34" pos="word" morph="none" start_char="1488" end_char="1489">is</TOKEN>
<TOKEN id="token-15-35" pos="word" morph="none" start_char="1491" end_char="1497">causing</TOKEN>
<TOKEN id="token-15-36" pos="word" morph="none" start_char="1499" end_char="1500">at</TOKEN>
<TOKEN id="token-15-37" pos="word" morph="none" start_char="1502" end_char="1505">this</TOKEN>
<TOKEN id="token-15-38" pos="word" morph="none" start_char="1507" end_char="1511">point</TOKEN>
<TOKEN id="token-15-39" pos="word" morph="none" start_char="1513" end_char="1515">30x</TOKEN>
<TOKEN id="token-15-40" pos="word" morph="none" start_char="1517" end_char="1520">less</TOKEN>
<TOKEN id="token-15-41" pos="word" morph="none" start_char="1522" end_char="1523">at</TOKEN>
<TOKEN id="token-15-42" pos="word" morph="none" start_char="1525" end_char="1531">minimum</TOKEN>
<TOKEN id="token-15-43" pos="word" morph="none" start_char="1533" end_char="1543">projections</TOKEN>
<TOKEN id="token-15-44" pos="word" morph="none" start_char="1545" end_char="1546">to</TOKEN>
<TOKEN id="token-15-45" pos="word" morph="none" start_char="1548" end_char="1550">60X</TOKEN>
<TOKEN id="token-15-46" pos="word" morph="none" start_char="1552" end_char="1555">less</TOKEN>
<TOKEN id="token-15-47" pos="word" morph="none" start_char="1557" end_char="1558">at</TOKEN>
<TOKEN id="token-15-48" pos="word" morph="none" start_char="1560" end_char="1563">high</TOKEN>
<TOKEN id="token-15-49" pos="word" morph="none" start_char="1565" end_char="1567">end</TOKEN>
<TOKEN id="token-15-50" pos="word" morph="none" start_char="1569" end_char="1579">projections</TOKEN>
<TOKEN id="token-15-51" pos="word" morph="none" start_char="1581" end_char="1582">of</TOKEN>
<TOKEN id="token-15-52" pos="word" morph="none" start_char="1584" end_char="1586">the</TOKEN>
<TOKEN id="token-15-53" pos="word" morph="none" start_char="1588" end_char="1593">yearly</TOKEN>
<TOKEN id="token-15-54" pos="word" morph="none" start_char="1595" end_char="1603">Influenza</TOKEN>
<TOKEN id="token-15-55" pos="word" morph="none" start_char="1605" end_char="1609">cases</TOKEN>
<TOKEN id="token-15-56" pos="punct" morph="none" start_char="1610" end_char="1610">,</TOKEN>
<TOKEN id="token-15-57" pos="word" morph="none" start_char="1612" end_char="1616">which</TOKEN>
<TOKEN id="token-15-58" pos="word" morph="none" start_char="1618" end_char="1621">this</TOKEN>
<TOKEN id="token-15-59" pos="word" morph="none" start_char="1623" end_char="1626">year</TOKEN>
<TOKEN id="token-15-60" pos="word" morph="none" start_char="1628" end_char="1629">is</TOKEN>
<TOKEN id="token-15-61" pos="word" morph="none" start_char="1631" end_char="1639">projected</TOKEN>
<TOKEN id="token-15-62" pos="word" morph="none" start_char="1641" end_char="1642">at</TOKEN>
<TOKEN id="token-15-63" pos="unknown" morph="none" start_char="1644" end_char="1653">39,000,000</TOKEN>
<TOKEN id="token-15-64" pos="word" morph="none" start_char="1655" end_char="1656">to</TOKEN>
<TOKEN id="token-15-65" pos="unknown" morph="none" start_char="1658" end_char="1667">56,000,000</TOKEN>
<TOKEN id="token-15-66" pos="word" morph="none" start_char="1669" end_char="1670">in</TOKEN>
<TOKEN id="token-15-67" pos="unknown" morph="none" start_char="1672" end_char="1674">U.S</TOKEN>
<TOKEN id="token-15-68" pos="punct" morph="none" start_char="1675" end_char="1675">.</TOKEN>
<TOKEN id="token-15-69" pos="word" morph="none" start_char="1677" end_char="1684">compared</TOKEN>
<TOKEN id="token-15-70" pos="word" morph="none" start_char="1686" end_char="1687">to</TOKEN>
<TOKEN id="token-15-71" pos="unknown" morph="none" start_char="1689" end_char="1695">532,879</TOKEN>
<TOKEN id="token-15-72" pos="word" morph="none" start_char="1697" end_char="1699">for</TOKEN>
<TOKEN id="token-15-73" pos="word" morph="none" start_char="1701" end_char="1705">Covid</TOKEN>
<TOKEN id="token-15-74" pos="word" morph="none" start_char="1707" end_char="1708">19</TOKEN>
<TOKEN id="token-15-75" pos="word" morph="none" start_char="1710" end_char="1712">and</TOKEN>
<TOKEN id="token-15-76" pos="word" morph="none" start_char="1714" end_char="1715">at</TOKEN>
<TOKEN id="token-15-77" pos="word" morph="none" start_char="1717" end_char="1724">possible</TOKEN>
<TOKEN id="token-15-78" pos="unknown" morph="none" start_char="1726" end_char="1728">1/3</TOKEN>
<TOKEN id="token-15-79" pos="word" morph="none" start_char="1730" end_char="1731">of</TOKEN>
<TOKEN id="token-15-80" pos="word" morph="none" start_char="1733" end_char="1735">the</TOKEN>
<TOKEN id="token-15-81" pos="word" morph="none" start_char="1737" end_char="1742">deaths</TOKEN>
<TOKEN id="token-15-82" pos="unknown" morph="none" start_char="1744" end_char="1749">24,000</TOKEN>
<TOKEN id="token-15-83" pos="word" morph="none" start_char="1751" end_char="1752">to</TOKEN>
<TOKEN id="token-15-84" pos="unknown" morph="none" start_char="1754" end_char="1759">62,000</TOKEN>
<TOKEN id="token-15-85" pos="word" morph="none" start_char="1761" end_char="1762">of</TOKEN>
<TOKEN id="token-15-86" pos="word" morph="none" start_char="1764" end_char="1766">the</TOKEN>
<TOKEN id="token-15-87" pos="word" morph="none" start_char="1768" end_char="1774">regular</TOKEN>
<TOKEN id="token-15-88" pos="word" morph="none" start_char="1776" end_char="1784">influenza</TOKEN>
<TOKEN id="token-15-89" pos="word" morph="none" start_char="1786" end_char="1789">this</TOKEN>
<TOKEN id="token-15-90" pos="word" morph="none" start_char="1791" end_char="1794">year</TOKEN>
<TOKEN id="token-15-91" pos="punct" morph="none" start_char="1795" end_char="1795">,</TOKEN>
<TOKEN id="token-15-92" pos="word" morph="none" start_char="1797" end_char="1804">compared</TOKEN>
<TOKEN id="token-15-93" pos="word" morph="none" start_char="1806" end_char="1807">to</TOKEN>
<TOKEN id="token-15-94" pos="punct" morph="none" start_char="1808" end_char="1808">…</TOKEN>
<TOKEN id="token-15-95" pos="word" morph="none" start_char="1810" end_char="1813">Read</TOKEN>
<TOKEN id="token-15-96" pos="word" morph="none" start_char="1815" end_char="1818">more</TOKEN>
<TOKEN id="token-15-97" pos="punct" morph="none" start_char="1820" end_char="1820">»</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1824" end_char="1854">
<ORIGINAL_TEXT>Because Covid19 IS NOT THE FLU.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1824" end_char="1830">Because</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1832" end_char="1838">Covid19</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1840" end_char="1841">IS</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1843" end_char="1845">NOT</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1847" end_char="1849">THE</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1851" end_char="1853">FLU</TOKEN>
<TOKEN id="token-16-6" pos="punct" morph="none" start_char="1854" end_char="1854">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1856" end_char="1976">
<ORIGINAL_TEXT>It is more dangerous and more contagious, therefore, if we didn’t do a lockdown, the numbers would be worse than the flu.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1856" end_char="1857">It</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1859" end_char="1860">is</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1862" end_char="1865">more</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1867" end_char="1875">dangerous</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1877" end_char="1879">and</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1881" end_char="1884">more</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1886" end_char="1895">contagious</TOKEN>
<TOKEN id="token-17-7" pos="punct" morph="none" start_char="1896" end_char="1896">,</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1898" end_char="1906">therefore</TOKEN>
<TOKEN id="token-17-9" pos="punct" morph="none" start_char="1907" end_char="1907">,</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1909" end_char="1910">if</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="1912" end_char="1913">we</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="1915" end_char="1920">didn’t</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1922" end_char="1923">do</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1925" end_char="1925">a</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1927" end_char="1934">lockdown</TOKEN>
<TOKEN id="token-17-16" pos="punct" morph="none" start_char="1935" end_char="1935">,</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1937" end_char="1939">the</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1941" end_char="1947">numbers</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="1949" end_char="1953">would</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="1955" end_char="1956">be</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="1958" end_char="1962">worse</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="1964" end_char="1967">than</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="1969" end_char="1971">the</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="1973" end_char="1975">flu</TOKEN>
<TOKEN id="token-17-25" pos="punct" morph="none" start_char="1976" end_char="1976">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1978" end_char="2043">
<ORIGINAL_TEXT>The numbers are low because of the lockdown and social distancing.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1978" end_char="1980">The</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1982" end_char="1988">numbers</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1990" end_char="1992">are</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1994" end_char="1996">low</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1998" end_char="2004">because</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2006" end_char="2007">of</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2009" end_char="2011">the</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2013" end_char="2020">lockdown</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2022" end_char="2024">and</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2026" end_char="2031">social</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2033" end_char="2042">distancing</TOKEN>
<TOKEN id="token-18-11" pos="punct" morph="none" start_char="2043" end_char="2043">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2047" end_char="2120">
<ORIGINAL_TEXT>It seems the theory lab leakage , accidental or deliberate need more study</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2047" end_char="2048">It</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2050" end_char="2054">seems</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2056" end_char="2058">the</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2060" end_char="2065">theory</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2067" end_char="2069">lab</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2071" end_char="2077">leakage</TOKEN>
<TOKEN id="token-19-6" pos="punct" morph="none" start_char="2079" end_char="2079">,</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2081" end_char="2090">accidental</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2092" end_char="2093">or</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2095" end_char="2104">deliberate</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2106" end_char="2109">need</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2111" end_char="2114">more</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2116" end_char="2120">study</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2124" end_char="2209">
<ORIGINAL_TEXT>I don’t think they are implying it was an intentional leak but moresso a lab accident.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2124" end_char="2124">I</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2126" end_char="2130">don’t</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2132" end_char="2136">think</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2138" end_char="2141">they</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2143" end_char="2145">are</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2147" end_char="2154">implying</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2156" end_char="2157">it</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2159" end_char="2161">was</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2163" end_char="2164">an</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2166" end_char="2176">intentional</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2178" end_char="2181">leak</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2183" end_char="2185">but</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2187" end_char="2193">moresso</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2195" end_char="2195">a</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2197" end_char="2199">lab</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2201" end_char="2208">accident</TOKEN>
<TOKEN id="token-20-16" pos="punct" morph="none" start_char="2209" end_char="2209">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2211" end_char="2332">
<ORIGINAL_TEXT>The most intriguing part to me is the fact that a lab worker/doctor was found guilty selling test animals to meat markets.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2211" end_char="2213">The</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2215" end_char="2218">most</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2220" end_char="2229">intriguing</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2231" end_char="2234">part</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2236" end_char="2237">to</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2239" end_char="2240">me</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2242" end_char="2243">is</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2245" end_char="2247">the</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2249" end_char="2252">fact</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2254" end_char="2257">that</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2259" end_char="2259">a</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2261" end_char="2263">lab</TOKEN>
<TOKEN id="token-21-12" pos="unknown" morph="none" start_char="2265" end_char="2277">worker/doctor</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2279" end_char="2281">was</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2283" end_char="2287">found</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="2289" end_char="2294">guilty</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2296" end_char="2302">selling</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="2304" end_char="2307">test</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2309" end_char="2315">animals</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="2317" end_char="2318">to</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2320" end_char="2323">meat</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2325" end_char="2331">markets</TOKEN>
<TOKEN id="token-21-22" pos="punct" morph="none" start_char="2332" end_char="2332">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2334" end_char="2390">
<ORIGINAL_TEXT>That could have been a way it became prevalent in humans.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2334" end_char="2337">That</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2339" end_char="2343">could</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2345" end_char="2348">have</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2350" end_char="2353">been</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2355" end_char="2355">a</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2357" end_char="2359">way</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2361" end_char="2362">it</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2364" end_char="2369">became</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2371" end_char="2379">prevalent</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2381" end_char="2382">in</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2384" end_char="2389">humans</TOKEN>
<TOKEN id="token-22-11" pos="punct" morph="none" start_char="2390" end_char="2390">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2394" end_char="2450">
<ORIGINAL_TEXT>Can I have a source for that report of them being guilty?</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2394" end_char="2396">Can</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2398" end_char="2398">I</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2400" end_char="2403">have</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2405" end_char="2405">a</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2407" end_char="2412">source</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2414" end_char="2416">for</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2418" end_char="2421">that</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2423" end_char="2428">report</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2430" end_char="2431">of</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2433" end_char="2436">them</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2438" end_char="2442">being</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="2444" end_char="2449">guilty</TOKEN>
<TOKEN id="token-23-12" pos="punct" morph="none" start_char="2450" end_char="2450">?</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2454" end_char="2841">
<ORIGINAL_TEXT>Dr Li Ning, an expert in cloning and former director of the State Key Laboratory of Agrobiotechnology, was found guilty of illegally transferring the funds in the form of "investments" to several companies he controlled, though there was no evidence he spent any of the money on himself, the Intermediate People’s Court of Songyuan in northeast China’s Jilin province said in its verdict.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2454" end_char="2455">Dr</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2457" end_char="2458">Li</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2460" end_char="2463">Ning</TOKEN>
<TOKEN id="token-24-3" pos="punct" morph="none" start_char="2464" end_char="2464">,</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="2466" end_char="2467">an</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2469" end_char="2474">expert</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2476" end_char="2477">in</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="2479" end_char="2485">cloning</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="2487" end_char="2489">and</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="2491" end_char="2496">former</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2498" end_char="2505">director</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2507" end_char="2508">of</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2510" end_char="2512">the</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="2514" end_char="2518">State</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="2520" end_char="2522">Key</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="2524" end_char="2533">Laboratory</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="2535" end_char="2536">of</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="2538" end_char="2554">Agrobiotechnology</TOKEN>
<TOKEN id="token-24-18" pos="punct" morph="none" start_char="2555" end_char="2555">,</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="2557" end_char="2559">was</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="2561" end_char="2565">found</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="2567" end_char="2572">guilty</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="2574" end_char="2575">of</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="2577" end_char="2585">illegally</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="2587" end_char="2598">transferring</TOKEN>
<TOKEN id="token-24-25" pos="word" morph="none" start_char="2600" end_char="2602">the</TOKEN>
<TOKEN id="token-24-26" pos="word" morph="none" start_char="2604" end_char="2608">funds</TOKEN>
<TOKEN id="token-24-27" pos="word" morph="none" start_char="2610" end_char="2611">in</TOKEN>
<TOKEN id="token-24-28" pos="word" morph="none" start_char="2613" end_char="2615">the</TOKEN>
<TOKEN id="token-24-29" pos="word" morph="none" start_char="2617" end_char="2620">form</TOKEN>
<TOKEN id="token-24-30" pos="word" morph="none" start_char="2622" end_char="2623">of</TOKEN>
<TOKEN id="token-24-31" pos="punct" morph="none" start_char="2625" end_char="2625">"</TOKEN>
<TOKEN id="token-24-32" pos="word" morph="none" start_char="2626" end_char="2636">investments</TOKEN>
<TOKEN id="token-24-33" pos="punct" morph="none" start_char="2637" end_char="2637">"</TOKEN>
<TOKEN id="token-24-34" pos="word" morph="none" start_char="2639" end_char="2640">to</TOKEN>
<TOKEN id="token-24-35" pos="word" morph="none" start_char="2642" end_char="2648">several</TOKEN>
<TOKEN id="token-24-36" pos="word" morph="none" start_char="2650" end_char="2658">companies</TOKEN>
<TOKEN id="token-24-37" pos="word" morph="none" start_char="2660" end_char="2661">he</TOKEN>
<TOKEN id="token-24-38" pos="word" morph="none" start_char="2663" end_char="2672">controlled</TOKEN>
<TOKEN id="token-24-39" pos="punct" morph="none" start_char="2673" end_char="2673">,</TOKEN>
<TOKEN id="token-24-40" pos="word" morph="none" start_char="2675" end_char="2680">though</TOKEN>
<TOKEN id="token-24-41" pos="word" morph="none" start_char="2682" end_char="2686">there</TOKEN>
<TOKEN id="token-24-42" pos="word" morph="none" start_char="2688" end_char="2690">was</TOKEN>
<TOKEN id="token-24-43" pos="word" morph="none" start_char="2692" end_char="2693">no</TOKEN>
<TOKEN id="token-24-44" pos="word" morph="none" start_char="2695" end_char="2702">evidence</TOKEN>
<TOKEN id="token-24-45" pos="word" morph="none" start_char="2704" end_char="2705">he</TOKEN>
<TOKEN id="token-24-46" pos="word" morph="none" start_char="2707" end_char="2711">spent</TOKEN>
<TOKEN id="token-24-47" pos="word" morph="none" start_char="2713" end_char="2715">any</TOKEN>
<TOKEN id="token-24-48" pos="word" morph="none" start_char="2717" end_char="2718">of</TOKEN>
<TOKEN id="token-24-49" pos="word" morph="none" start_char="2720" end_char="2722">the</TOKEN>
<TOKEN id="token-24-50" pos="word" morph="none" start_char="2724" end_char="2728">money</TOKEN>
<TOKEN id="token-24-51" pos="word" morph="none" start_char="2730" end_char="2731">on</TOKEN>
<TOKEN id="token-24-52" pos="word" morph="none" start_char="2733" end_char="2739">himself</TOKEN>
<TOKEN id="token-24-53" pos="punct" morph="none" start_char="2740" end_char="2740">,</TOKEN>
<TOKEN id="token-24-54" pos="word" morph="none" start_char="2742" end_char="2744">the</TOKEN>
<TOKEN id="token-24-55" pos="word" morph="none" start_char="2746" end_char="2757">Intermediate</TOKEN>
<TOKEN id="token-24-56" pos="word" morph="none" start_char="2759" end_char="2766">People’s</TOKEN>
<TOKEN id="token-24-57" pos="word" morph="none" start_char="2768" end_char="2772">Court</TOKEN>
<TOKEN id="token-24-58" pos="word" morph="none" start_char="2774" end_char="2775">of</TOKEN>
<TOKEN id="token-24-59" pos="word" morph="none" start_char="2777" end_char="2784">Songyuan</TOKEN>
<TOKEN id="token-24-60" pos="word" morph="none" start_char="2786" end_char="2787">in</TOKEN>
<TOKEN id="token-24-61" pos="word" morph="none" start_char="2789" end_char="2797">northeast</TOKEN>
<TOKEN id="token-24-62" pos="word" morph="none" start_char="2799" end_char="2805">China’s</TOKEN>
<TOKEN id="token-24-63" pos="word" morph="none" start_char="2807" end_char="2811">Jilin</TOKEN>
<TOKEN id="token-24-64" pos="word" morph="none" start_char="2813" end_char="2820">province</TOKEN>
<TOKEN id="token-24-65" pos="word" morph="none" start_char="2822" end_char="2825">said</TOKEN>
<TOKEN id="token-24-66" pos="word" morph="none" start_char="2827" end_char="2828">in</TOKEN>
<TOKEN id="token-24-67" pos="word" morph="none" start_char="2830" end_char="2832">its</TOKEN>
<TOKEN id="token-24-68" pos="word" morph="none" start_char="2834" end_char="2840">verdict</TOKEN>
<TOKEN id="token-24-69" pos="punct" morph="none" start_char="2841" end_char="2841">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2844" end_char="3009">
<ORIGINAL_TEXT>Li was also fined 3 million yuan, while his assistant, Dr Zhang Lei, was sentenced to five years and eight months in prison and fined 200,000 yuan on the same charge.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2844" end_char="2845">Li</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2847" end_char="2849">was</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2851" end_char="2854">also</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2856" end_char="2860">fined</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2862" end_char="2862">3</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="2864" end_char="2870">million</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="2872" end_char="2875">yuan</TOKEN>
<TOKEN id="token-25-7" pos="punct" morph="none" start_char="2876" end_char="2876">,</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="2878" end_char="2882">while</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="2884" end_char="2886">his</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="2888" end_char="2896">assistant</TOKEN>
<TOKEN id="token-25-11" pos="punct" morph="none" start_char="2897" end_char="2897">,</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="2899" end_char="2900">Dr</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="2902" end_char="2906">Zhang</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="2908" end_char="2910">Lei</TOKEN>
<TOKEN id="token-25-15" pos="punct" morph="none" start_char="2911" end_char="2911">,</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="2913" end_char="2915">was</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="2917" end_char="2925">sentenced</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="2927" end_char="2928">to</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="2930" end_char="2933">five</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="2935" end_char="2939">years</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="2941" end_char="2943">and</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="2945" end_char="2949">eight</TOKEN>
<TOKEN id="token-25-23" pos="word" morph="none" start_char="2951" end_char="2956">months</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="2958" end_char="2959">in</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="2961" end_char="2966">prison</TOKEN>
<TOKEN id="token-25-26" pos="word" morph="none" start_char="2968" end_char="2970">and</TOKEN>
<TOKEN id="token-25-27" pos="word" morph="none" start_char="2972" end_char="2976">fined</TOKEN>
<TOKEN id="token-25-28" pos="unknown" morph="none" start_char="2978" end_char="2984">200,000</TOKEN>
<TOKEN id="token-25-29" pos="word" morph="none" start_char="2986" end_char="2989">yuan</TOKEN>
<TOKEN id="token-25-30" pos="word" morph="none" start_char="2991" end_char="2992">on</TOKEN>
<TOKEN id="token-25-31" pos="word" morph="none" start_char="2994" end_char="2996">the</TOKEN>
<TOKEN id="token-25-32" pos="word" morph="none" start_char="2998" end_char="3001">same</TOKEN>
<TOKEN id="token-25-33" pos="word" morph="none" start_char="3003" end_char="3008">charge</TOKEN>
<TOKEN id="token-25-34" pos="punct" morph="none" start_char="3009" end_char="3009">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3012" end_char="3150">
<ORIGINAL_TEXT>https://www.google.com/amp/s/amp.scmp.com/news/china/society/article/3044556/chinese-scientist-li-ning-gets-12-years-prison-embezzling-us43</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="url" morph="none" start_char="3012" end_char="3150">https://www.google.com/amp/s/amp.scmp.com/news/china/society/article/3044556/chinese-scientist-li-ning-gets-12-years-prison-embezzling-us43</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3154" end_char="3205">
<ORIGINAL_TEXT>Good points but i disagree with the final statement.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3154" end_char="3157">Good</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="3159" end_char="3164">points</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="3166" end_char="3168">but</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3170" end_char="3170">i</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3172" end_char="3179">disagree</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="3181" end_char="3184">with</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3186" end_char="3188">the</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3190" end_char="3194">final</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3196" end_char="3204">statement</TOKEN>
<TOKEN id="token-27-9" pos="punct" morph="none" start_char="3205" end_char="3205">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3207" end_char="3413">
<ORIGINAL_TEXT>We all know that high containment labs need to be better managed as our own side has several more documented instances of accidents/violations than mentioned in this article, human nature being human nature.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="3207" end_char="3208">We</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="3210" end_char="3212">all</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3214" end_char="3217">know</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3219" end_char="3222">that</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3224" end_char="3227">high</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="3229" end_char="3239">containment</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3241" end_char="3244">labs</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="3246" end_char="3249">need</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3251" end_char="3252">to</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3254" end_char="3255">be</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3257" end_char="3262">better</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="3264" end_char="3270">managed</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3272" end_char="3273">as</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="3275" end_char="3277">our</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="3279" end_char="3281">own</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="3283" end_char="3286">side</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="3288" end_char="3290">has</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="3292" end_char="3298">several</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="3300" end_char="3303">more</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="3305" end_char="3314">documented</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="3316" end_char="3324">instances</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="3326" end_char="3327">of</TOKEN>
<TOKEN id="token-28-22" pos="unknown" morph="none" start_char="3329" end_char="3348">accidents/violations</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="3350" end_char="3353">than</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="3355" end_char="3363">mentioned</TOKEN>
<TOKEN id="token-28-25" pos="word" morph="none" start_char="3365" end_char="3366">in</TOKEN>
<TOKEN id="token-28-26" pos="word" morph="none" start_char="3368" end_char="3371">this</TOKEN>
<TOKEN id="token-28-27" pos="word" morph="none" start_char="3373" end_char="3379">article</TOKEN>
<TOKEN id="token-28-28" pos="punct" morph="none" start_char="3380" end_char="3380">,</TOKEN>
<TOKEN id="token-28-29" pos="word" morph="none" start_char="3382" end_char="3386">human</TOKEN>
<TOKEN id="token-28-30" pos="word" morph="none" start_char="3388" end_char="3393">nature</TOKEN>
<TOKEN id="token-28-31" pos="word" morph="none" start_char="3395" end_char="3399">being</TOKEN>
<TOKEN id="token-28-32" pos="word" morph="none" start_char="3401" end_char="3405">human</TOKEN>
<TOKEN id="token-28-33" pos="word" morph="none" start_char="3407" end_char="3412">nature</TOKEN>
<TOKEN id="token-28-34" pos="punct" morph="none" start_char="3413" end_char="3413">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3415" end_char="3455">
<ORIGINAL_TEXT>AI will no doubt help with those matters.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3415" end_char="3416">AI</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3418" end_char="3421">will</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3423" end_char="3424">no</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3426" end_char="3430">doubt</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="3432" end_char="3435">help</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="3437" end_char="3440">with</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="3442" end_char="3446">those</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="3448" end_char="3454">matters</TOKEN>
<TOKEN id="token-29-8" pos="punct" morph="none" start_char="3455" end_char="3455">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3457" end_char="3569">
<ORIGINAL_TEXT>But determining the source, assigning blame, matters less than realizing that this will not be the last pandemic.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3457" end_char="3459">But</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3461" end_char="3471">determining</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3473" end_char="3475">the</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="3477" end_char="3482">source</TOKEN>
<TOKEN id="token-30-4" pos="punct" morph="none" start_char="3483" end_char="3483">,</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3485" end_char="3493">assigning</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3495" end_char="3499">blame</TOKEN>
<TOKEN id="token-30-7" pos="punct" morph="none" start_char="3500" end_char="3500">,</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="3502" end_char="3508">matters</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="3510" end_char="3513">less</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="3515" end_char="3518">than</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="3520" end_char="3528">realizing</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="3530" end_char="3533">that</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="3535" end_char="3538">this</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="3540" end_char="3543">will</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="3545" end_char="3547">not</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="3549" end_char="3550">be</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="3552" end_char="3554">the</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="3556" end_char="3559">last</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="3561" end_char="3568">pandemic</TOKEN>
<TOKEN id="token-30-20" pos="punct" morph="none" start_char="3569" end_char="3569">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3571" end_char="3756">
<ORIGINAL_TEXT>We need to devote our resources into strengthening defenses against pandemics and understanding the thousands of potential human pathogenic viruses, how they work and how to defeat them.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="3571" end_char="3572">We</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="3574" end_char="3577">need</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="3579" end_char="3580">to</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="3582" end_char="3587">devote</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3589" end_char="3591">our</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3593" end_char="3601">resources</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3603" end_char="3606">into</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3608" end_char="3620">strengthening</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3622" end_char="3629">defenses</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="3631" end_char="3637">against</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3639" end_char="3647">pandemics</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="3649" end_char="3651">and</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="3653" end_char="3665">understanding</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="3667" end_char="3669">the</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="3671" end_char="3679">thousands</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="3681" end_char="3682">of</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="3684" end_char="3692">potential</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="3694" end_char="3698">human</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="3700" end_char="3709">pathogenic</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="3711" end_char="3717">viruses</TOKEN>
<TOKEN id="token-31-20" pos="punct" morph="none" start_char="3718" end_char="3718">,</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="3720" end_char="3722">how</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="3724" end_char="3727">they</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="3729" end_char="3732">work</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="3734" end_char="3736">and</TOKEN>
<TOKEN id="token-31-25" pos="word" morph="none" start_char="3738" end_char="3740">how</TOKEN>
<TOKEN id="token-31-26" pos="word" morph="none" start_char="3742" end_char="3743">to</TOKEN>
<TOKEN id="token-31-27" pos="word" morph="none" start_char="3745" end_char="3750">defeat</TOKEN>
<TOKEN id="token-31-28" pos="word" morph="none" start_char="3752" end_char="3755">them</TOKEN>
<TOKEN id="token-31-29" pos="punct" morph="none" start_char="3756" end_char="3756">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3758" end_char="3792">
<ORIGINAL_TEXT>This endeavor will not… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3758" end_char="3761">This</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="3763" end_char="3770">endeavor</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3772" end_char="3775">will</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="3777" end_char="3779">not</TOKEN>
<TOKEN id="token-32-4" pos="punct" morph="none" start_char="3780" end_char="3780">…</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3782" end_char="3785">Read</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3787" end_char="3790">more</TOKEN>
<TOKEN id="token-32-7" pos="punct" morph="none" start_char="3792" end_char="3792">»</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3796" end_char="3904">
<ORIGINAL_TEXT>absolutely…and yet we see populism and all the bright side of humanity fully at work … in the other direction</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="unknown" morph="none" start_char="3796" end_char="3809">absolutely…and</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3811" end_char="3813">yet</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="3815" end_char="3816">we</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="3818" end_char="3820">see</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="3822" end_char="3829">populism</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="3831" end_char="3833">and</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="3835" end_char="3837">all</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="3839" end_char="3841">the</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="3843" end_char="3848">bright</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="3850" end_char="3853">side</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="3855" end_char="3856">of</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="3858" end_char="3865">humanity</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="3867" end_char="3871">fully</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="3873" end_char="3874">at</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="3876" end_char="3879">work</TOKEN>
<TOKEN id="token-33-15" pos="punct" morph="none" start_char="3881" end_char="3881">…</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="3883" end_char="3884">in</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="3886" end_char="3888">the</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="3890" end_char="3894">other</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="3896" end_char="3904">direction</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3908" end_char="4136">
<ORIGINAL_TEXT>I agree "We all know that high containment labs need to be better managed" and we cannot expect the people working in those labs, and risking causing a pandemic, to be the same people who will suddenly start behaving responsibly.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3908" end_char="3908">I</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="3910" end_char="3914">agree</TOKEN>
<TOKEN id="token-34-2" pos="punct" morph="none" start_char="3916" end_char="3916">"</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3917" end_char="3918">We</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="3920" end_char="3922">all</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="3924" end_char="3927">know</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="3929" end_char="3932">that</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="3934" end_char="3937">high</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="3939" end_char="3949">containment</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="3951" end_char="3954">labs</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="3956" end_char="3959">need</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="3961" end_char="3962">to</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="3964" end_char="3965">be</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="3967" end_char="3972">better</TOKEN>
<TOKEN id="token-34-14" pos="word" morph="none" start_char="3974" end_char="3980">managed</TOKEN>
<TOKEN id="token-34-15" pos="punct" morph="none" start_char="3981" end_char="3981">"</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="3983" end_char="3985">and</TOKEN>
<TOKEN id="token-34-17" pos="word" morph="none" start_char="3987" end_char="3988">we</TOKEN>
<TOKEN id="token-34-18" pos="word" morph="none" start_char="3990" end_char="3995">cannot</TOKEN>
<TOKEN id="token-34-19" pos="word" morph="none" start_char="3997" end_char="4002">expect</TOKEN>
<TOKEN id="token-34-20" pos="word" morph="none" start_char="4004" end_char="4006">the</TOKEN>
<TOKEN id="token-34-21" pos="word" morph="none" start_char="4008" end_char="4013">people</TOKEN>
<TOKEN id="token-34-22" pos="word" morph="none" start_char="4015" end_char="4021">working</TOKEN>
<TOKEN id="token-34-23" pos="word" morph="none" start_char="4023" end_char="4024">in</TOKEN>
<TOKEN id="token-34-24" pos="word" morph="none" start_char="4026" end_char="4030">those</TOKEN>
<TOKEN id="token-34-25" pos="word" morph="none" start_char="4032" end_char="4035">labs</TOKEN>
<TOKEN id="token-34-26" pos="punct" morph="none" start_char="4036" end_char="4036">,</TOKEN>
<TOKEN id="token-34-27" pos="word" morph="none" start_char="4038" end_char="4040">and</TOKEN>
<TOKEN id="token-34-28" pos="word" morph="none" start_char="4042" end_char="4048">risking</TOKEN>
<TOKEN id="token-34-29" pos="word" morph="none" start_char="4050" end_char="4056">causing</TOKEN>
<TOKEN id="token-34-30" pos="word" morph="none" start_char="4058" end_char="4058">a</TOKEN>
<TOKEN id="token-34-31" pos="word" morph="none" start_char="4060" end_char="4067">pandemic</TOKEN>
<TOKEN id="token-34-32" pos="punct" morph="none" start_char="4068" end_char="4068">,</TOKEN>
<TOKEN id="token-34-33" pos="word" morph="none" start_char="4070" end_char="4071">to</TOKEN>
<TOKEN id="token-34-34" pos="word" morph="none" start_char="4073" end_char="4074">be</TOKEN>
<TOKEN id="token-34-35" pos="word" morph="none" start_char="4076" end_char="4078">the</TOKEN>
<TOKEN id="token-34-36" pos="word" morph="none" start_char="4080" end_char="4083">same</TOKEN>
<TOKEN id="token-34-37" pos="word" morph="none" start_char="4085" end_char="4090">people</TOKEN>
<TOKEN id="token-34-38" pos="word" morph="none" start_char="4092" end_char="4094">who</TOKEN>
<TOKEN id="token-34-39" pos="word" morph="none" start_char="4096" end_char="4099">will</TOKEN>
<TOKEN id="token-34-40" pos="word" morph="none" start_char="4101" end_char="4108">suddenly</TOKEN>
<TOKEN id="token-34-41" pos="word" morph="none" start_char="4110" end_char="4114">start</TOKEN>
<TOKEN id="token-34-42" pos="word" morph="none" start_char="4116" end_char="4123">behaving</TOKEN>
<TOKEN id="token-34-43" pos="word" morph="none" start_char="4125" end_char="4135">responsibly</TOKEN>
<TOKEN id="token-34-44" pos="punct" morph="none" start_char="4136" end_char="4136">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4138" end_char="4261">
<ORIGINAL_TEXT>It is laboratories in the USA (and France) that trained some of the scientists in the Wuhan labs working with coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4138" end_char="4139">It</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="4141" end_char="4142">is</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4144" end_char="4155">laboratories</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="4157" end_char="4158">in</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4160" end_char="4162">the</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="4164" end_char="4166">USA</TOKEN>
<TOKEN id="token-35-6" pos="punct" morph="none" start_char="4168" end_char="4168">(</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="4169" end_char="4171">and</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="4173" end_char="4178">France</TOKEN>
<TOKEN id="token-35-9" pos="punct" morph="none" start_char="4179" end_char="4179">)</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="4181" end_char="4184">that</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="4186" end_char="4192">trained</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="4194" end_char="4197">some</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="4199" end_char="4200">of</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="4202" end_char="4204">the</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="4206" end_char="4215">scientists</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="4217" end_char="4218">in</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="4220" end_char="4222">the</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="4224" end_char="4228">Wuhan</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="4230" end_char="4233">labs</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="4235" end_char="4241">working</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="4243" end_char="4246">with</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="4248" end_char="4260">coronaviruses</TOKEN>
<TOKEN id="token-35-23" pos="punct" morph="none" start_char="4261" end_char="4261">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="4263" end_char="4310">
<ORIGINAL_TEXT>There is already too much trust and cooperation!</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="4263" end_char="4267">There</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="4269" end_char="4270">is</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="4272" end_char="4278">already</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="4280" end_char="4282">too</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="4284" end_char="4287">much</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="4289" end_char="4293">trust</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="4295" end_char="4297">and</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="4299" end_char="4309">cooperation</TOKEN>
<TOKEN id="token-36-8" pos="punct" morph="none" start_char="4310" end_char="4310">!</TOKEN>
</SEG>
<SEG id="segment-37" start_char="4312" end_char="4453">
<ORIGINAL_TEXT>The scientific community needs oversight from people who are not building their careers through the technologies they are claiming to oversee.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="4312" end_char="4314">The</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="4316" end_char="4325">scientific</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="4327" end_char="4335">community</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4337" end_char="4341">needs</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4343" end_char="4351">oversight</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4353" end_char="4356">from</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="4358" end_char="4363">people</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4365" end_char="4367">who</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4369" end_char="4371">are</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4373" end_char="4375">not</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="4377" end_char="4384">building</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="4386" end_char="4390">their</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="4392" end_char="4398">careers</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="4400" end_char="4406">through</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="4408" end_char="4410">the</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="4412" end_char="4423">technologies</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="4425" end_char="4428">they</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="4430" end_char="4432">are</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="4434" end_char="4441">claiming</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="4443" end_char="4444">to</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="4446" end_char="4452">oversee</TOKEN>
<TOKEN id="token-37-21" pos="punct" morph="none" start_char="4453" end_char="4453">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4455" end_char="4546">
<ORIGINAL_TEXT>For example, gain-of-function research on deadly viruses in unsafe laboratories… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4455" end_char="4457">For</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4459" end_char="4465">example</TOKEN>
<TOKEN id="token-38-2" pos="punct" morph="none" start_char="4466" end_char="4466">,</TOKEN>
<TOKEN id="token-38-3" pos="unknown" morph="none" start_char="4468" end_char="4483">gain-of-function</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4485" end_char="4492">research</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4494" end_char="4495">on</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4497" end_char="4502">deadly</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4504" end_char="4510">viruses</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4512" end_char="4513">in</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4515" end_char="4520">unsafe</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="4522" end_char="4533">laboratories</TOKEN>
<TOKEN id="token-38-11" pos="punct" morph="none" start_char="4534" end_char="4534">…</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4536" end_char="4539">Read</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="4541" end_char="4544">more</TOKEN>
<TOKEN id="token-38-14" pos="punct" morph="none" start_char="4546" end_char="4546">»</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4550" end_char="4606">
<ORIGINAL_TEXT>Yes, cooperation with the Chinese scientist is important.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4550" end_char="4552">Yes</TOKEN>
<TOKEN id="token-39-1" pos="punct" morph="none" start_char="4553" end_char="4553">,</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4555" end_char="4565">cooperation</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4567" end_char="4570">with</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="4572" end_char="4574">the</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4576" end_char="4582">Chinese</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="4584" end_char="4592">scientist</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="4594" end_char="4595">is</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="4597" end_char="4605">important</TOKEN>
<TOKEN id="token-39-9" pos="punct" morph="none" start_char="4606" end_char="4606">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4608" end_char="4727">
<ORIGINAL_TEXT>But first we have to get over the hurdle of the Chinese government that doesn’t allow for free speech and communication.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="4608" end_char="4610">But</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4612" end_char="4616">first</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="4618" end_char="4619">we</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4621" end_char="4624">have</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4626" end_char="4627">to</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4629" end_char="4631">get</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4633" end_char="4636">over</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4638" end_char="4640">the</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="4642" end_char="4647">hurdle</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4649" end_char="4650">of</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="4652" end_char="4654">the</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="4656" end_char="4662">Chinese</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="4664" end_char="4673">government</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="4675" end_char="4678">that</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="4680" end_char="4686">doesn’t</TOKEN>
<TOKEN id="token-40-15" pos="word" morph="none" start_char="4688" end_char="4692">allow</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="4694" end_char="4696">for</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="4698" end_char="4701">free</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="4703" end_char="4708">speech</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="4710" end_char="4712">and</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="4714" end_char="4726">communication</TOKEN>
<TOKEN id="token-40-21" pos="punct" morph="none" start_char="4727" end_char="4727">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4729" end_char="4778">
<ORIGINAL_TEXT>Until that happens there will always be mysteries.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4729" end_char="4733">Until</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="4735" end_char="4738">that</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4740" end_char="4746">happens</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4748" end_char="4752">there</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4754" end_char="4757">will</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="4759" end_char="4764">always</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="4766" end_char="4767">be</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="4769" end_char="4777">mysteries</TOKEN>
<TOKEN id="token-41-8" pos="punct" morph="none" start_char="4778" end_char="4778">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="4782" end_char="4793">
<ORIGINAL_TEXT>And US does?</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="4782" end_char="4784">And</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="4786" end_char="4787">US</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="4789" end_char="4792">does</TOKEN>
<TOKEN id="token-42-3" pos="punct" morph="none" start_char="4793" end_char="4793">?</TOKEN>
</SEG>
<SEG id="segment-43" start_char="4795" end_char="4938">
<ORIGINAL_TEXT>It seems far more of a cacophony in place of information, and where information is needed (from the White House), there’s foolhardy podium-love.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="4795" end_char="4796">It</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="4798" end_char="4802">seems</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="4804" end_char="4806">far</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="4808" end_char="4811">more</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="4813" end_char="4814">of</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="4816" end_char="4816">a</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="4818" end_char="4826">cacophony</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="4828" end_char="4829">in</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="4831" end_char="4835">place</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="4837" end_char="4838">of</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="4840" end_char="4850">information</TOKEN>
<TOKEN id="token-43-11" pos="punct" morph="none" start_char="4851" end_char="4851">,</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="4853" end_char="4855">and</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="4857" end_char="4861">where</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="4863" end_char="4873">information</TOKEN>
<TOKEN id="token-43-15" pos="word" morph="none" start_char="4875" end_char="4876">is</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="4878" end_char="4883">needed</TOKEN>
<TOKEN id="token-43-17" pos="punct" morph="none" start_char="4885" end_char="4885">(</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="4886" end_char="4889">from</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="4891" end_char="4893">the</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="4895" end_char="4899">White</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="4901" end_char="4905">House</TOKEN>
<TOKEN id="token-43-22" pos="punct" morph="none" start_char="4906" end_char="4907">),</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="4909" end_char="4915">there’s</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="4917" end_char="4925">foolhardy</TOKEN>
<TOKEN id="token-43-25" pos="unknown" morph="none" start_char="4927" end_char="4937">podium-love</TOKEN>
<TOKEN id="token-43-26" pos="punct" morph="none" start_char="4938" end_char="4938">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="4942" end_char="5047">
<ORIGINAL_TEXT>This kind of report could so quickly become the source of a conspiracy theory in the hands of "Fox News"!!</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="4942" end_char="4945">This</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="4947" end_char="4950">kind</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="4952" end_char="4953">of</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="4955" end_char="4960">report</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="4962" end_char="4966">could</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="4968" end_char="4969">so</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="4971" end_char="4977">quickly</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="4979" end_char="4984">become</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="4986" end_char="4988">the</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="4990" end_char="4995">source</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="4997" end_char="4998">of</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="5000" end_char="5000">a</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="5002" end_char="5011">conspiracy</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="5013" end_char="5018">theory</TOKEN>
<TOKEN id="token-44-14" pos="word" morph="none" start_char="5020" end_char="5021">in</TOKEN>
<TOKEN id="token-44-15" pos="word" morph="none" start_char="5023" end_char="5025">the</TOKEN>
<TOKEN id="token-44-16" pos="word" morph="none" start_char="5027" end_char="5031">hands</TOKEN>
<TOKEN id="token-44-17" pos="word" morph="none" start_char="5033" end_char="5034">of</TOKEN>
<TOKEN id="token-44-18" pos="punct" morph="none" start_char="5036" end_char="5036">"</TOKEN>
<TOKEN id="token-44-19" pos="word" morph="none" start_char="5037" end_char="5039">Fox</TOKEN>
<TOKEN id="token-44-20" pos="word" morph="none" start_char="5041" end_char="5044">News</TOKEN>
<TOKEN id="token-44-21" pos="punct" morph="none" start_char="5045" end_char="5047">"!!</TOKEN>
</SEG>
<SEG id="segment-45" start_char="5051" end_char="5110">
<ORIGINAL_TEXT>However, that does not mean the issue should not be studied.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="5051" end_char="5057">However</TOKEN>
<TOKEN id="token-45-1" pos="punct" morph="none" start_char="5058" end_char="5058">,</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="5060" end_char="5063">that</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="5065" end_char="5068">does</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="5070" end_char="5072">not</TOKEN>
<TOKEN id="token-45-5" pos="word" morph="none" start_char="5074" end_char="5077">mean</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="5079" end_char="5081">the</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="5083" end_char="5087">issue</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="5089" end_char="5094">should</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="5096" end_char="5098">not</TOKEN>
<TOKEN id="token-45-10" pos="word" morph="none" start_char="5100" end_char="5101">be</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="5103" end_char="5109">studied</TOKEN>
<TOKEN id="token-45-12" pos="punct" morph="none" start_char="5110" end_char="5110">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="5112" end_char="5180">
<ORIGINAL_TEXT>There are already a multitude of conspiracy theories out there on it.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="5112" end_char="5116">There</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="5118" end_char="5120">are</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="5122" end_char="5128">already</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="5130" end_char="5130">a</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="5132" end_char="5140">multitude</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="5142" end_char="5143">of</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="5145" end_char="5154">conspiracy</TOKEN>
<TOKEN id="token-46-7" pos="word" morph="none" start_char="5156" end_char="5163">theories</TOKEN>
<TOKEN id="token-46-8" pos="word" morph="none" start_char="5165" end_char="5167">out</TOKEN>
<TOKEN id="token-46-9" pos="word" morph="none" start_char="5169" end_char="5173">there</TOKEN>
<TOKEN id="token-46-10" pos="word" morph="none" start_char="5175" end_char="5176">on</TOKEN>
<TOKEN id="token-46-11" pos="word" morph="none" start_char="5178" end_char="5179">it</TOKEN>
<TOKEN id="token-46-12" pos="punct" morph="none" start_char="5180" end_char="5180">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="5182" end_char="5272">
<ORIGINAL_TEXT>You don’t find the truth by worrying that someone might misread the purpose of your search.</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="5182" end_char="5184">You</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="5186" end_char="5190">don’t</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="5192" end_char="5195">find</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="5197" end_char="5199">the</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="5201" end_char="5205">truth</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="5207" end_char="5208">by</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="5210" end_char="5217">worrying</TOKEN>
<TOKEN id="token-47-7" pos="word" morph="none" start_char="5219" end_char="5222">that</TOKEN>
<TOKEN id="token-47-8" pos="word" morph="none" start_char="5224" end_char="5230">someone</TOKEN>
<TOKEN id="token-47-9" pos="word" morph="none" start_char="5232" end_char="5236">might</TOKEN>
<TOKEN id="token-47-10" pos="word" morph="none" start_char="5238" end_char="5244">misread</TOKEN>
<TOKEN id="token-47-11" pos="word" morph="none" start_char="5246" end_char="5248">the</TOKEN>
<TOKEN id="token-47-12" pos="word" morph="none" start_char="5250" end_char="5256">purpose</TOKEN>
<TOKEN id="token-47-13" pos="word" morph="none" start_char="5258" end_char="5259">of</TOKEN>
<TOKEN id="token-47-14" pos="word" morph="none" start_char="5261" end_char="5264">your</TOKEN>
<TOKEN id="token-47-15" pos="word" morph="none" start_char="5266" end_char="5271">search</TOKEN>
<TOKEN id="token-47-16" pos="punct" morph="none" start_char="5272" end_char="5272">.</TOKEN>
</SEG>
<SEG id="segment-48" start_char="5276" end_char="5349">
<ORIGINAL_TEXT>Why is finding the root cause or discussing potential causes inconvenient?</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="5276" end_char="5278">Why</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="5280" end_char="5281">is</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="5283" end_char="5289">finding</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="5291" end_char="5293">the</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="5295" end_char="5298">root</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="5300" end_char="5304">cause</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="5306" end_char="5307">or</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="5309" end_char="5318">discussing</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="5320" end_char="5328">potential</TOKEN>
<TOKEN id="token-48-9" pos="word" morph="none" start_char="5330" end_char="5335">causes</TOKEN>
<TOKEN id="token-48-10" pos="word" morph="none" start_char="5337" end_char="5348">inconvenient</TOKEN>
<TOKEN id="token-48-11" pos="punct" morph="none" start_char="5349" end_char="5349">?</TOKEN>
</SEG>
<SEG id="segment-49" start_char="5351" end_char="5421">
<ORIGINAL_TEXT>What if all it is proven to be correct, will it be a conspiracy theory?</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="5351" end_char="5354">What</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="5356" end_char="5357">if</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="5359" end_char="5361">all</TOKEN>
<TOKEN id="token-49-3" pos="word" morph="none" start_char="5363" end_char="5364">it</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="5366" end_char="5367">is</TOKEN>
<TOKEN id="token-49-5" pos="word" morph="none" start_char="5369" end_char="5374">proven</TOKEN>
<TOKEN id="token-49-6" pos="word" morph="none" start_char="5376" end_char="5377">to</TOKEN>
<TOKEN id="token-49-7" pos="word" morph="none" start_char="5379" end_char="5380">be</TOKEN>
<TOKEN id="token-49-8" pos="word" morph="none" start_char="5382" end_char="5388">correct</TOKEN>
<TOKEN id="token-49-9" pos="punct" morph="none" start_char="5389" end_char="5389">,</TOKEN>
<TOKEN id="token-49-10" pos="word" morph="none" start_char="5391" end_char="5394">will</TOKEN>
<TOKEN id="token-49-11" pos="word" morph="none" start_char="5396" end_char="5397">it</TOKEN>
<TOKEN id="token-49-12" pos="word" morph="none" start_char="5399" end_char="5400">be</TOKEN>
<TOKEN id="token-49-13" pos="word" morph="none" start_char="5402" end_char="5402">a</TOKEN>
<TOKEN id="token-49-14" pos="word" morph="none" start_char="5404" end_char="5413">conspiracy</TOKEN>
<TOKEN id="token-49-15" pos="word" morph="none" start_char="5415" end_char="5420">theory</TOKEN>
<TOKEN id="token-49-16" pos="punct" morph="none" start_char="5421" end_char="5421">?</TOKEN>
</SEG>
<SEG id="segment-50" start_char="5423" end_char="5570">
<ORIGINAL_TEXT>Does that bother or worry you MORE than understanding who/how is responsible for what the hell is killing people and shutting down the entire world?</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="5423" end_char="5426">Does</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="5428" end_char="5431">that</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="5433" end_char="5438">bother</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="5440" end_char="5441">or</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="5443" end_char="5447">worry</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="5449" end_char="5451">you</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="5453" end_char="5456">MORE</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="5458" end_char="5461">than</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="5463" end_char="5475">understanding</TOKEN>
<TOKEN id="token-50-9" pos="unknown" morph="none" start_char="5477" end_char="5483">who/how</TOKEN>
<TOKEN id="token-50-10" pos="word" morph="none" start_char="5485" end_char="5486">is</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="5488" end_char="5498">responsible</TOKEN>
<TOKEN id="token-50-12" pos="word" morph="none" start_char="5500" end_char="5502">for</TOKEN>
<TOKEN id="token-50-13" pos="word" morph="none" start_char="5504" end_char="5507">what</TOKEN>
<TOKEN id="token-50-14" pos="word" morph="none" start_char="5509" end_char="5511">the</TOKEN>
<TOKEN id="token-50-15" pos="word" morph="none" start_char="5513" end_char="5516">hell</TOKEN>
<TOKEN id="token-50-16" pos="word" morph="none" start_char="5518" end_char="5519">is</TOKEN>
<TOKEN id="token-50-17" pos="word" morph="none" start_char="5521" end_char="5527">killing</TOKEN>
<TOKEN id="token-50-18" pos="word" morph="none" start_char="5529" end_char="5534">people</TOKEN>
<TOKEN id="token-50-19" pos="word" morph="none" start_char="5536" end_char="5538">and</TOKEN>
<TOKEN id="token-50-20" pos="word" morph="none" start_char="5540" end_char="5547">shutting</TOKEN>
<TOKEN id="token-50-21" pos="word" morph="none" start_char="5549" end_char="5552">down</TOKEN>
<TOKEN id="token-50-22" pos="word" morph="none" start_char="5554" end_char="5556">the</TOKEN>
<TOKEN id="token-50-23" pos="word" morph="none" start_char="5558" end_char="5563">entire</TOKEN>
<TOKEN id="token-50-24" pos="word" morph="none" start_char="5565" end_char="5569">world</TOKEN>
<TOKEN id="token-50-25" pos="punct" morph="none" start_char="5570" end_char="5570">?</TOKEN>
</SEG>
<SEG id="segment-51" start_char="5572" end_char="5635">
<ORIGINAL_TEXT>It’s time for people to drop the partisan and childish comments…</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="word" morph="none" start_char="5572" end_char="5575">It’s</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="5577" end_char="5580">time</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="5582" end_char="5584">for</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="5586" end_char="5591">people</TOKEN>
<TOKEN id="token-51-4" pos="word" morph="none" start_char="5593" end_char="5594">to</TOKEN>
<TOKEN id="token-51-5" pos="word" morph="none" start_char="5596" end_char="5599">drop</TOKEN>
<TOKEN id="token-51-6" pos="word" morph="none" start_char="5601" end_char="5603">the</TOKEN>
<TOKEN id="token-51-7" pos="word" morph="none" start_char="5605" end_char="5612">partisan</TOKEN>
<TOKEN id="token-51-8" pos="word" morph="none" start_char="5614" end_char="5616">and</TOKEN>
<TOKEN id="token-51-9" pos="word" morph="none" start_char="5618" end_char="5625">childish</TOKEN>
<TOKEN id="token-51-10" pos="word" morph="none" start_char="5627" end_char="5634">comments</TOKEN>
<TOKEN id="token-51-11" pos="punct" morph="none" start_char="5635" end_char="5635">…</TOKEN>
</SEG>
<SEG id="segment-52" start_char="5638" end_char="5715">
<ORIGINAL_TEXT>Become a part of the solution instead of just an observer that offers nothing.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="5638" end_char="5643">Become</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="5645" end_char="5645">a</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="5647" end_char="5650">part</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="5652" end_char="5653">of</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="5655" end_char="5657">the</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="5659" end_char="5666">solution</TOKEN>
<TOKEN id="token-52-6" pos="word" morph="none" start_char="5668" end_char="5674">instead</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="5676" end_char="5677">of</TOKEN>
<TOKEN id="token-52-8" pos="word" morph="none" start_char="5679" end_char="5682">just</TOKEN>
<TOKEN id="token-52-9" pos="word" morph="none" start_char="5684" end_char="5685">an</TOKEN>
<TOKEN id="token-52-10" pos="word" morph="none" start_char="5687" end_char="5694">observer</TOKEN>
<TOKEN id="token-52-11" pos="word" morph="none" start_char="5696" end_char="5699">that</TOKEN>
<TOKEN id="token-52-12" pos="word" morph="none" start_char="5701" end_char="5706">offers</TOKEN>
<TOKEN id="token-52-13" pos="word" morph="none" start_char="5708" end_char="5714">nothing</TOKEN>
<TOKEN id="token-52-14" pos="punct" morph="none" start_char="5715" end_char="5715">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="5719" end_char="5845">
<ORIGINAL_TEXT>A question from a layman…in the lab release scenario, what is the source of the virus that might have been released from a lab?</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="5719" end_char="5719">A</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="5721" end_char="5728">question</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="5730" end_char="5733">from</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="5735" end_char="5735">a</TOKEN>
<TOKEN id="token-53-4" pos="unknown" morph="none" start_char="5737" end_char="5745">layman…in</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="5747" end_char="5749">the</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="5751" end_char="5753">lab</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="5755" end_char="5761">release</TOKEN>
<TOKEN id="token-53-8" pos="word" morph="none" start_char="5763" end_char="5770">scenario</TOKEN>
<TOKEN id="token-53-9" pos="punct" morph="none" start_char="5771" end_char="5771">,</TOKEN>
<TOKEN id="token-53-10" pos="word" morph="none" start_char="5773" end_char="5776">what</TOKEN>
<TOKEN id="token-53-11" pos="word" morph="none" start_char="5778" end_char="5779">is</TOKEN>
<TOKEN id="token-53-12" pos="word" morph="none" start_char="5781" end_char="5783">the</TOKEN>
<TOKEN id="token-53-13" pos="word" morph="none" start_char="5785" end_char="5790">source</TOKEN>
<TOKEN id="token-53-14" pos="word" morph="none" start_char="5792" end_char="5793">of</TOKEN>
<TOKEN id="token-53-15" pos="word" morph="none" start_char="5795" end_char="5797">the</TOKEN>
<TOKEN id="token-53-16" pos="word" morph="none" start_char="5799" end_char="5803">virus</TOKEN>
<TOKEN id="token-53-17" pos="word" morph="none" start_char="5805" end_char="5808">that</TOKEN>
<TOKEN id="token-53-18" pos="word" morph="none" start_char="5810" end_char="5814">might</TOKEN>
<TOKEN id="token-53-19" pos="word" morph="none" start_char="5816" end_char="5819">have</TOKEN>
<TOKEN id="token-53-20" pos="word" morph="none" start_char="5821" end_char="5824">been</TOKEN>
<TOKEN id="token-53-21" pos="word" morph="none" start_char="5826" end_char="5833">released</TOKEN>
<TOKEN id="token-53-22" pos="word" morph="none" start_char="5835" end_char="5838">from</TOKEN>
<TOKEN id="token-53-23" pos="word" morph="none" start_char="5840" end_char="5840">a</TOKEN>
<TOKEN id="token-53-24" pos="word" morph="none" start_char="5842" end_char="5844">lab</TOKEN>
<TOKEN id="token-53-25" pos="punct" morph="none" start_char="5845" end_char="5845">?</TOKEN>
</SEG>
<SEG id="segment-54" start_char="5847" end_char="5892">
<ORIGINAL_TEXT>Are the virus samples collected from the wild?</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="5847" end_char="5849">Are</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="5851" end_char="5853">the</TOKEN>
<TOKEN id="token-54-2" pos="word" morph="none" start_char="5855" end_char="5859">virus</TOKEN>
<TOKEN id="token-54-3" pos="word" morph="none" start_char="5861" end_char="5867">samples</TOKEN>
<TOKEN id="token-54-4" pos="word" morph="none" start_char="5869" end_char="5877">collected</TOKEN>
<TOKEN id="token-54-5" pos="word" morph="none" start_char="5879" end_char="5882">from</TOKEN>
<TOKEN id="token-54-6" pos="word" morph="none" start_char="5884" end_char="5886">the</TOKEN>
<TOKEN id="token-54-7" pos="word" morph="none" start_char="5888" end_char="5891">wild</TOKEN>
<TOKEN id="token-54-8" pos="punct" morph="none" start_char="5892" end_char="5892">?</TOKEN>
</SEG>
<SEG id="segment-55" start_char="5894" end_char="6007">
<ORIGINAL_TEXT>If so, aren’t the odds much higher that the initial humans infected were exposed to the naturally occurring virus?</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="5894" end_char="5895">If</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="5897" end_char="5898">so</TOKEN>
<TOKEN id="token-55-2" pos="punct" morph="none" start_char="5899" end_char="5899">,</TOKEN>
<TOKEN id="token-55-3" pos="word" morph="none" start_char="5901" end_char="5906">aren’t</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="5908" end_char="5910">the</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="5912" end_char="5915">odds</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="5917" end_char="5920">much</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="5922" end_char="5927">higher</TOKEN>
<TOKEN id="token-55-8" pos="word" morph="none" start_char="5929" end_char="5932">that</TOKEN>
<TOKEN id="token-55-9" pos="word" morph="none" start_char="5934" end_char="5936">the</TOKEN>
<TOKEN id="token-55-10" pos="word" morph="none" start_char="5938" end_char="5944">initial</TOKEN>
<TOKEN id="token-55-11" pos="word" morph="none" start_char="5946" end_char="5951">humans</TOKEN>
<TOKEN id="token-55-12" pos="word" morph="none" start_char="5953" end_char="5960">infected</TOKEN>
<TOKEN id="token-55-13" pos="word" morph="none" start_char="5962" end_char="5965">were</TOKEN>
<TOKEN id="token-55-14" pos="word" morph="none" start_char="5967" end_char="5973">exposed</TOKEN>
<TOKEN id="token-55-15" pos="word" morph="none" start_char="5975" end_char="5976">to</TOKEN>
<TOKEN id="token-55-16" pos="word" morph="none" start_char="5978" end_char="5980">the</TOKEN>
<TOKEN id="token-55-17" pos="word" morph="none" start_char="5982" end_char="5990">naturally</TOKEN>
<TOKEN id="token-55-18" pos="word" morph="none" start_char="5992" end_char="6000">occurring</TOKEN>
<TOKEN id="token-55-19" pos="word" morph="none" start_char="6002" end_char="6006">virus</TOKEN>
<TOKEN id="token-55-20" pos="punct" morph="none" start_char="6007" end_char="6007">?</TOKEN>
</SEG>
<SEG id="segment-56" start_char="6009" end_char="6031">
<ORIGINAL_TEXT>What am I missing here?</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="6009" end_char="6012">What</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="6014" end_char="6015">am</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="6017" end_char="6017">I</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="6019" end_char="6025">missing</TOKEN>
<TOKEN id="token-56-4" pos="word" morph="none" start_char="6027" end_char="6030">here</TOKEN>
<TOKEN id="token-56-5" pos="punct" morph="none" start_char="6031" end_char="6031">?</TOKEN>
</SEG>
<SEG id="segment-57" start_char="6035" end_char="6135">
<ORIGINAL_TEXT>Nobody really knows if the source was an accidental lab release or if the cycle was entirely natural.</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="6035" end_char="6040">Nobody</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="6042" end_char="6047">really</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="6049" end_char="6053">knows</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="6055" end_char="6056">if</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="6058" end_char="6060">the</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="6062" end_char="6067">source</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="6069" end_char="6071">was</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="6073" end_char="6074">an</TOKEN>
<TOKEN id="token-57-8" pos="word" morph="none" start_char="6076" end_char="6085">accidental</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="6087" end_char="6089">lab</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="6091" end_char="6097">release</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="6099" end_char="6100">or</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="6102" end_char="6103">if</TOKEN>
<TOKEN id="token-57-13" pos="word" morph="none" start_char="6105" end_char="6107">the</TOKEN>
<TOKEN id="token-57-14" pos="word" morph="none" start_char="6109" end_char="6113">cycle</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="6115" end_char="6117">was</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="6119" end_char="6126">entirely</TOKEN>
<TOKEN id="token-57-17" pos="word" morph="none" start_char="6128" end_char="6134">natural</TOKEN>
<TOKEN id="token-57-18" pos="punct" morph="none" start_char="6135" end_char="6135">.</TOKEN>
</SEG>
<SEG id="segment-58" start_char="6137" end_char="6265">
<ORIGINAL_TEXT>The problem with the lab scenario is that the virus may have been introduced to a new species that is local to Wuhan as a result.</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="6137" end_char="6139">The</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="6141" end_char="6147">problem</TOKEN>
<TOKEN id="token-58-2" pos="word" morph="none" start_char="6149" end_char="6152">with</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="6154" end_char="6156">the</TOKEN>
<TOKEN id="token-58-4" pos="word" morph="none" start_char="6158" end_char="6160">lab</TOKEN>
<TOKEN id="token-58-5" pos="word" morph="none" start_char="6162" end_char="6169">scenario</TOKEN>
<TOKEN id="token-58-6" pos="word" morph="none" start_char="6171" end_char="6172">is</TOKEN>
<TOKEN id="token-58-7" pos="word" morph="none" start_char="6174" end_char="6177">that</TOKEN>
<TOKEN id="token-58-8" pos="word" morph="none" start_char="6179" end_char="6181">the</TOKEN>
<TOKEN id="token-58-9" pos="word" morph="none" start_char="6183" end_char="6187">virus</TOKEN>
<TOKEN id="token-58-10" pos="word" morph="none" start_char="6189" end_char="6191">may</TOKEN>
<TOKEN id="token-58-11" pos="word" morph="none" start_char="6193" end_char="6196">have</TOKEN>
<TOKEN id="token-58-12" pos="word" morph="none" start_char="6198" end_char="6201">been</TOKEN>
<TOKEN id="token-58-13" pos="word" morph="none" start_char="6203" end_char="6212">introduced</TOKEN>
<TOKEN id="token-58-14" pos="word" morph="none" start_char="6214" end_char="6215">to</TOKEN>
<TOKEN id="token-58-15" pos="word" morph="none" start_char="6217" end_char="6217">a</TOKEN>
<TOKEN id="token-58-16" pos="word" morph="none" start_char="6219" end_char="6221">new</TOKEN>
<TOKEN id="token-58-17" pos="word" morph="none" start_char="6223" end_char="6229">species</TOKEN>
<TOKEN id="token-58-18" pos="word" morph="none" start_char="6231" end_char="6234">that</TOKEN>
<TOKEN id="token-58-19" pos="word" morph="none" start_char="6236" end_char="6237">is</TOKEN>
<TOKEN id="token-58-20" pos="word" morph="none" start_char="6239" end_char="6243">local</TOKEN>
<TOKEN id="token-58-21" pos="word" morph="none" start_char="6245" end_char="6246">to</TOKEN>
<TOKEN id="token-58-22" pos="word" morph="none" start_char="6248" end_char="6252">Wuhan</TOKEN>
<TOKEN id="token-58-23" pos="word" morph="none" start_char="6254" end_char="6255">as</TOKEN>
<TOKEN id="token-58-24" pos="word" morph="none" start_char="6257" end_char="6257">a</TOKEN>
<TOKEN id="token-58-25" pos="word" morph="none" start_char="6259" end_char="6264">result</TOKEN>
<TOKEN id="token-58-26" pos="punct" morph="none" start_char="6265" end_char="6265">.</TOKEN>
</SEG>
<SEG id="segment-59" start_char="6267" end_char="6391">
<ORIGINAL_TEXT>When viruses are introduced to new species they can evolve differently since different animals have different immune systems.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="6267" end_char="6270">When</TOKEN>
<TOKEN id="token-59-1" pos="word" morph="none" start_char="6272" end_char="6278">viruses</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="6280" end_char="6282">are</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="6284" end_char="6293">introduced</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="6295" end_char="6296">to</TOKEN>
<TOKEN id="token-59-5" pos="word" morph="none" start_char="6298" end_char="6300">new</TOKEN>
<TOKEN id="token-59-6" pos="word" morph="none" start_char="6302" end_char="6308">species</TOKEN>
<TOKEN id="token-59-7" pos="word" morph="none" start_char="6310" end_char="6313">they</TOKEN>
<TOKEN id="token-59-8" pos="word" morph="none" start_char="6315" end_char="6317">can</TOKEN>
<TOKEN id="token-59-9" pos="word" morph="none" start_char="6319" end_char="6324">evolve</TOKEN>
<TOKEN id="token-59-10" pos="word" morph="none" start_char="6326" end_char="6336">differently</TOKEN>
<TOKEN id="token-59-11" pos="word" morph="none" start_char="6338" end_char="6342">since</TOKEN>
<TOKEN id="token-59-12" pos="word" morph="none" start_char="6344" end_char="6352">different</TOKEN>
<TOKEN id="token-59-13" pos="word" morph="none" start_char="6354" end_char="6360">animals</TOKEN>
<TOKEN id="token-59-14" pos="word" morph="none" start_char="6362" end_char="6365">have</TOKEN>
<TOKEN id="token-59-15" pos="word" morph="none" start_char="6367" end_char="6375">different</TOKEN>
<TOKEN id="token-59-16" pos="word" morph="none" start_char="6377" end_char="6382">immune</TOKEN>
<TOKEN id="token-59-17" pos="word" morph="none" start_char="6384" end_char="6390">systems</TOKEN>
<TOKEN id="token-59-18" pos="punct" morph="none" start_char="6391" end_char="6391">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="6393" end_char="6474">
<ORIGINAL_TEXT>It’s an even bigger problem if it’s an animal that is eaten by people in the area.</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="word" morph="none" start_char="6393" end_char="6396">It’s</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="6398" end_char="6399">an</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="6401" end_char="6404">even</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="6406" end_char="6411">bigger</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="6413" end_char="6419">problem</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="6421" end_char="6422">if</TOKEN>
<TOKEN id="token-60-6" pos="word" morph="none" start_char="6424" end_char="6427">it’s</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="6429" end_char="6430">an</TOKEN>
<TOKEN id="token-60-8" pos="word" morph="none" start_char="6432" end_char="6437">animal</TOKEN>
<TOKEN id="token-60-9" pos="word" morph="none" start_char="6439" end_char="6442">that</TOKEN>
<TOKEN id="token-60-10" pos="word" morph="none" start_char="6444" end_char="6445">is</TOKEN>
<TOKEN id="token-60-11" pos="word" morph="none" start_char="6447" end_char="6451">eaten</TOKEN>
<TOKEN id="token-60-12" pos="word" morph="none" start_char="6453" end_char="6454">by</TOKEN>
<TOKEN id="token-60-13" pos="word" morph="none" start_char="6456" end_char="6461">people</TOKEN>
<TOKEN id="token-60-14" pos="word" morph="none" start_char="6463" end_char="6464">in</TOKEN>
<TOKEN id="token-60-15" pos="word" morph="none" start_char="6466" end_char="6468">the</TOKEN>
<TOKEN id="token-60-16" pos="word" morph="none" start_char="6470" end_char="6473">area</TOKEN>
<TOKEN id="token-60-17" pos="punct" morph="none" start_char="6474" end_char="6474">.</TOKEN>
</SEG>
<SEG id="segment-61" start_char="6476" end_char="6593">
<ORIGINAL_TEXT>The bats that carry RaTG13 live in Yunan, some 2000 kms away from Wuhan and was discovered in 2013 by the… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="word" morph="none" start_char="6476" end_char="6478">The</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="6480" end_char="6483">bats</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="6485" end_char="6488">that</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="6490" end_char="6494">carry</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="6496" end_char="6501">RaTG13</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="6503" end_char="6506">live</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="6508" end_char="6509">in</TOKEN>
<TOKEN id="token-61-7" pos="word" morph="none" start_char="6511" end_char="6515">Yunan</TOKEN>
<TOKEN id="token-61-8" pos="punct" morph="none" start_char="6516" end_char="6516">,</TOKEN>
<TOKEN id="token-61-9" pos="word" morph="none" start_char="6518" end_char="6521">some</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="6523" end_char="6526">2000</TOKEN>
<TOKEN id="token-61-11" pos="word" morph="none" start_char="6528" end_char="6530">kms</TOKEN>
<TOKEN id="token-61-12" pos="word" morph="none" start_char="6532" end_char="6535">away</TOKEN>
<TOKEN id="token-61-13" pos="word" morph="none" start_char="6537" end_char="6540">from</TOKEN>
<TOKEN id="token-61-14" pos="word" morph="none" start_char="6542" end_char="6546">Wuhan</TOKEN>
<TOKEN id="token-61-15" pos="word" morph="none" start_char="6548" end_char="6550">and</TOKEN>
<TOKEN id="token-61-16" pos="word" morph="none" start_char="6552" end_char="6554">was</TOKEN>
<TOKEN id="token-61-17" pos="word" morph="none" start_char="6556" end_char="6565">discovered</TOKEN>
<TOKEN id="token-61-18" pos="word" morph="none" start_char="6567" end_char="6568">in</TOKEN>
<TOKEN id="token-61-19" pos="word" morph="none" start_char="6570" end_char="6573">2013</TOKEN>
<TOKEN id="token-61-20" pos="word" morph="none" start_char="6575" end_char="6576">by</TOKEN>
<TOKEN id="token-61-21" pos="word" morph="none" start_char="6578" end_char="6580">the</TOKEN>
<TOKEN id="token-61-22" pos="punct" morph="none" start_char="6581" end_char="6581">…</TOKEN>
<TOKEN id="token-61-23" pos="word" morph="none" start_char="6583" end_char="6586">Read</TOKEN>
<TOKEN id="token-61-24" pos="word" morph="none" start_char="6588" end_char="6591">more</TOKEN>
<TOKEN id="token-61-25" pos="punct" morph="none" start_char="6593" end_char="6593">»</TOKEN>
</SEG>
<SEG id="segment-62" start_char="6597" end_char="6632">
<ORIGINAL_TEXT>Samples are collected from the wild.</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="6597" end_char="6603">Samples</TOKEN>
<TOKEN id="token-62-1" pos="word" morph="none" start_char="6605" end_char="6607">are</TOKEN>
<TOKEN id="token-62-2" pos="word" morph="none" start_char="6609" end_char="6617">collected</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="6619" end_char="6622">from</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="6624" end_char="6626">the</TOKEN>
<TOKEN id="token-62-5" pos="word" morph="none" start_char="6628" end_char="6631">wild</TOKEN>
<TOKEN id="token-62-6" pos="punct" morph="none" start_char="6632" end_char="6632">.</TOKEN>
</SEG>
<SEG id="segment-63" start_char="6634" end_char="6771">
<ORIGINAL_TEXT>Then those samples can be cultured in the lab and an artificial acceleration of natural mutations can be produced through experimentation.</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="6634" end_char="6637">Then</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="6639" end_char="6643">those</TOKEN>
<TOKEN id="token-63-2" pos="word" morph="none" start_char="6645" end_char="6651">samples</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="6653" end_char="6655">can</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="6657" end_char="6658">be</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="6660" end_char="6667">cultured</TOKEN>
<TOKEN id="token-63-6" pos="word" morph="none" start_char="6669" end_char="6670">in</TOKEN>
<TOKEN id="token-63-7" pos="word" morph="none" start_char="6672" end_char="6674">the</TOKEN>
<TOKEN id="token-63-8" pos="word" morph="none" start_char="6676" end_char="6678">lab</TOKEN>
<TOKEN id="token-63-9" pos="word" morph="none" start_char="6680" end_char="6682">and</TOKEN>
<TOKEN id="token-63-10" pos="word" morph="none" start_char="6684" end_char="6685">an</TOKEN>
<TOKEN id="token-63-11" pos="word" morph="none" start_char="6687" end_char="6696">artificial</TOKEN>
<TOKEN id="token-63-12" pos="word" morph="none" start_char="6698" end_char="6709">acceleration</TOKEN>
<TOKEN id="token-63-13" pos="word" morph="none" start_char="6711" end_char="6712">of</TOKEN>
<TOKEN id="token-63-14" pos="word" morph="none" start_char="6714" end_char="6720">natural</TOKEN>
<TOKEN id="token-63-15" pos="word" morph="none" start_char="6722" end_char="6730">mutations</TOKEN>
<TOKEN id="token-63-16" pos="word" morph="none" start_char="6732" end_char="6734">can</TOKEN>
<TOKEN id="token-63-17" pos="word" morph="none" start_char="6736" end_char="6737">be</TOKEN>
<TOKEN id="token-63-18" pos="word" morph="none" start_char="6739" end_char="6746">produced</TOKEN>
<TOKEN id="token-63-19" pos="word" morph="none" start_char="6748" end_char="6754">through</TOKEN>
<TOKEN id="token-63-20" pos="word" morph="none" start_char="6756" end_char="6770">experimentation</TOKEN>
<TOKEN id="token-63-21" pos="punct" morph="none" start_char="6771" end_char="6771">.</TOKEN>
</SEG>
<SEG id="segment-64" start_char="6773" end_char="6885">
<ORIGINAL_TEXT>Then new viruses can be selected for new "improved" functions, this filtering can be done through animal testing.</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="6773" end_char="6776">Then</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="6778" end_char="6780">new</TOKEN>
<TOKEN id="token-64-2" pos="word" morph="none" start_char="6782" end_char="6788">viruses</TOKEN>
<TOKEN id="token-64-3" pos="word" morph="none" start_char="6790" end_char="6792">can</TOKEN>
<TOKEN id="token-64-4" pos="word" morph="none" start_char="6794" end_char="6795">be</TOKEN>
<TOKEN id="token-64-5" pos="word" morph="none" start_char="6797" end_char="6804">selected</TOKEN>
<TOKEN id="token-64-6" pos="word" morph="none" start_char="6806" end_char="6808">for</TOKEN>
<TOKEN id="token-64-7" pos="word" morph="none" start_char="6810" end_char="6812">new</TOKEN>
<TOKEN id="token-64-8" pos="punct" morph="none" start_char="6814" end_char="6814">"</TOKEN>
<TOKEN id="token-64-9" pos="word" morph="none" start_char="6815" end_char="6822">improved</TOKEN>
<TOKEN id="token-64-10" pos="punct" morph="none" start_char="6823" end_char="6823">"</TOKEN>
<TOKEN id="token-64-11" pos="word" morph="none" start_char="6825" end_char="6833">functions</TOKEN>
<TOKEN id="token-64-12" pos="punct" morph="none" start_char="6834" end_char="6834">,</TOKEN>
<TOKEN id="token-64-13" pos="word" morph="none" start_char="6836" end_char="6839">this</TOKEN>
<TOKEN id="token-64-14" pos="word" morph="none" start_char="6841" end_char="6849">filtering</TOKEN>
<TOKEN id="token-64-15" pos="word" morph="none" start_char="6851" end_char="6853">can</TOKEN>
<TOKEN id="token-64-16" pos="word" morph="none" start_char="6855" end_char="6856">be</TOKEN>
<TOKEN id="token-64-17" pos="word" morph="none" start_char="6858" end_char="6861">done</TOKEN>
<TOKEN id="token-64-18" pos="word" morph="none" start_char="6863" end_char="6869">through</TOKEN>
<TOKEN id="token-64-19" pos="word" morph="none" start_char="6871" end_char="6876">animal</TOKEN>
<TOKEN id="token-64-20" pos="word" morph="none" start_char="6878" end_char="6884">testing</TOKEN>
<TOKEN id="token-64-21" pos="punct" morph="none" start_char="6885" end_char="6885">.</TOKEN>
</SEG>
<SEG id="segment-65" start_char="6887" end_char="6971">
<ORIGINAL_TEXT>There is a huge risk that one of the people working in the lab could become infected.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="word" morph="none" start_char="6887" end_char="6891">There</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="6893" end_char="6894">is</TOKEN>
<TOKEN id="token-65-2" pos="word" morph="none" start_char="6896" end_char="6896">a</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="6898" end_char="6901">huge</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="6903" end_char="6906">risk</TOKEN>
<TOKEN id="token-65-5" pos="word" morph="none" start_char="6908" end_char="6911">that</TOKEN>
<TOKEN id="token-65-6" pos="word" morph="none" start_char="6913" end_char="6915">one</TOKEN>
<TOKEN id="token-65-7" pos="word" morph="none" start_char="6917" end_char="6918">of</TOKEN>
<TOKEN id="token-65-8" pos="word" morph="none" start_char="6920" end_char="6922">the</TOKEN>
<TOKEN id="token-65-9" pos="word" morph="none" start_char="6924" end_char="6929">people</TOKEN>
<TOKEN id="token-65-10" pos="word" morph="none" start_char="6931" end_char="6937">working</TOKEN>
<TOKEN id="token-65-11" pos="word" morph="none" start_char="6939" end_char="6940">in</TOKEN>
<TOKEN id="token-65-12" pos="word" morph="none" start_char="6942" end_char="6944">the</TOKEN>
<TOKEN id="token-65-13" pos="word" morph="none" start_char="6946" end_char="6948">lab</TOKEN>
<TOKEN id="token-65-14" pos="word" morph="none" start_char="6950" end_char="6954">could</TOKEN>
<TOKEN id="token-65-15" pos="word" morph="none" start_char="6956" end_char="6961">become</TOKEN>
<TOKEN id="token-65-16" pos="word" morph="none" start_char="6963" end_char="6970">infected</TOKEN>
<TOKEN id="token-65-17" pos="punct" morph="none" start_char="6971" end_char="6971">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="6973" end_char="7062">
<ORIGINAL_TEXT>For example, this could happen due to human error, it has happened many times in the past.</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="6973" end_char="6975">For</TOKEN>
<TOKEN id="token-66-1" pos="word" morph="none" start_char="6977" end_char="6983">example</TOKEN>
<TOKEN id="token-66-2" pos="punct" morph="none" start_char="6984" end_char="6984">,</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="6986" end_char="6989">this</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="6991" end_char="6995">could</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="6997" end_char="7002">happen</TOKEN>
<TOKEN id="token-66-6" pos="word" morph="none" start_char="7004" end_char="7006">due</TOKEN>
<TOKEN id="token-66-7" pos="word" morph="none" start_char="7008" end_char="7009">to</TOKEN>
<TOKEN id="token-66-8" pos="word" morph="none" start_char="7011" end_char="7015">human</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="7017" end_char="7021">error</TOKEN>
<TOKEN id="token-66-10" pos="punct" morph="none" start_char="7022" end_char="7022">,</TOKEN>
<TOKEN id="token-66-11" pos="word" morph="none" start_char="7024" end_char="7025">it</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="7027" end_char="7029">has</TOKEN>
<TOKEN id="token-66-13" pos="word" morph="none" start_char="7031" end_char="7038">happened</TOKEN>
<TOKEN id="token-66-14" pos="word" morph="none" start_char="7040" end_char="7043">many</TOKEN>
<TOKEN id="token-66-15" pos="word" morph="none" start_char="7045" end_char="7049">times</TOKEN>
<TOKEN id="token-66-16" pos="word" morph="none" start_char="7051" end_char="7052">in</TOKEN>
<TOKEN id="token-66-17" pos="word" morph="none" start_char="7054" end_char="7056">the</TOKEN>
<TOKEN id="token-66-18" pos="word" morph="none" start_char="7058" end_char="7061">past</TOKEN>
<TOKEN id="token-66-19" pos="punct" morph="none" start_char="7062" end_char="7062">.</TOKEN>
</SEG>
<SEG id="segment-67" start_char="7064" end_char="7136">
<ORIGINAL_TEXT>The above article gives several examples of these mistakes happening e.g.</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="7064" end_char="7066">The</TOKEN>
<TOKEN id="token-67-1" pos="word" morph="none" start_char="7068" end_char="7072">above</TOKEN>
<TOKEN id="token-67-2" pos="word" morph="none" start_char="7074" end_char="7080">article</TOKEN>
<TOKEN id="token-67-3" pos="word" morph="none" start_char="7082" end_char="7086">gives</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="7088" end_char="7094">several</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="7096" end_char="7103">examples</TOKEN>
<TOKEN id="token-67-6" pos="word" morph="none" start_char="7105" end_char="7106">of</TOKEN>
<TOKEN id="token-67-7" pos="word" morph="none" start_char="7108" end_char="7112">these</TOKEN>
<TOKEN id="token-67-8" pos="word" morph="none" start_char="7114" end_char="7121">mistakes</TOKEN>
<TOKEN id="token-67-9" pos="word" morph="none" start_char="7123" end_char="7131">happening</TOKEN>
<TOKEN id="token-67-10" pos="unknown" morph="none" start_char="7133" end_char="7135">e.g</TOKEN>
<TOKEN id="token-67-11" pos="punct" morph="none" start_char="7136" end_char="7136">.</TOKEN>
</SEG>
<SEG id="segment-68" start_char="7138" end_char="7152">
<ORIGINAL_TEXT>Beijing in 2004</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="7138" end_char="7144">Beijing</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="7146" end_char="7147">in</TOKEN>
<TOKEN id="token-68-2" pos="word" morph="none" start_char="7149" end_char="7152">2004</TOKEN>
</SEG>
<SEG id="segment-69" start_char="7156" end_char="7374">
<ORIGINAL_TEXT>This article mentions the study by Botao Xiao and Lei Xiao entitled "The possible origins of 2019-nCoV coronavirus" as part of the "circumstantial evidence that supports the possibility that a lab release was involved."</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="word" morph="none" start_char="7156" end_char="7159">This</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="7161" end_char="7167">article</TOKEN>
<TOKEN id="token-69-2" pos="word" morph="none" start_char="7169" end_char="7176">mentions</TOKEN>
<TOKEN id="token-69-3" pos="word" morph="none" start_char="7178" end_char="7180">the</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="7182" end_char="7186">study</TOKEN>
<TOKEN id="token-69-5" pos="word" morph="none" start_char="7188" end_char="7189">by</TOKEN>
<TOKEN id="token-69-6" pos="word" morph="none" start_char="7191" end_char="7195">Botao</TOKEN>
<TOKEN id="token-69-7" pos="word" morph="none" start_char="7197" end_char="7200">Xiao</TOKEN>
<TOKEN id="token-69-8" pos="word" morph="none" start_char="7202" end_char="7204">and</TOKEN>
<TOKEN id="token-69-9" pos="word" morph="none" start_char="7206" end_char="7208">Lei</TOKEN>
<TOKEN id="token-69-10" pos="word" morph="none" start_char="7210" end_char="7213">Xiao</TOKEN>
<TOKEN id="token-69-11" pos="word" morph="none" start_char="7215" end_char="7222">entitled</TOKEN>
<TOKEN id="token-69-12" pos="punct" morph="none" start_char="7224" end_char="7224">"</TOKEN>
<TOKEN id="token-69-13" pos="word" morph="none" start_char="7225" end_char="7227">The</TOKEN>
<TOKEN id="token-69-14" pos="word" morph="none" start_char="7229" end_char="7236">possible</TOKEN>
<TOKEN id="token-69-15" pos="word" morph="none" start_char="7238" end_char="7244">origins</TOKEN>
<TOKEN id="token-69-16" pos="word" morph="none" start_char="7246" end_char="7247">of</TOKEN>
<TOKEN id="token-69-17" pos="unknown" morph="none" start_char="7249" end_char="7257">2019-nCoV</TOKEN>
<TOKEN id="token-69-18" pos="word" morph="none" start_char="7259" end_char="7269">coronavirus</TOKEN>
<TOKEN id="token-69-19" pos="punct" morph="none" start_char="7270" end_char="7270">"</TOKEN>
<TOKEN id="token-69-20" pos="word" morph="none" start_char="7272" end_char="7273">as</TOKEN>
<TOKEN id="token-69-21" pos="word" morph="none" start_char="7275" end_char="7278">part</TOKEN>
<TOKEN id="token-69-22" pos="word" morph="none" start_char="7280" end_char="7281">of</TOKEN>
<TOKEN id="token-69-23" pos="word" morph="none" start_char="7283" end_char="7285">the</TOKEN>
<TOKEN id="token-69-24" pos="punct" morph="none" start_char="7287" end_char="7287">"</TOKEN>
<TOKEN id="token-69-25" pos="word" morph="none" start_char="7288" end_char="7301">circumstantial</TOKEN>
<TOKEN id="token-69-26" pos="word" morph="none" start_char="7303" end_char="7310">evidence</TOKEN>
<TOKEN id="token-69-27" pos="word" morph="none" start_char="7312" end_char="7315">that</TOKEN>
<TOKEN id="token-69-28" pos="word" morph="none" start_char="7317" end_char="7324">supports</TOKEN>
<TOKEN id="token-69-29" pos="word" morph="none" start_char="7326" end_char="7328">the</TOKEN>
<TOKEN id="token-69-30" pos="word" morph="none" start_char="7330" end_char="7340">possibility</TOKEN>
<TOKEN id="token-69-31" pos="word" morph="none" start_char="7342" end_char="7345">that</TOKEN>
<TOKEN id="token-69-32" pos="word" morph="none" start_char="7347" end_char="7347">a</TOKEN>
<TOKEN id="token-69-33" pos="word" morph="none" start_char="7349" end_char="7351">lab</TOKEN>
<TOKEN id="token-69-34" pos="word" morph="none" start_char="7353" end_char="7359">release</TOKEN>
<TOKEN id="token-69-35" pos="word" morph="none" start_char="7361" end_char="7363">was</TOKEN>
<TOKEN id="token-69-36" pos="word" morph="none" start_char="7365" end_char="7372">involved</TOKEN>
<TOKEN id="token-69-37" pos="punct" morph="none" start_char="7373" end_char="7374">."</TOKEN>
</SEG>
<SEG id="segment-70" start_char="7376" end_char="7547">
<ORIGINAL_TEXT>The following is stated: ‘’"The paper was later removed from ResearchGate, a commercial social-networking site for scientists and researchers to share papers," Huang wrote.</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="word" morph="none" start_char="7376" end_char="7378">The</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="7380" end_char="7388">following</TOKEN>
<TOKEN id="token-70-2" pos="word" morph="none" start_char="7390" end_char="7391">is</TOKEN>
<TOKEN id="token-70-3" pos="word" morph="none" start_char="7393" end_char="7398">stated</TOKEN>
<TOKEN id="token-70-4" pos="punct" morph="none" start_char="7399" end_char="7399">:</TOKEN>
<TOKEN id="token-70-5" pos="punct" morph="none" start_char="7401" end_char="7403">‘’"</TOKEN>
<TOKEN id="token-70-6" pos="word" morph="none" start_char="7404" end_char="7406">The</TOKEN>
<TOKEN id="token-70-7" pos="word" morph="none" start_char="7408" end_char="7412">paper</TOKEN>
<TOKEN id="token-70-8" pos="word" morph="none" start_char="7414" end_char="7416">was</TOKEN>
<TOKEN id="token-70-9" pos="word" morph="none" start_char="7418" end_char="7422">later</TOKEN>
<TOKEN id="token-70-10" pos="word" morph="none" start_char="7424" end_char="7430">removed</TOKEN>
<TOKEN id="token-70-11" pos="word" morph="none" start_char="7432" end_char="7435">from</TOKEN>
<TOKEN id="token-70-12" pos="word" morph="none" start_char="7437" end_char="7448">ResearchGate</TOKEN>
<TOKEN id="token-70-13" pos="punct" morph="none" start_char="7449" end_char="7449">,</TOKEN>
<TOKEN id="token-70-14" pos="word" morph="none" start_char="7451" end_char="7451">a</TOKEN>
<TOKEN id="token-70-15" pos="word" morph="none" start_char="7453" end_char="7462">commercial</TOKEN>
<TOKEN id="token-70-16" pos="unknown" morph="none" start_char="7464" end_char="7480">social-networking</TOKEN>
<TOKEN id="token-70-17" pos="word" morph="none" start_char="7482" end_char="7485">site</TOKEN>
<TOKEN id="token-70-18" pos="word" morph="none" start_char="7487" end_char="7489">for</TOKEN>
<TOKEN id="token-70-19" pos="word" morph="none" start_char="7491" end_char="7500">scientists</TOKEN>
<TOKEN id="token-70-20" pos="word" morph="none" start_char="7502" end_char="7504">and</TOKEN>
<TOKEN id="token-70-21" pos="word" morph="none" start_char="7506" end_char="7516">researchers</TOKEN>
<TOKEN id="token-70-22" pos="word" morph="none" start_char="7518" end_char="7519">to</TOKEN>
<TOKEN id="token-70-23" pos="word" morph="none" start_char="7521" end_char="7525">share</TOKEN>
<TOKEN id="token-70-24" pos="word" morph="none" start_char="7527" end_char="7532">papers</TOKEN>
<TOKEN id="token-70-25" pos="punct" morph="none" start_char="7533" end_char="7534">,"</TOKEN>
<TOKEN id="token-70-26" pos="word" morph="none" start_char="7536" end_char="7540">Huang</TOKEN>
<TOKEN id="token-70-27" pos="word" morph="none" start_char="7542" end_char="7546">wrote</TOKEN>
<TOKEN id="token-70-28" pos="punct" morph="none" start_char="7547" end_char="7547">.</TOKEN>
</SEG>
<SEG id="segment-71" start_char="7549" end_char="7622">
<ORIGINAL_TEXT>"Thus far, no scientists have confirmed or refuted the paper’s findings.""</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="punct" morph="none" start_char="7549" end_char="7549">"</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="7550" end_char="7553">Thus</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="7555" end_char="7557">far</TOKEN>
<TOKEN id="token-71-3" pos="punct" morph="none" start_char="7558" end_char="7558">,</TOKEN>
<TOKEN id="token-71-4" pos="word" morph="none" start_char="7560" end_char="7561">no</TOKEN>
<TOKEN id="token-71-5" pos="word" morph="none" start_char="7563" end_char="7572">scientists</TOKEN>
<TOKEN id="token-71-6" pos="word" morph="none" start_char="7574" end_char="7577">have</TOKEN>
<TOKEN id="token-71-7" pos="word" morph="none" start_char="7579" end_char="7587">confirmed</TOKEN>
<TOKEN id="token-71-8" pos="word" morph="none" start_char="7589" end_char="7590">or</TOKEN>
<TOKEN id="token-71-9" pos="word" morph="none" start_char="7592" end_char="7598">refuted</TOKEN>
<TOKEN id="token-71-10" pos="word" morph="none" start_char="7600" end_char="7602">the</TOKEN>
<TOKEN id="token-71-11" pos="word" morph="none" start_char="7604" end_char="7610">paper’s</TOKEN>
<TOKEN id="token-71-12" pos="word" morph="none" start_char="7612" end_char="7619">findings</TOKEN>
<TOKEN id="token-71-13" pos="punct" morph="none" start_char="7620" end_char="7622">.""</TOKEN>
</SEG>
<SEG id="segment-72" start_char="7624" end_char="7803">
<ORIGINAL_TEXT>The facts are that this very brief, poor quality paper has not been subjected to the process of peer-review by a scientific journal (or if it has, the world is unaware… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="word" morph="none" start_char="7624" end_char="7626">The</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="7628" end_char="7632">facts</TOKEN>
<TOKEN id="token-72-2" pos="word" morph="none" start_char="7634" end_char="7636">are</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="7638" end_char="7641">that</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="7643" end_char="7646">this</TOKEN>
<TOKEN id="token-72-5" pos="word" morph="none" start_char="7648" end_char="7651">very</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="7653" end_char="7657">brief</TOKEN>
<TOKEN id="token-72-7" pos="punct" morph="none" start_char="7658" end_char="7658">,</TOKEN>
<TOKEN id="token-72-8" pos="word" morph="none" start_char="7660" end_char="7663">poor</TOKEN>
<TOKEN id="token-72-9" pos="word" morph="none" start_char="7665" end_char="7671">quality</TOKEN>
<TOKEN id="token-72-10" pos="word" morph="none" start_char="7673" end_char="7677">paper</TOKEN>
<TOKEN id="token-72-11" pos="word" morph="none" start_char="7679" end_char="7681">has</TOKEN>
<TOKEN id="token-72-12" pos="word" morph="none" start_char="7683" end_char="7685">not</TOKEN>
<TOKEN id="token-72-13" pos="word" morph="none" start_char="7687" end_char="7690">been</TOKEN>
<TOKEN id="token-72-14" pos="word" morph="none" start_char="7692" end_char="7700">subjected</TOKEN>
<TOKEN id="token-72-15" pos="word" morph="none" start_char="7702" end_char="7703">to</TOKEN>
<TOKEN id="token-72-16" pos="word" morph="none" start_char="7705" end_char="7707">the</TOKEN>
<TOKEN id="token-72-17" pos="word" morph="none" start_char="7709" end_char="7715">process</TOKEN>
<TOKEN id="token-72-18" pos="word" morph="none" start_char="7717" end_char="7718">of</TOKEN>
<TOKEN id="token-72-19" pos="unknown" morph="none" start_char="7720" end_char="7730">peer-review</TOKEN>
<TOKEN id="token-72-20" pos="word" morph="none" start_char="7732" end_char="7733">by</TOKEN>
<TOKEN id="token-72-21" pos="word" morph="none" start_char="7735" end_char="7735">a</TOKEN>
<TOKEN id="token-72-22" pos="word" morph="none" start_char="7737" end_char="7746">scientific</TOKEN>
<TOKEN id="token-72-23" pos="word" morph="none" start_char="7748" end_char="7754">journal</TOKEN>
<TOKEN id="token-72-24" pos="punct" morph="none" start_char="7756" end_char="7756">(</TOKEN>
<TOKEN id="token-72-25" pos="word" morph="none" start_char="7757" end_char="7758">or</TOKEN>
<TOKEN id="token-72-26" pos="word" morph="none" start_char="7760" end_char="7761">if</TOKEN>
<TOKEN id="token-72-27" pos="word" morph="none" start_char="7763" end_char="7764">it</TOKEN>
<TOKEN id="token-72-28" pos="word" morph="none" start_char="7766" end_char="7768">has</TOKEN>
<TOKEN id="token-72-29" pos="punct" morph="none" start_char="7769" end_char="7769">,</TOKEN>
<TOKEN id="token-72-30" pos="word" morph="none" start_char="7771" end_char="7773">the</TOKEN>
<TOKEN id="token-72-31" pos="word" morph="none" start_char="7775" end_char="7779">world</TOKEN>
<TOKEN id="token-72-32" pos="word" morph="none" start_char="7781" end_char="7782">is</TOKEN>
<TOKEN id="token-72-33" pos="word" morph="none" start_char="7784" end_char="7790">unaware</TOKEN>
<TOKEN id="token-72-34" pos="punct" morph="none" start_char="7791" end_char="7791">…</TOKEN>
<TOKEN id="token-72-35" pos="word" morph="none" start_char="7793" end_char="7796">Read</TOKEN>
<TOKEN id="token-72-36" pos="word" morph="none" start_char="7798" end_char="7801">more</TOKEN>
<TOKEN id="token-72-37" pos="punct" morph="none" start_char="7803" end_char="7803">»</TOKEN>
</SEG>
<SEG id="segment-73" start_char="7807" end_char="8036">
<ORIGINAL_TEXT>" However, since we observed all notable SARS-CoV-2 features, including the optimized RBD and polybasic cleavage site, in related coronaviruses in nature, we do not believe that any type of laboratory-based scenario is plausible."</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="punct" morph="none" start_char="7807" end_char="7807">"</TOKEN>
<TOKEN id="token-73-1" pos="word" morph="none" start_char="7809" end_char="7815">However</TOKEN>
<TOKEN id="token-73-2" pos="punct" morph="none" start_char="7816" end_char="7816">,</TOKEN>
<TOKEN id="token-73-3" pos="word" morph="none" start_char="7818" end_char="7822">since</TOKEN>
<TOKEN id="token-73-4" pos="word" morph="none" start_char="7824" end_char="7825">we</TOKEN>
<TOKEN id="token-73-5" pos="word" morph="none" start_char="7827" end_char="7834">observed</TOKEN>
<TOKEN id="token-73-6" pos="word" morph="none" start_char="7836" end_char="7838">all</TOKEN>
<TOKEN id="token-73-7" pos="word" morph="none" start_char="7840" end_char="7846">notable</TOKEN>
<TOKEN id="token-73-8" pos="unknown" morph="none" start_char="7848" end_char="7857">SARS-CoV-2</TOKEN>
<TOKEN id="token-73-9" pos="word" morph="none" start_char="7859" end_char="7866">features</TOKEN>
<TOKEN id="token-73-10" pos="punct" morph="none" start_char="7867" end_char="7867">,</TOKEN>
<TOKEN id="token-73-11" pos="word" morph="none" start_char="7869" end_char="7877">including</TOKEN>
<TOKEN id="token-73-12" pos="word" morph="none" start_char="7879" end_char="7881">the</TOKEN>
<TOKEN id="token-73-13" pos="word" morph="none" start_char="7883" end_char="7891">optimized</TOKEN>
<TOKEN id="token-73-14" pos="word" morph="none" start_char="7893" end_char="7895">RBD</TOKEN>
<TOKEN id="token-73-15" pos="word" morph="none" start_char="7897" end_char="7899">and</TOKEN>
<TOKEN id="token-73-16" pos="word" morph="none" start_char="7901" end_char="7909">polybasic</TOKEN>
<TOKEN id="token-73-17" pos="word" morph="none" start_char="7911" end_char="7918">cleavage</TOKEN>
<TOKEN id="token-73-18" pos="word" morph="none" start_char="7920" end_char="7923">site</TOKEN>
<TOKEN id="token-73-19" pos="punct" morph="none" start_char="7924" end_char="7924">,</TOKEN>
<TOKEN id="token-73-20" pos="word" morph="none" start_char="7926" end_char="7927">in</TOKEN>
<TOKEN id="token-73-21" pos="word" morph="none" start_char="7929" end_char="7935">related</TOKEN>
<TOKEN id="token-73-22" pos="word" morph="none" start_char="7937" end_char="7949">coronaviruses</TOKEN>
<TOKEN id="token-73-23" pos="word" morph="none" start_char="7951" end_char="7952">in</TOKEN>
<TOKEN id="token-73-24" pos="word" morph="none" start_char="7954" end_char="7959">nature</TOKEN>
<TOKEN id="token-73-25" pos="punct" morph="none" start_char="7960" end_char="7960">,</TOKEN>
<TOKEN id="token-73-26" pos="word" morph="none" start_char="7962" end_char="7963">we</TOKEN>
<TOKEN id="token-73-27" pos="word" morph="none" start_char="7965" end_char="7966">do</TOKEN>
<TOKEN id="token-73-28" pos="word" morph="none" start_char="7968" end_char="7970">not</TOKEN>
<TOKEN id="token-73-29" pos="word" morph="none" start_char="7972" end_char="7978">believe</TOKEN>
<TOKEN id="token-73-30" pos="word" morph="none" start_char="7980" end_char="7983">that</TOKEN>
<TOKEN id="token-73-31" pos="word" morph="none" start_char="7985" end_char="7987">any</TOKEN>
<TOKEN id="token-73-32" pos="word" morph="none" start_char="7989" end_char="7992">type</TOKEN>
<TOKEN id="token-73-33" pos="word" morph="none" start_char="7994" end_char="7995">of</TOKEN>
<TOKEN id="token-73-34" pos="unknown" morph="none" start_char="7997" end_char="8012">laboratory-based</TOKEN>
<TOKEN id="token-73-35" pos="word" morph="none" start_char="8014" end_char="8021">scenario</TOKEN>
<TOKEN id="token-73-36" pos="word" morph="none" start_char="8023" end_char="8024">is</TOKEN>
<TOKEN id="token-73-37" pos="word" morph="none" start_char="8026" end_char="8034">plausible</TOKEN>
<TOKEN id="token-73-38" pos="punct" morph="none" start_char="8035" end_char="8036">."</TOKEN>
</SEG>
<SEG id="segment-74" start_char="8039" end_char="8139">
<ORIGINAL_TEXT>So all these features are found in nature, but they’re all also part of viruses used in lab research.</ORIGINAL_TEXT>
<TOKEN id="token-74-0" pos="word" morph="none" start_char="8039" end_char="8040">So</TOKEN>
<TOKEN id="token-74-1" pos="word" morph="none" start_char="8042" end_char="8044">all</TOKEN>
<TOKEN id="token-74-2" pos="word" morph="none" start_char="8046" end_char="8050">these</TOKEN>
<TOKEN id="token-74-3" pos="word" morph="none" start_char="8052" end_char="8059">features</TOKEN>
<TOKEN id="token-74-4" pos="word" morph="none" start_char="8061" end_char="8063">are</TOKEN>
<TOKEN id="token-74-5" pos="word" morph="none" start_char="8065" end_char="8069">found</TOKEN>
<TOKEN id="token-74-6" pos="word" morph="none" start_char="8071" end_char="8072">in</TOKEN>
<TOKEN id="token-74-7" pos="word" morph="none" start_char="8074" end_char="8079">nature</TOKEN>
<TOKEN id="token-74-8" pos="punct" morph="none" start_char="8080" end_char="8080">,</TOKEN>
<TOKEN id="token-74-9" pos="word" morph="none" start_char="8082" end_char="8084">but</TOKEN>
<TOKEN id="token-74-10" pos="word" morph="none" start_char="8086" end_char="8092">they’re</TOKEN>
<TOKEN id="token-74-11" pos="word" morph="none" start_char="8094" end_char="8096">all</TOKEN>
<TOKEN id="token-74-12" pos="word" morph="none" start_char="8098" end_char="8101">also</TOKEN>
<TOKEN id="token-74-13" pos="word" morph="none" start_char="8103" end_char="8106">part</TOKEN>
<TOKEN id="token-74-14" pos="word" morph="none" start_char="8108" end_char="8109">of</TOKEN>
<TOKEN id="token-74-15" pos="word" morph="none" start_char="8111" end_char="8117">viruses</TOKEN>
<TOKEN id="token-74-16" pos="word" morph="none" start_char="8119" end_char="8122">used</TOKEN>
<TOKEN id="token-74-17" pos="word" morph="none" start_char="8124" end_char="8125">in</TOKEN>
<TOKEN id="token-74-18" pos="word" morph="none" start_char="8127" end_char="8129">lab</TOKEN>
<TOKEN id="token-74-19" pos="word" morph="none" start_char="8131" end_char="8138">research</TOKEN>
<TOKEN id="token-74-20" pos="punct" morph="none" start_char="8139" end_char="8139">.</TOKEN>
</SEG>
<SEG id="segment-75" start_char="8141" end_char="8195">
<ORIGINAL_TEXT>That’s a terrible argument against a lab-escaped virus.</ORIGINAL_TEXT>
<TOKEN id="token-75-0" pos="word" morph="none" start_char="8141" end_char="8146">That’s</TOKEN>
<TOKEN id="token-75-1" pos="word" morph="none" start_char="8148" end_char="8148">a</TOKEN>
<TOKEN id="token-75-2" pos="word" morph="none" start_char="8150" end_char="8157">terrible</TOKEN>
<TOKEN id="token-75-3" pos="word" morph="none" start_char="8159" end_char="8166">argument</TOKEN>
<TOKEN id="token-75-4" pos="word" morph="none" start_char="8168" end_char="8174">against</TOKEN>
<TOKEN id="token-75-5" pos="word" morph="none" start_char="8176" end_char="8176">a</TOKEN>
<TOKEN id="token-75-6" pos="unknown" morph="none" start_char="8178" end_char="8188">lab-escaped</TOKEN>
<TOKEN id="token-75-7" pos="word" morph="none" start_char="8190" end_char="8194">virus</TOKEN>
<TOKEN id="token-75-8" pos="punct" morph="none" start_char="8195" end_char="8195">.</TOKEN>
</SEG>
<SEG id="segment-76" start_char="8199" end_char="8330">
<ORIGINAL_TEXT>The line about ResearchGate having removed the South China Institute of technology paper from their site does not appear to be true:</ORIGINAL_TEXT>
<TOKEN id="token-76-0" pos="word" morph="none" start_char="8199" end_char="8201">The</TOKEN>
<TOKEN id="token-76-1" pos="word" morph="none" start_char="8203" end_char="8206">line</TOKEN>
<TOKEN id="token-76-2" pos="word" morph="none" start_char="8208" end_char="8212">about</TOKEN>
<TOKEN id="token-76-3" pos="word" morph="none" start_char="8214" end_char="8225">ResearchGate</TOKEN>
<TOKEN id="token-76-4" pos="word" morph="none" start_char="8227" end_char="8232">having</TOKEN>
<TOKEN id="token-76-5" pos="word" morph="none" start_char="8234" end_char="8240">removed</TOKEN>
<TOKEN id="token-76-6" pos="word" morph="none" start_char="8242" end_char="8244">the</TOKEN>
<TOKEN id="token-76-7" pos="word" morph="none" start_char="8246" end_char="8250">South</TOKEN>
<TOKEN id="token-76-8" pos="word" morph="none" start_char="8252" end_char="8256">China</TOKEN>
<TOKEN id="token-76-9" pos="word" morph="none" start_char="8258" end_char="8266">Institute</TOKEN>
<TOKEN id="token-76-10" pos="word" morph="none" start_char="8268" end_char="8269">of</TOKEN>
<TOKEN id="token-76-11" pos="word" morph="none" start_char="8271" end_char="8280">technology</TOKEN>
<TOKEN id="token-76-12" pos="word" morph="none" start_char="8282" end_char="8286">paper</TOKEN>
<TOKEN id="token-76-13" pos="word" morph="none" start_char="8288" end_char="8291">from</TOKEN>
<TOKEN id="token-76-14" pos="word" morph="none" start_char="8293" end_char="8297">their</TOKEN>
<TOKEN id="token-76-15" pos="word" morph="none" start_char="8299" end_char="8302">site</TOKEN>
<TOKEN id="token-76-16" pos="word" morph="none" start_char="8304" end_char="8307">does</TOKEN>
<TOKEN id="token-76-17" pos="word" morph="none" start_char="8309" end_char="8311">not</TOKEN>
<TOKEN id="token-76-18" pos="word" morph="none" start_char="8313" end_char="8318">appear</TOKEN>
<TOKEN id="token-76-19" pos="word" morph="none" start_char="8320" end_char="8321">to</TOKEN>
<TOKEN id="token-76-20" pos="word" morph="none" start_char="8323" end_char="8324">be</TOKEN>
<TOKEN id="token-76-21" pos="word" morph="none" start_char="8326" end_char="8329">true</TOKEN>
<TOKEN id="token-76-22" pos="punct" morph="none" start_char="8330" end_char="8330">:</TOKEN>
</SEG>
<SEG id="segment-77" start_char="8333" end_char="8520">
<ORIGINAL_TEXT>https://www.researchgate.net/publication/332998684_Novel_Bat_Alphacoronaviruses_in_Southern_China_Support_Chinese_Horseshoe_Bats_as_an_Important_Reservoir_for_Potential_Novel_Coronaviruses</ORIGINAL_TEXT>
<TOKEN id="token-77-0" pos="url" morph="none" start_char="8333" end_char="8520">https://www.researchgate.net/publication/332998684_Novel_Bat_Alphacoronaviruses_in_Southern_China_Support_Chinese_Horseshoe_Bats_as_an_Important_Reservoir_for_Potential_Novel_Coronaviruses</TOKEN>
</SEG>
<SEG id="segment-78" start_char="8523" end_char="8547">
<ORIGINAL_TEXT>It still shows up for me.</ORIGINAL_TEXT>
<TOKEN id="token-78-0" pos="word" morph="none" start_char="8523" end_char="8524">It</TOKEN>
<TOKEN id="token-78-1" pos="word" morph="none" start_char="8526" end_char="8530">still</TOKEN>
<TOKEN id="token-78-2" pos="word" morph="none" start_char="8532" end_char="8536">shows</TOKEN>
<TOKEN id="token-78-3" pos="word" morph="none" start_char="8538" end_char="8539">up</TOKEN>
<TOKEN id="token-78-4" pos="word" morph="none" start_char="8541" end_char="8543">for</TOKEN>
<TOKEN id="token-78-5" pos="word" morph="none" start_char="8545" end_char="8546">me</TOKEN>
<TOKEN id="token-78-6" pos="punct" morph="none" start_char="8547" end_char="8547">.</TOKEN>
</SEG>
<SEG id="segment-79" start_char="8551" end_char="8586">
<ORIGINAL_TEXT>That is an entirely different study.</ORIGINAL_TEXT>
<TOKEN id="token-79-0" pos="word" morph="none" start_char="8551" end_char="8554">That</TOKEN>
<TOKEN id="token-79-1" pos="word" morph="none" start_char="8556" end_char="8557">is</TOKEN>
<TOKEN id="token-79-2" pos="word" morph="none" start_char="8559" end_char="8560">an</TOKEN>
<TOKEN id="token-79-3" pos="word" morph="none" start_char="8562" end_char="8569">entirely</TOKEN>
<TOKEN id="token-79-4" pos="word" morph="none" start_char="8571" end_char="8579">different</TOKEN>
<TOKEN id="token-79-5" pos="word" morph="none" start_char="8581" end_char="8585">study</TOKEN>
<TOKEN id="token-79-6" pos="punct" morph="none" start_char="8586" end_char="8586">.</TOKEN>
</SEG>
<SEG id="segment-80" start_char="8588" end_char="8629">
<ORIGINAL_TEXT>Click on hyperlink "study" in the article.</ORIGINAL_TEXT>
<TOKEN id="token-80-0" pos="word" morph="none" start_char="8588" end_char="8592">Click</TOKEN>
<TOKEN id="token-80-1" pos="word" morph="none" start_char="8594" end_char="8595">on</TOKEN>
<TOKEN id="token-80-2" pos="word" morph="none" start_char="8597" end_char="8605">hyperlink</TOKEN>
<TOKEN id="token-80-3" pos="punct" morph="none" start_char="8607" end_char="8607">"</TOKEN>
<TOKEN id="token-80-4" pos="word" morph="none" start_char="8608" end_char="8612">study</TOKEN>
<TOKEN id="token-80-5" pos="punct" morph="none" start_char="8613" end_char="8613">"</TOKEN>
<TOKEN id="token-80-6" pos="word" morph="none" start_char="8615" end_char="8616">in</TOKEN>
<TOKEN id="token-80-7" pos="word" morph="none" start_char="8618" end_char="8620">the</TOKEN>
<TOKEN id="token-80-8" pos="word" morph="none" start_char="8622" end_char="8628">article</TOKEN>
<TOKEN id="token-80-9" pos="punct" morph="none" start_char="8629" end_char="8629">.</TOKEN>
</SEG>
<SEG id="segment-81" start_char="8633" end_char="8846">
<ORIGINAL_TEXT>It is obvious Chinese researchers in Wuhan were studying corona viruses for years and isolated at least one that uses the ACE2 receptor as the binding receptor for the virus, the exact same mechanism Covid-19 uses.</ORIGINAL_TEXT>
<TOKEN id="token-81-0" pos="word" morph="none" start_char="8633" end_char="8634">It</TOKEN>
<TOKEN id="token-81-1" pos="word" morph="none" start_char="8636" end_char="8637">is</TOKEN>
<TOKEN id="token-81-2" pos="word" morph="none" start_char="8639" end_char="8645">obvious</TOKEN>
<TOKEN id="token-81-3" pos="word" morph="none" start_char="8647" end_char="8653">Chinese</TOKEN>
<TOKEN id="token-81-4" pos="word" morph="none" start_char="8655" end_char="8665">researchers</TOKEN>
<TOKEN id="token-81-5" pos="word" morph="none" start_char="8667" end_char="8668">in</TOKEN>
<TOKEN id="token-81-6" pos="word" morph="none" start_char="8670" end_char="8674">Wuhan</TOKEN>
<TOKEN id="token-81-7" pos="word" morph="none" start_char="8676" end_char="8679">were</TOKEN>
<TOKEN id="token-81-8" pos="word" morph="none" start_char="8681" end_char="8688">studying</TOKEN>
<TOKEN id="token-81-9" pos="word" morph="none" start_char="8690" end_char="8695">corona</TOKEN>
<TOKEN id="token-81-10" pos="word" morph="none" start_char="8697" end_char="8703">viruses</TOKEN>
<TOKEN id="token-81-11" pos="word" morph="none" start_char="8705" end_char="8707">for</TOKEN>
<TOKEN id="token-81-12" pos="word" morph="none" start_char="8709" end_char="8713">years</TOKEN>
<TOKEN id="token-81-13" pos="word" morph="none" start_char="8715" end_char="8717">and</TOKEN>
<TOKEN id="token-81-14" pos="word" morph="none" start_char="8719" end_char="8726">isolated</TOKEN>
<TOKEN id="token-81-15" pos="word" morph="none" start_char="8728" end_char="8729">at</TOKEN>
<TOKEN id="token-81-16" pos="word" morph="none" start_char="8731" end_char="8735">least</TOKEN>
<TOKEN id="token-81-17" pos="word" morph="none" start_char="8737" end_char="8739">one</TOKEN>
<TOKEN id="token-81-18" pos="word" morph="none" start_char="8741" end_char="8744">that</TOKEN>
<TOKEN id="token-81-19" pos="word" morph="none" start_char="8746" end_char="8749">uses</TOKEN>
<TOKEN id="token-81-20" pos="word" morph="none" start_char="8751" end_char="8753">the</TOKEN>
<TOKEN id="token-81-21" pos="word" morph="none" start_char="8755" end_char="8758">ACE2</TOKEN>
<TOKEN id="token-81-22" pos="word" morph="none" start_char="8760" end_char="8767">receptor</TOKEN>
<TOKEN id="token-81-23" pos="word" morph="none" start_char="8769" end_char="8770">as</TOKEN>
<TOKEN id="token-81-24" pos="word" morph="none" start_char="8772" end_char="8774">the</TOKEN>
<TOKEN id="token-81-25" pos="word" morph="none" start_char="8776" end_char="8782">binding</TOKEN>
<TOKEN id="token-81-26" pos="word" morph="none" start_char="8784" end_char="8791">receptor</TOKEN>
<TOKEN id="token-81-27" pos="word" morph="none" start_char="8793" end_char="8795">for</TOKEN>
<TOKEN id="token-81-28" pos="word" morph="none" start_char="8797" end_char="8799">the</TOKEN>
<TOKEN id="token-81-29" pos="word" morph="none" start_char="8801" end_char="8805">virus</TOKEN>
<TOKEN id="token-81-30" pos="punct" morph="none" start_char="8806" end_char="8806">,</TOKEN>
<TOKEN id="token-81-31" pos="word" morph="none" start_char="8808" end_char="8810">the</TOKEN>
<TOKEN id="token-81-32" pos="word" morph="none" start_char="8812" end_char="8816">exact</TOKEN>
<TOKEN id="token-81-33" pos="word" morph="none" start_char="8818" end_char="8821">same</TOKEN>
<TOKEN id="token-81-34" pos="word" morph="none" start_char="8823" end_char="8831">mechanism</TOKEN>
<TOKEN id="token-81-35" pos="unknown" morph="none" start_char="8833" end_char="8840">Covid-19</TOKEN>
<TOKEN id="token-81-36" pos="word" morph="none" start_char="8842" end_char="8845">uses</TOKEN>
<TOKEN id="token-81-37" pos="punct" morph="none" start_char="8846" end_char="8846">.</TOKEN>
</SEG>
<SEG id="segment-82" start_char="8848" end_char="8907">
<ORIGINAL_TEXT>see this paper- https://www.ncbi.nlm.nih.gov/pubmed/24172901</ORIGINAL_TEXT>
<TOKEN id="token-82-0" pos="word" morph="none" start_char="8848" end_char="8850">see</TOKEN>
<TOKEN id="token-82-1" pos="word" morph="none" start_char="8852" end_char="8855">this</TOKEN>
<TOKEN id="token-82-2" pos="word" morph="none" start_char="8857" end_char="8861">paper</TOKEN>
<TOKEN id="token-82-3" pos="punct" morph="none" start_char="8862" end_char="8862">-</TOKEN>
<TOKEN id="token-82-4" pos="url" morph="none" start_char="8864" end_char="8907">https://www.ncbi.nlm.nih.gov/pubmed/24172901</TOKEN>
</SEG>
<SEG id="segment-83" start_char="8911" end_char="9023">
<ORIGINAL_TEXT>The best way I guess is to make inventory of Wuhan bio lab scientists /staff whether they are still alive or not.</ORIGINAL_TEXT>
<TOKEN id="token-83-0" pos="word" morph="none" start_char="8911" end_char="8913">The</TOKEN>
<TOKEN id="token-83-1" pos="word" morph="none" start_char="8915" end_char="8918">best</TOKEN>
<TOKEN id="token-83-2" pos="word" morph="none" start_char="8920" end_char="8922">way</TOKEN>
<TOKEN id="token-83-3" pos="word" morph="none" start_char="8924" end_char="8924">I</TOKEN>
<TOKEN id="token-83-4" pos="word" morph="none" start_char="8926" end_char="8930">guess</TOKEN>
<TOKEN id="token-83-5" pos="word" morph="none" start_char="8932" end_char="8933">is</TOKEN>
<TOKEN id="token-83-6" pos="word" morph="none" start_char="8935" end_char="8936">to</TOKEN>
<TOKEN id="token-83-7" pos="word" morph="none" start_char="8938" end_char="8941">make</TOKEN>
<TOKEN id="token-83-8" pos="word" morph="none" start_char="8943" end_char="8951">inventory</TOKEN>
<TOKEN id="token-83-9" pos="word" morph="none" start_char="8953" end_char="8954">of</TOKEN>
<TOKEN id="token-83-10" pos="word" morph="none" start_char="8956" end_char="8960">Wuhan</TOKEN>
<TOKEN id="token-83-11" pos="word" morph="none" start_char="8962" end_char="8964">bio</TOKEN>
<TOKEN id="token-83-12" pos="word" morph="none" start_char="8966" end_char="8968">lab</TOKEN>
<TOKEN id="token-83-13" pos="word" morph="none" start_char="8970" end_char="8979">scientists</TOKEN>
<TOKEN id="token-83-14" pos="punct" morph="none" start_char="8981" end_char="8981">/</TOKEN>
<TOKEN id="token-83-15" pos="word" morph="none" start_char="8982" end_char="8986">staff</TOKEN>
<TOKEN id="token-83-16" pos="word" morph="none" start_char="8988" end_char="8994">whether</TOKEN>
<TOKEN id="token-83-17" pos="word" morph="none" start_char="8996" end_char="8999">they</TOKEN>
<TOKEN id="token-83-18" pos="word" morph="none" start_char="9001" end_char="9003">are</TOKEN>
<TOKEN id="token-83-19" pos="word" morph="none" start_char="9005" end_char="9009">still</TOKEN>
<TOKEN id="token-83-20" pos="word" morph="none" start_char="9011" end_char="9015">alive</TOKEN>
<TOKEN id="token-83-21" pos="word" morph="none" start_char="9017" end_char="9018">or</TOKEN>
<TOKEN id="token-83-22" pos="word" morph="none" start_char="9020" end_char="9022">not</TOKEN>
<TOKEN id="token-83-23" pos="punct" morph="none" start_char="9023" end_char="9023">.</TOKEN>
</SEG>
<SEG id="segment-84" start_char="9025" end_char="9082">
<ORIGINAL_TEXT>And if not, the cause of death and date and time of death.</ORIGINAL_TEXT>
<TOKEN id="token-84-0" pos="word" morph="none" start_char="9025" end_char="9027">And</TOKEN>
<TOKEN id="token-84-1" pos="word" morph="none" start_char="9029" end_char="9030">if</TOKEN>
<TOKEN id="token-84-2" pos="word" morph="none" start_char="9032" end_char="9034">not</TOKEN>
<TOKEN id="token-84-3" pos="punct" morph="none" start_char="9035" end_char="9035">,</TOKEN>
<TOKEN id="token-84-4" pos="word" morph="none" start_char="9037" end_char="9039">the</TOKEN>
<TOKEN id="token-84-5" pos="word" morph="none" start_char="9041" end_char="9045">cause</TOKEN>
<TOKEN id="token-84-6" pos="word" morph="none" start_char="9047" end_char="9048">of</TOKEN>
<TOKEN id="token-84-7" pos="word" morph="none" start_char="9050" end_char="9054">death</TOKEN>
<TOKEN id="token-84-8" pos="word" morph="none" start_char="9056" end_char="9058">and</TOKEN>
<TOKEN id="token-84-9" pos="word" morph="none" start_char="9060" end_char="9063">date</TOKEN>
<TOKEN id="token-84-10" pos="word" morph="none" start_char="9065" end_char="9067">and</TOKEN>
<TOKEN id="token-84-11" pos="word" morph="none" start_char="9069" end_char="9072">time</TOKEN>
<TOKEN id="token-84-12" pos="word" morph="none" start_char="9074" end_char="9075">of</TOKEN>
<TOKEN id="token-84-13" pos="word" morph="none" start_char="9077" end_char="9081">death</TOKEN>
<TOKEN id="token-84-14" pos="punct" morph="none" start_char="9082" end_char="9082">.</TOKEN>
</SEG>
<SEG id="segment-85" start_char="9084" end_char="9168">
<ORIGINAL_TEXT>Experts say it is important to find Patient 0 so they can understand better COVID19..</ORIGINAL_TEXT>
<TOKEN id="token-85-0" pos="word" morph="none" start_char="9084" end_char="9090">Experts</TOKEN>
<TOKEN id="token-85-1" pos="word" morph="none" start_char="9092" end_char="9094">say</TOKEN>
<TOKEN id="token-85-2" pos="word" morph="none" start_char="9096" end_char="9097">it</TOKEN>
<TOKEN id="token-85-3" pos="word" morph="none" start_char="9099" end_char="9100">is</TOKEN>
<TOKEN id="token-85-4" pos="word" morph="none" start_char="9102" end_char="9110">important</TOKEN>
<TOKEN id="token-85-5" pos="word" morph="none" start_char="9112" end_char="9113">to</TOKEN>
<TOKEN id="token-85-6" pos="word" morph="none" start_char="9115" end_char="9118">find</TOKEN>
<TOKEN id="token-85-7" pos="word" morph="none" start_char="9120" end_char="9126">Patient</TOKEN>
<TOKEN id="token-85-8" pos="word" morph="none" start_char="9128" end_char="9128">0</TOKEN>
<TOKEN id="token-85-9" pos="word" morph="none" start_char="9130" end_char="9131">so</TOKEN>
<TOKEN id="token-85-10" pos="word" morph="none" start_char="9133" end_char="9136">they</TOKEN>
<TOKEN id="token-85-11" pos="word" morph="none" start_char="9138" end_char="9140">can</TOKEN>
<TOKEN id="token-85-12" pos="word" morph="none" start_char="9142" end_char="9151">understand</TOKEN>
<TOKEN id="token-85-13" pos="word" morph="none" start_char="9153" end_char="9158">better</TOKEN>
<TOKEN id="token-85-14" pos="word" morph="none" start_char="9160" end_char="9166">COVID19</TOKEN>
<TOKEN id="token-85-15" pos="punct" morph="none" start_char="9167" end_char="9168">..</TOKEN>
</SEG>
<SEG id="segment-86" start_char="9172" end_char="9251">
<ORIGINAL_TEXT>Actually lab-escape theory is so far the more widely accepted conspiracy theory.</ORIGINAL_TEXT>
<TOKEN id="token-86-0" pos="word" morph="none" start_char="9172" end_char="9179">Actually</TOKEN>
<TOKEN id="token-86-1" pos="unknown" morph="none" start_char="9181" end_char="9190">lab-escape</TOKEN>
<TOKEN id="token-86-2" pos="word" morph="none" start_char="9192" end_char="9197">theory</TOKEN>
<TOKEN id="token-86-3" pos="word" morph="none" start_char="9199" end_char="9200">is</TOKEN>
<TOKEN id="token-86-4" pos="word" morph="none" start_char="9202" end_char="9203">so</TOKEN>
<TOKEN id="token-86-5" pos="word" morph="none" start_char="9205" end_char="9207">far</TOKEN>
<TOKEN id="token-86-6" pos="word" morph="none" start_char="9209" end_char="9211">the</TOKEN>
<TOKEN id="token-86-7" pos="word" morph="none" start_char="9213" end_char="9216">more</TOKEN>
<TOKEN id="token-86-8" pos="word" morph="none" start_char="9218" end_char="9223">widely</TOKEN>
<TOKEN id="token-86-9" pos="word" morph="none" start_char="9225" end_char="9232">accepted</TOKEN>
<TOKEN id="token-86-10" pos="word" morph="none" start_char="9234" end_char="9243">conspiracy</TOKEN>
<TOKEN id="token-86-11" pos="word" morph="none" start_char="9245" end_char="9250">theory</TOKEN>
<TOKEN id="token-86-12" pos="punct" morph="none" start_char="9251" end_char="9251">.</TOKEN>
</SEG>
<SEG id="segment-87" start_char="9253" end_char="9389">
<ORIGINAL_TEXT>It had been circulating on social media for weeks, and gained considerable visibility following a New York Post article in late February.</ORIGINAL_TEXT>
<TOKEN id="token-87-0" pos="word" morph="none" start_char="9253" end_char="9254">It</TOKEN>
<TOKEN id="token-87-1" pos="word" morph="none" start_char="9256" end_char="9258">had</TOKEN>
<TOKEN id="token-87-2" pos="word" morph="none" start_char="9260" end_char="9263">been</TOKEN>
<TOKEN id="token-87-3" pos="word" morph="none" start_char="9265" end_char="9275">circulating</TOKEN>
<TOKEN id="token-87-4" pos="word" morph="none" start_char="9277" end_char="9278">on</TOKEN>
<TOKEN id="token-87-5" pos="word" morph="none" start_char="9280" end_char="9285">social</TOKEN>
<TOKEN id="token-87-6" pos="word" morph="none" start_char="9287" end_char="9291">media</TOKEN>
<TOKEN id="token-87-7" pos="word" morph="none" start_char="9293" end_char="9295">for</TOKEN>
<TOKEN id="token-87-8" pos="word" morph="none" start_char="9297" end_char="9301">weeks</TOKEN>
<TOKEN id="token-87-9" pos="punct" morph="none" start_char="9302" end_char="9302">,</TOKEN>
<TOKEN id="token-87-10" pos="word" morph="none" start_char="9304" end_char="9306">and</TOKEN>
<TOKEN id="token-87-11" pos="word" morph="none" start_char="9308" end_char="9313">gained</TOKEN>
<TOKEN id="token-87-12" pos="word" morph="none" start_char="9315" end_char="9326">considerable</TOKEN>
<TOKEN id="token-87-13" pos="word" morph="none" start_char="9328" end_char="9337">visibility</TOKEN>
<TOKEN id="token-87-14" pos="word" morph="none" start_char="9339" end_char="9347">following</TOKEN>
<TOKEN id="token-87-15" pos="word" morph="none" start_char="9349" end_char="9349">a</TOKEN>
<TOKEN id="token-87-16" pos="word" morph="none" start_char="9351" end_char="9353">New</TOKEN>
<TOKEN id="token-87-17" pos="word" morph="none" start_char="9355" end_char="9358">York</TOKEN>
<TOKEN id="token-87-18" pos="word" morph="none" start_char="9360" end_char="9363">Post</TOKEN>
<TOKEN id="token-87-19" pos="word" morph="none" start_char="9365" end_char="9371">article</TOKEN>
<TOKEN id="token-87-20" pos="word" morph="none" start_char="9373" end_char="9374">in</TOKEN>
<TOKEN id="token-87-21" pos="word" morph="none" start_char="9376" end_char="9379">late</TOKEN>
<TOKEN id="token-87-22" pos="word" morph="none" start_char="9381" end_char="9388">February</TOKEN>
<TOKEN id="token-87-23" pos="punct" morph="none" start_char="9389" end_char="9389">.</TOKEN>
</SEG>
<SEG id="segment-88" start_char="9391" end_char="9498">
<ORIGINAL_TEXT>But there is a lot of scholarly evidence to suggest that coronavirus was not manufactured in the laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-88-0" pos="word" morph="none" start_char="9391" end_char="9393">But</TOKEN>
<TOKEN id="token-88-1" pos="word" morph="none" start_char="9395" end_char="9399">there</TOKEN>
<TOKEN id="token-88-2" pos="word" morph="none" start_char="9401" end_char="9402">is</TOKEN>
<TOKEN id="token-88-3" pos="word" morph="none" start_char="9404" end_char="9404">a</TOKEN>
<TOKEN id="token-88-4" pos="word" morph="none" start_char="9406" end_char="9408">lot</TOKEN>
<TOKEN id="token-88-5" pos="word" morph="none" start_char="9410" end_char="9411">of</TOKEN>
<TOKEN id="token-88-6" pos="word" morph="none" start_char="9413" end_char="9421">scholarly</TOKEN>
<TOKEN id="token-88-7" pos="word" morph="none" start_char="9423" end_char="9430">evidence</TOKEN>
<TOKEN id="token-88-8" pos="word" morph="none" start_char="9432" end_char="9433">to</TOKEN>
<TOKEN id="token-88-9" pos="word" morph="none" start_char="9435" end_char="9441">suggest</TOKEN>
<TOKEN id="token-88-10" pos="word" morph="none" start_char="9443" end_char="9446">that</TOKEN>
<TOKEN id="token-88-11" pos="word" morph="none" start_char="9448" end_char="9458">coronavirus</TOKEN>
<TOKEN id="token-88-12" pos="word" morph="none" start_char="9460" end_char="9462">was</TOKEN>
<TOKEN id="token-88-13" pos="word" morph="none" start_char="9464" end_char="9466">not</TOKEN>
<TOKEN id="token-88-14" pos="word" morph="none" start_char="9468" end_char="9479">manufactured</TOKEN>
<TOKEN id="token-88-15" pos="word" morph="none" start_char="9481" end_char="9482">in</TOKEN>
<TOKEN id="token-88-16" pos="word" morph="none" start_char="9484" end_char="9486">the</TOKEN>
<TOKEN id="token-88-17" pos="word" morph="none" start_char="9488" end_char="9497">laboratory</TOKEN>
<TOKEN id="token-88-18" pos="punct" morph="none" start_char="9498" end_char="9498">.</TOKEN>
</SEG>
<SEG id="segment-89" start_char="9500" end_char="9597">
<ORIGINAL_TEXT>Even if the exact source of the disease is not known yet, the virus originally came from wildlife.</ORIGINAL_TEXT>
<TOKEN id="token-89-0" pos="word" morph="none" start_char="9500" end_char="9503">Even</TOKEN>
<TOKEN id="token-89-1" pos="word" morph="none" start_char="9505" end_char="9506">if</TOKEN>
<TOKEN id="token-89-2" pos="word" morph="none" start_char="9508" end_char="9510">the</TOKEN>
<TOKEN id="token-89-3" pos="word" morph="none" start_char="9512" end_char="9516">exact</TOKEN>
<TOKEN id="token-89-4" pos="word" morph="none" start_char="9518" end_char="9523">source</TOKEN>
<TOKEN id="token-89-5" pos="word" morph="none" start_char="9525" end_char="9526">of</TOKEN>
<TOKEN id="token-89-6" pos="word" morph="none" start_char="9528" end_char="9530">the</TOKEN>
<TOKEN id="token-89-7" pos="word" morph="none" start_char="9532" end_char="9538">disease</TOKEN>
<TOKEN id="token-89-8" pos="word" morph="none" start_char="9540" end_char="9541">is</TOKEN>
<TOKEN id="token-89-9" pos="word" morph="none" start_char="9543" end_char="9545">not</TOKEN>
<TOKEN id="token-89-10" pos="word" morph="none" start_char="9547" end_char="9551">known</TOKEN>
<TOKEN id="token-89-11" pos="word" morph="none" start_char="9553" end_char="9555">yet</TOKEN>
<TOKEN id="token-89-12" pos="punct" morph="none" start_char="9556" end_char="9556">,</TOKEN>
<TOKEN id="token-89-13" pos="word" morph="none" start_char="9558" end_char="9560">the</TOKEN>
<TOKEN id="token-89-14" pos="word" morph="none" start_char="9562" end_char="9566">virus</TOKEN>
<TOKEN id="token-89-15" pos="word" morph="none" start_char="9568" end_char="9577">originally</TOKEN>
<TOKEN id="token-89-16" pos="word" morph="none" start_char="9579" end_char="9582">came</TOKEN>
<TOKEN id="token-89-17" pos="word" morph="none" start_char="9584" end_char="9587">from</TOKEN>
<TOKEN id="token-89-18" pos="word" morph="none" start_char="9589" end_char="9596">wildlife</TOKEN>
<TOKEN id="token-89-19" pos="punct" morph="none" start_char="9597" end_char="9597">.</TOKEN>
</SEG>
<SEG id="segment-90" start_char="9599" end_char="9747">
<ORIGINAL_TEXT>In the past , these viruses have spread through wild bats that infect another type of animal – an intermediate host – that then spreads it to humans.</ORIGINAL_TEXT>
<TOKEN id="token-90-0" pos="word" morph="none" start_char="9599" end_char="9600">In</TOKEN>
<TOKEN id="token-90-1" pos="word" morph="none" start_char="9602" end_char="9604">the</TOKEN>
<TOKEN id="token-90-2" pos="word" morph="none" start_char="9606" end_char="9609">past</TOKEN>
<TOKEN id="token-90-3" pos="punct" morph="none" start_char="9611" end_char="9611">,</TOKEN>
<TOKEN id="token-90-4" pos="word" morph="none" start_char="9613" end_char="9617">these</TOKEN>
<TOKEN id="token-90-5" pos="word" morph="none" start_char="9619" end_char="9625">viruses</TOKEN>
<TOKEN id="token-90-6" pos="word" morph="none" start_char="9627" end_char="9630">have</TOKEN>
<TOKEN id="token-90-7" pos="word" morph="none" start_char="9632" end_char="9637">spread</TOKEN>
<TOKEN id="token-90-8" pos="word" morph="none" start_char="9639" end_char="9645">through</TOKEN>
<TOKEN id="token-90-9" pos="word" morph="none" start_char="9647" end_char="9650">wild</TOKEN>
<TOKEN id="token-90-10" pos="word" morph="none" start_char="9652" end_char="9655">bats</TOKEN>
<TOKEN id="token-90-11" pos="word" morph="none" start_char="9657" end_char="9660">that</TOKEN>
<TOKEN id="token-90-12" pos="word" morph="none" start_char="9662" end_char="9667">infect</TOKEN>
<TOKEN id="token-90-13" pos="word" morph="none" start_char="9669" end_char="9675">another</TOKEN>
<TOKEN id="token-90-14" pos="word" morph="none" start_char="9677" end_char="9680">type</TOKEN>
<TOKEN id="token-90-15" pos="word" morph="none" start_char="9682" end_char="9683">of</TOKEN>
<TOKEN id="token-90-16" pos="word" morph="none" start_char="9685" end_char="9690">animal</TOKEN>
<TOKEN id="token-90-17" pos="punct" morph="none" start_char="9692" end_char="9692">–</TOKEN>
<TOKEN id="token-90-18" pos="word" morph="none" start_char="9694" end_char="9695">an</TOKEN>
<TOKEN id="token-90-19" pos="word" morph="none" start_char="9697" end_char="9708">intermediate</TOKEN>
<TOKEN id="token-90-20" pos="word" morph="none" start_char="9710" end_char="9713">host</TOKEN>
<TOKEN id="token-90-21" pos="punct" morph="none" start_char="9715" end_char="9715">–</TOKEN>
<TOKEN id="token-90-22" pos="word" morph="none" start_char="9717" end_char="9720">that</TOKEN>
<TOKEN id="token-90-23" pos="word" morph="none" start_char="9722" end_char="9725">then</TOKEN>
<TOKEN id="token-90-24" pos="word" morph="none" start_char="9727" end_char="9733">spreads</TOKEN>
<TOKEN id="token-90-25" pos="word" morph="none" start_char="9735" end_char="9736">it</TOKEN>
<TOKEN id="token-90-26" pos="word" morph="none" start_char="9738" end_char="9739">to</TOKEN>
<TOKEN id="token-90-27" pos="word" morph="none" start_char="9741" end_char="9746">humans</TOKEN>
<TOKEN id="token-90-28" pos="punct" morph="none" start_char="9747" end_char="9747">.</TOKEN>
</SEG>
<SEG id="segment-91" start_char="9751" end_char="9860">
<ORIGINAL_TEXT>So first you make sure to point out the "social media conspiracy theory" to set the stage for what comes next.</ORIGINAL_TEXT>
<TOKEN id="token-91-0" pos="word" morph="none" start_char="9751" end_char="9752">So</TOKEN>
<TOKEN id="token-91-1" pos="word" morph="none" start_char="9754" end_char="9758">first</TOKEN>
<TOKEN id="token-91-2" pos="word" morph="none" start_char="9760" end_char="9762">you</TOKEN>
<TOKEN id="token-91-3" pos="word" morph="none" start_char="9764" end_char="9767">make</TOKEN>
<TOKEN id="token-91-4" pos="word" morph="none" start_char="9769" end_char="9772">sure</TOKEN>
<TOKEN id="token-91-5" pos="word" morph="none" start_char="9774" end_char="9775">to</TOKEN>
<TOKEN id="token-91-6" pos="word" morph="none" start_char="9777" end_char="9781">point</TOKEN>
<TOKEN id="token-91-7" pos="word" morph="none" start_char="9783" end_char="9785">out</TOKEN>
<TOKEN id="token-91-8" pos="word" morph="none" start_char="9787" end_char="9789">the</TOKEN>
<TOKEN id="token-91-9" pos="punct" morph="none" start_char="9791" end_char="9791">"</TOKEN>
<TOKEN id="token-91-10" pos="word" morph="none" start_char="9792" end_char="9797">social</TOKEN>
<TOKEN id="token-91-11" pos="word" morph="none" start_char="9799" end_char="9803">media</TOKEN>
<TOKEN id="token-91-12" pos="word" morph="none" start_char="9805" end_char="9814">conspiracy</TOKEN>
<TOKEN id="token-91-13" pos="word" morph="none" start_char="9816" end_char="9821">theory</TOKEN>
<TOKEN id="token-91-14" pos="punct" morph="none" start_char="9822" end_char="9822">"</TOKEN>
<TOKEN id="token-91-15" pos="word" morph="none" start_char="9824" end_char="9825">to</TOKEN>
<TOKEN id="token-91-16" pos="word" morph="none" start_char="9827" end_char="9829">set</TOKEN>
<TOKEN id="token-91-17" pos="word" morph="none" start_char="9831" end_char="9833">the</TOKEN>
<TOKEN id="token-91-18" pos="word" morph="none" start_char="9835" end_char="9839">stage</TOKEN>
<TOKEN id="token-91-19" pos="word" morph="none" start_char="9841" end_char="9843">for</TOKEN>
<TOKEN id="token-91-20" pos="word" morph="none" start_char="9845" end_char="9848">what</TOKEN>
<TOKEN id="token-91-21" pos="word" morph="none" start_char="9850" end_char="9854">comes</TOKEN>
<TOKEN id="token-91-22" pos="word" morph="none" start_char="9856" end_char="9859">next</TOKEN>
<TOKEN id="token-91-23" pos="punct" morph="none" start_char="9860" end_char="9860">.</TOKEN>
</SEG>
<SEG id="segment-92" start_char="9862" end_char="10072">
<ORIGINAL_TEXT>What comes next is your refutation of the possibility of a lab escape accident, with this statement… "But there is a lot of scholarly evidence to suggest that coronavirus was not manufactured in the laboratory."</ORIGINAL_TEXT>
<TOKEN id="token-92-0" pos="word" morph="none" start_char="9862" end_char="9865">What</TOKEN>
<TOKEN id="token-92-1" pos="word" morph="none" start_char="9867" end_char="9871">comes</TOKEN>
<TOKEN id="token-92-2" pos="word" morph="none" start_char="9873" end_char="9876">next</TOKEN>
<TOKEN id="token-92-3" pos="word" morph="none" start_char="9878" end_char="9879">is</TOKEN>
<TOKEN id="token-92-4" pos="word" morph="none" start_char="9881" end_char="9884">your</TOKEN>
<TOKEN id="token-92-5" pos="word" morph="none" start_char="9886" end_char="9895">refutation</TOKEN>
<TOKEN id="token-92-6" pos="word" morph="none" start_char="9897" end_char="9898">of</TOKEN>
<TOKEN id="token-92-7" pos="word" morph="none" start_char="9900" end_char="9902">the</TOKEN>
<TOKEN id="token-92-8" pos="word" morph="none" start_char="9904" end_char="9914">possibility</TOKEN>
<TOKEN id="token-92-9" pos="word" morph="none" start_char="9916" end_char="9917">of</TOKEN>
<TOKEN id="token-92-10" pos="word" morph="none" start_char="9919" end_char="9919">a</TOKEN>
<TOKEN id="token-92-11" pos="word" morph="none" start_char="9921" end_char="9923">lab</TOKEN>
<TOKEN id="token-92-12" pos="word" morph="none" start_char="9925" end_char="9930">escape</TOKEN>
<TOKEN id="token-92-13" pos="word" morph="none" start_char="9932" end_char="9939">accident</TOKEN>
<TOKEN id="token-92-14" pos="punct" morph="none" start_char="9940" end_char="9940">,</TOKEN>
<TOKEN id="token-92-15" pos="word" morph="none" start_char="9942" end_char="9945">with</TOKEN>
<TOKEN id="token-92-16" pos="word" morph="none" start_char="9947" end_char="9950">this</TOKEN>
<TOKEN id="token-92-17" pos="word" morph="none" start_char="9952" end_char="9960">statement</TOKEN>
<TOKEN id="token-92-18" pos="punct" morph="none" start_char="9961" end_char="9961">…</TOKEN>
<TOKEN id="token-92-19" pos="punct" morph="none" start_char="9963" end_char="9963">"</TOKEN>
<TOKEN id="token-92-20" pos="word" morph="none" start_char="9964" end_char="9966">But</TOKEN>
<TOKEN id="token-92-21" pos="word" morph="none" start_char="9968" end_char="9972">there</TOKEN>
<TOKEN id="token-92-22" pos="word" morph="none" start_char="9974" end_char="9975">is</TOKEN>
<TOKEN id="token-92-23" pos="word" morph="none" start_char="9977" end_char="9977">a</TOKEN>
<TOKEN id="token-92-24" pos="word" morph="none" start_char="9979" end_char="9981">lot</TOKEN>
<TOKEN id="token-92-25" pos="word" morph="none" start_char="9983" end_char="9984">of</TOKEN>
<TOKEN id="token-92-26" pos="word" morph="none" start_char="9986" end_char="9994">scholarly</TOKEN>
<TOKEN id="token-92-27" pos="word" morph="none" start_char="9996" end_char="10003">evidence</TOKEN>
<TOKEN id="token-92-28" pos="word" morph="none" start_char="10005" end_char="10006">to</TOKEN>
<TOKEN id="token-92-29" pos="word" morph="none" start_char="10008" end_char="10014">suggest</TOKEN>
<TOKEN id="token-92-30" pos="word" morph="none" start_char="10016" end_char="10019">that</TOKEN>
<TOKEN id="token-92-31" pos="word" morph="none" start_char="10021" end_char="10031">coronavirus</TOKEN>
<TOKEN id="token-92-32" pos="word" morph="none" start_char="10033" end_char="10035">was</TOKEN>
<TOKEN id="token-92-33" pos="word" morph="none" start_char="10037" end_char="10039">not</TOKEN>
<TOKEN id="token-92-34" pos="word" morph="none" start_char="10041" end_char="10052">manufactured</TOKEN>
<TOKEN id="token-92-35" pos="word" morph="none" start_char="10054" end_char="10055">in</TOKEN>
<TOKEN id="token-92-36" pos="word" morph="none" start_char="10057" end_char="10059">the</TOKEN>
<TOKEN id="token-92-37" pos="word" morph="none" start_char="10061" end_char="10070">laboratory</TOKEN>
<TOKEN id="token-92-38" pos="punct" morph="none" start_char="10071" end_char="10072">."</TOKEN>
</SEG>
<SEG id="segment-93" start_char="10074" end_char="10123">
<ORIGINAL_TEXT>That is a textbook example of a strawman argument.</ORIGINAL_TEXT>
<TOKEN id="token-93-0" pos="word" morph="none" start_char="10074" end_char="10077">That</TOKEN>
<TOKEN id="token-93-1" pos="word" morph="none" start_char="10079" end_char="10080">is</TOKEN>
<TOKEN id="token-93-2" pos="word" morph="none" start_char="10082" end_char="10082">a</TOKEN>
<TOKEN id="token-93-3" pos="word" morph="none" start_char="10084" end_char="10091">textbook</TOKEN>
<TOKEN id="token-93-4" pos="word" morph="none" start_char="10093" end_char="10099">example</TOKEN>
<TOKEN id="token-93-5" pos="word" morph="none" start_char="10101" end_char="10102">of</TOKEN>
<TOKEN id="token-93-6" pos="word" morph="none" start_char="10104" end_char="10104">a</TOKEN>
<TOKEN id="token-93-7" pos="word" morph="none" start_char="10106" end_char="10113">strawman</TOKEN>
<TOKEN id="token-93-8" pos="word" morph="none" start_char="10115" end_char="10122">argument</TOKEN>
<TOKEN id="token-93-9" pos="punct" morph="none" start_char="10123" end_char="10123">.</TOKEN>
</SEG>
<SEG id="segment-94" start_char="10125" end_char="10206">
<ORIGINAL_TEXT>The lab escape theory in no way depends upon the virus being a man-made construct.</ORIGINAL_TEXT>
<TOKEN id="token-94-0" pos="word" morph="none" start_char="10125" end_char="10127">The</TOKEN>
<TOKEN id="token-94-1" pos="word" morph="none" start_char="10129" end_char="10131">lab</TOKEN>
<TOKEN id="token-94-2" pos="word" morph="none" start_char="10133" end_char="10138">escape</TOKEN>
<TOKEN id="token-94-3" pos="word" morph="none" start_char="10140" end_char="10145">theory</TOKEN>
<TOKEN id="token-94-4" pos="word" morph="none" start_char="10147" end_char="10148">in</TOKEN>
<TOKEN id="token-94-5" pos="word" morph="none" start_char="10150" end_char="10151">no</TOKEN>
<TOKEN id="token-94-6" pos="word" morph="none" start_char="10153" end_char="10155">way</TOKEN>
<TOKEN id="token-94-7" pos="word" morph="none" start_char="10157" end_char="10163">depends</TOKEN>
<TOKEN id="token-94-8" pos="word" morph="none" start_char="10165" end_char="10168">upon</TOKEN>
<TOKEN id="token-94-9" pos="word" morph="none" start_char="10170" end_char="10172">the</TOKEN>
<TOKEN id="token-94-10" pos="word" morph="none" start_char="10174" end_char="10178">virus</TOKEN>
<TOKEN id="token-94-11" pos="word" morph="none" start_char="10180" end_char="10184">being</TOKEN>
<TOKEN id="token-94-12" pos="word" morph="none" start_char="10186" end_char="10186">a</TOKEN>
<TOKEN id="token-94-13" pos="unknown" morph="none" start_char="10188" end_char="10195">man-made</TOKEN>
<TOKEN id="token-94-14" pos="word" morph="none" start_char="10197" end_char="10205">construct</TOKEN>
<TOKEN id="token-94-15" pos="punct" morph="none" start_char="10206" end_char="10206">.</TOKEN>
</SEG>
<SEG id="segment-95" start_char="10208" end_char="10302">
<ORIGINAL_TEXT>Conclusively proving that the virus was not man-made in no way disproves the lab escape theory.</ORIGINAL_TEXT>
<TOKEN id="token-95-0" pos="word" morph="none" start_char="10208" end_char="10219">Conclusively</TOKEN>
<TOKEN id="token-95-1" pos="word" morph="none" start_char="10221" end_char="10227">proving</TOKEN>
<TOKEN id="token-95-2" pos="word" morph="none" start_char="10229" end_char="10232">that</TOKEN>
<TOKEN id="token-95-3" pos="word" morph="none" start_char="10234" end_char="10236">the</TOKEN>
<TOKEN id="token-95-4" pos="word" morph="none" start_char="10238" end_char="10242">virus</TOKEN>
<TOKEN id="token-95-5" pos="word" morph="none" start_char="10244" end_char="10246">was</TOKEN>
<TOKEN id="token-95-6" pos="word" morph="none" start_char="10248" end_char="10250">not</TOKEN>
<TOKEN id="token-95-7" pos="unknown" morph="none" start_char="10252" end_char="10259">man-made</TOKEN>
<TOKEN id="token-95-8" pos="word" morph="none" start_char="10261" end_char="10262">in</TOKEN>
<TOKEN id="token-95-9" pos="word" morph="none" start_char="10264" end_char="10265">no</TOKEN>
<TOKEN id="token-95-10" pos="word" morph="none" start_char="10267" end_char="10269">way</TOKEN>
<TOKEN id="token-95-11" pos="word" morph="none" start_char="10271" end_char="10279">disproves</TOKEN>
<TOKEN id="token-95-12" pos="word" morph="none" start_char="10281" end_char="10283">the</TOKEN>
<TOKEN id="token-95-13" pos="word" morph="none" start_char="10285" end_char="10287">lab</TOKEN>
<TOKEN id="token-95-14" pos="word" morph="none" start_char="10289" end_char="10294">escape</TOKEN>
<TOKEN id="token-95-15" pos="word" morph="none" start_char="10296" end_char="10301">theory</TOKEN>
<TOKEN id="token-95-16" pos="punct" morph="none" start_char="10302" end_char="10302">.</TOKEN>
</SEG>
<SEG id="segment-96" start_char="10304" end_char="10329">
<ORIGINAL_TEXT>It is a known… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-96-0" pos="word" morph="none" start_char="10304" end_char="10305">It</TOKEN>
<TOKEN id="token-96-1" pos="word" morph="none" start_char="10307" end_char="10308">is</TOKEN>
<TOKEN id="token-96-2" pos="word" morph="none" start_char="10310" end_char="10310">a</TOKEN>
<TOKEN id="token-96-3" pos="word" morph="none" start_char="10312" end_char="10316">known</TOKEN>
<TOKEN id="token-96-4" pos="punct" morph="none" start_char="10317" end_char="10317">…</TOKEN>
<TOKEN id="token-96-5" pos="word" morph="none" start_char="10319" end_char="10322">Read</TOKEN>
<TOKEN id="token-96-6" pos="word" morph="none" start_char="10324" end_char="10327">more</TOKEN>
<TOKEN id="token-96-7" pos="punct" morph="none" start_char="10329" end_char="10329">»</TOKEN>
</SEG>
<SEG id="segment-97" start_char="10333" end_char="10390">
<ORIGINAL_TEXT>in case of a lab leak, wound many lab workers be infected?</ORIGINAL_TEXT>
<TOKEN id="token-97-0" pos="word" morph="none" start_char="10333" end_char="10334">in</TOKEN>
<TOKEN id="token-97-1" pos="word" morph="none" start_char="10336" end_char="10339">case</TOKEN>
<TOKEN id="token-97-2" pos="word" morph="none" start_char="10341" end_char="10342">of</TOKEN>
<TOKEN id="token-97-3" pos="word" morph="none" start_char="10344" end_char="10344">a</TOKEN>
<TOKEN id="token-97-4" pos="word" morph="none" start_char="10346" end_char="10348">lab</TOKEN>
<TOKEN id="token-97-5" pos="word" morph="none" start_char="10350" end_char="10353">leak</TOKEN>
<TOKEN id="token-97-6" pos="punct" morph="none" start_char="10354" end_char="10354">,</TOKEN>
<TOKEN id="token-97-7" pos="word" morph="none" start_char="10356" end_char="10360">wound</TOKEN>
<TOKEN id="token-97-8" pos="word" morph="none" start_char="10362" end_char="10365">many</TOKEN>
<TOKEN id="token-97-9" pos="word" morph="none" start_char="10367" end_char="10369">lab</TOKEN>
<TOKEN id="token-97-10" pos="word" morph="none" start_char="10371" end_char="10377">workers</TOKEN>
<TOKEN id="token-97-11" pos="word" morph="none" start_char="10379" end_char="10380">be</TOKEN>
<TOKEN id="token-97-12" pos="word" morph="none" start_char="10382" end_char="10389">infected</TOKEN>
<TOKEN id="token-97-13" pos="punct" morph="none" start_char="10390" end_char="10390">?</TOKEN>
</SEG>
<SEG id="segment-98" start_char="10394" end_char="10469">
<ORIGINAL_TEXT>Why doesn’t anyone talk about the CDC closing down the bioweapons lab at Ft.</ORIGINAL_TEXT>
<TOKEN id="token-98-0" pos="word" morph="none" start_char="10394" end_char="10396">Why</TOKEN>
<TOKEN id="token-98-1" pos="word" morph="none" start_char="10398" end_char="10404">doesn’t</TOKEN>
<TOKEN id="token-98-2" pos="word" morph="none" start_char="10406" end_char="10411">anyone</TOKEN>
<TOKEN id="token-98-3" pos="word" morph="none" start_char="10413" end_char="10416">talk</TOKEN>
<TOKEN id="token-98-4" pos="word" morph="none" start_char="10418" end_char="10422">about</TOKEN>
<TOKEN id="token-98-5" pos="word" morph="none" start_char="10424" end_char="10426">the</TOKEN>
<TOKEN id="token-98-6" pos="word" morph="none" start_char="10428" end_char="10430">CDC</TOKEN>
<TOKEN id="token-98-7" pos="word" morph="none" start_char="10432" end_char="10438">closing</TOKEN>
<TOKEN id="token-98-8" pos="word" morph="none" start_char="10440" end_char="10443">down</TOKEN>
<TOKEN id="token-98-9" pos="word" morph="none" start_char="10445" end_char="10447">the</TOKEN>
<TOKEN id="token-98-10" pos="word" morph="none" start_char="10449" end_char="10458">bioweapons</TOKEN>
<TOKEN id="token-98-11" pos="word" morph="none" start_char="10460" end_char="10462">lab</TOKEN>
<TOKEN id="token-98-12" pos="word" morph="none" start_char="10464" end_char="10465">at</TOKEN>
<TOKEN id="token-98-13" pos="word" morph="none" start_char="10467" end_char="10468">Ft</TOKEN>
<TOKEN id="token-98-14" pos="punct" morph="none" start_char="10469" end_char="10469">.</TOKEN>
</SEG>
<SEG id="segment-99" start_char="10471" end_char="10534">
<ORIGINAL_TEXT>Detrick for sloppy practices last August (reopened in November).</ORIGINAL_TEXT>
<TOKEN id="token-99-0" pos="word" morph="none" start_char="10471" end_char="10477">Detrick</TOKEN>
<TOKEN id="token-99-1" pos="word" morph="none" start_char="10479" end_char="10481">for</TOKEN>
<TOKEN id="token-99-2" pos="word" morph="none" start_char="10483" end_char="10488">sloppy</TOKEN>
<TOKEN id="token-99-3" pos="word" morph="none" start_char="10490" end_char="10498">practices</TOKEN>
<TOKEN id="token-99-4" pos="word" morph="none" start_char="10500" end_char="10503">last</TOKEN>
<TOKEN id="token-99-5" pos="word" morph="none" start_char="10505" end_char="10510">August</TOKEN>
<TOKEN id="token-99-6" pos="punct" morph="none" start_char="10512" end_char="10512">(</TOKEN>
<TOKEN id="token-99-7" pos="word" morph="none" start_char="10513" end_char="10520">reopened</TOKEN>
<TOKEN id="token-99-8" pos="word" morph="none" start_char="10522" end_char="10523">in</TOKEN>
<TOKEN id="token-99-9" pos="word" morph="none" start_char="10525" end_char="10532">November</TOKEN>
<TOKEN id="token-99-10" pos="punct" morph="none" start_char="10533" end_char="10534">).</TOKEN>
</SEG>
<SEG id="segment-100" start_char="10536" end_char="10558">
<ORIGINAL_TEXT>Remarkable coincidence.</ORIGINAL_TEXT>
<TOKEN id="token-100-0" pos="word" morph="none" start_char="10536" end_char="10545">Remarkable</TOKEN>
<TOKEN id="token-100-1" pos="word" morph="none" start_char="10547" end_char="10557">coincidence</TOKEN>
<TOKEN id="token-100-2" pos="punct" morph="none" start_char="10558" end_char="10558">.</TOKEN>
</SEG>
<SEG id="segment-101" start_char="10560" end_char="10767">
<ORIGINAL_TEXT>(See https://www.fredericknewspost.com/news/politics_and_government/military/fort-detrick-laboratory-restored-to-full-operations-after-being-shut-down-by-cdc/article_fcee204f-1493-52fb-ba9b-f80cb00da727.html)</ORIGINAL_TEXT>
<TOKEN id="token-101-0" pos="punct" morph="none" start_char="10560" end_char="10560">(</TOKEN>
<TOKEN id="token-101-1" pos="word" morph="none" start_char="10561" end_char="10563">See</TOKEN>
<TOKEN id="token-101-2" pos="url" morph="none" start_char="10565" end_char="10767">https://www.fredericknewspost.com/news/politics_and_government/military/fort-detrick-laboratory-restored-to-full-operations-after-being-shut-down-by-cdc/article_fcee204f-1493-52fb-ba9b-f80cb00da727.html)</TOKEN>
</SEG>
<SEG id="segment-102" start_char="10771" end_char="10825">
<ORIGINAL_TEXT>Lab leakage in secured labs outside China has occurred.</ORIGINAL_TEXT>
<TOKEN id="token-102-0" pos="word" morph="none" start_char="10771" end_char="10773">Lab</TOKEN>
<TOKEN id="token-102-1" pos="word" morph="none" start_char="10775" end_char="10781">leakage</TOKEN>
<TOKEN id="token-102-2" pos="word" morph="none" start_char="10783" end_char="10784">in</TOKEN>
<TOKEN id="token-102-3" pos="word" morph="none" start_char="10786" end_char="10792">secured</TOKEN>
<TOKEN id="token-102-4" pos="word" morph="none" start_char="10794" end_char="10797">labs</TOKEN>
<TOKEN id="token-102-5" pos="word" morph="none" start_char="10799" end_char="10805">outside</TOKEN>
<TOKEN id="token-102-6" pos="word" morph="none" start_char="10807" end_char="10811">China</TOKEN>
<TOKEN id="token-102-7" pos="word" morph="none" start_char="10813" end_char="10815">has</TOKEN>
<TOKEN id="token-102-8" pos="word" morph="none" start_char="10817" end_char="10824">occurred</TOKEN>
<TOKEN id="token-102-9" pos="punct" morph="none" start_char="10825" end_char="10825">.</TOKEN>
</SEG>
<SEG id="segment-103" start_char="10827" end_char="10866">
<ORIGINAL_TEXT>It happened in Detrick and in Singapore.</ORIGINAL_TEXT>
<TOKEN id="token-103-0" pos="word" morph="none" start_char="10827" end_char="10828">It</TOKEN>
<TOKEN id="token-103-1" pos="word" morph="none" start_char="10830" end_char="10837">happened</TOKEN>
<TOKEN id="token-103-2" pos="word" morph="none" start_char="10839" end_char="10840">in</TOKEN>
<TOKEN id="token-103-3" pos="word" morph="none" start_char="10842" end_char="10848">Detrick</TOKEN>
<TOKEN id="token-103-4" pos="word" morph="none" start_char="10850" end_char="10852">and</TOKEN>
<TOKEN id="token-103-5" pos="word" morph="none" start_char="10854" end_char="10855">in</TOKEN>
<TOKEN id="token-103-6" pos="word" morph="none" start_char="10857" end_char="10865">Singapore</TOKEN>
<TOKEN id="token-103-7" pos="punct" morph="none" start_char="10866" end_char="10866">.</TOKEN>
</SEG>
<SEG id="segment-104" start_char="10870" end_char="10905">
<ORIGINAL_TEXT>We humans are our own worst enemies.</ORIGINAL_TEXT>
<TOKEN id="token-104-0" pos="word" morph="none" start_char="10870" end_char="10871">We</TOKEN>
<TOKEN id="token-104-1" pos="word" morph="none" start_char="10873" end_char="10878">humans</TOKEN>
<TOKEN id="token-104-2" pos="word" morph="none" start_char="10880" end_char="10882">are</TOKEN>
<TOKEN id="token-104-3" pos="word" morph="none" start_char="10884" end_char="10886">our</TOKEN>
<TOKEN id="token-104-4" pos="word" morph="none" start_char="10888" end_char="10890">own</TOKEN>
<TOKEN id="token-104-5" pos="word" morph="none" start_char="10892" end_char="10896">worst</TOKEN>
<TOKEN id="token-104-6" pos="word" morph="none" start_char="10898" end_char="10904">enemies</TOKEN>
<TOKEN id="token-104-7" pos="punct" morph="none" start_char="10905" end_char="10905">.</TOKEN>
</SEG>
<SEG id="segment-105" start_char="10907" end_char="11015">
<ORIGINAL_TEXT>What the hell are scientists (anywhere) doing fooling around with viruses that have no benefit to human life?</ORIGINAL_TEXT>
<TOKEN id="token-105-0" pos="word" morph="none" start_char="10907" end_char="10910">What</TOKEN>
<TOKEN id="token-105-1" pos="word" morph="none" start_char="10912" end_char="10914">the</TOKEN>
<TOKEN id="token-105-2" pos="word" morph="none" start_char="10916" end_char="10919">hell</TOKEN>
<TOKEN id="token-105-3" pos="word" morph="none" start_char="10921" end_char="10923">are</TOKEN>
<TOKEN id="token-105-4" pos="word" morph="none" start_char="10925" end_char="10934">scientists</TOKEN>
<TOKEN id="token-105-5" pos="punct" morph="none" start_char="10936" end_char="10936">(</TOKEN>
<TOKEN id="token-105-6" pos="word" morph="none" start_char="10937" end_char="10944">anywhere</TOKEN>
<TOKEN id="token-105-7" pos="punct" morph="none" start_char="10945" end_char="10945">)</TOKEN>
<TOKEN id="token-105-8" pos="word" morph="none" start_char="10947" end_char="10951">doing</TOKEN>
<TOKEN id="token-105-9" pos="word" morph="none" start_char="10953" end_char="10959">fooling</TOKEN>
<TOKEN id="token-105-10" pos="word" morph="none" start_char="10961" end_char="10966">around</TOKEN>
<TOKEN id="token-105-11" pos="word" morph="none" start_char="10968" end_char="10971">with</TOKEN>
<TOKEN id="token-105-12" pos="word" morph="none" start_char="10973" end_char="10979">viruses</TOKEN>
<TOKEN id="token-105-13" pos="word" morph="none" start_char="10981" end_char="10984">that</TOKEN>
<TOKEN id="token-105-14" pos="word" morph="none" start_char="10986" end_char="10989">have</TOKEN>
<TOKEN id="token-105-15" pos="word" morph="none" start_char="10991" end_char="10992">no</TOKEN>
<TOKEN id="token-105-16" pos="word" morph="none" start_char="10994" end_char="11000">benefit</TOKEN>
<TOKEN id="token-105-17" pos="word" morph="none" start_char="11002" end_char="11003">to</TOKEN>
<TOKEN id="token-105-18" pos="word" morph="none" start_char="11005" end_char="11009">human</TOKEN>
<TOKEN id="token-105-19" pos="word" morph="none" start_char="11011" end_char="11014">life</TOKEN>
<TOKEN id="token-105-20" pos="punct" morph="none" start_char="11015" end_char="11015">?</TOKEN>
</SEG>
<SEG id="segment-106" start_char="11017" end_char="11148">
<ORIGINAL_TEXT>This virus is more dangerous than Chernobyl…it’s probably going to reappear each year until more sensible scientists find a vaccine.</ORIGINAL_TEXT>
<TOKEN id="token-106-0" pos="word" morph="none" start_char="11017" end_char="11020">This</TOKEN>
<TOKEN id="token-106-1" pos="word" morph="none" start_char="11022" end_char="11026">virus</TOKEN>
<TOKEN id="token-106-2" pos="word" morph="none" start_char="11028" end_char="11029">is</TOKEN>
<TOKEN id="token-106-3" pos="word" morph="none" start_char="11031" end_char="11034">more</TOKEN>
<TOKEN id="token-106-4" pos="word" morph="none" start_char="11036" end_char="11044">dangerous</TOKEN>
<TOKEN id="token-106-5" pos="word" morph="none" start_char="11046" end_char="11049">than</TOKEN>
<TOKEN id="token-106-6" pos="unknown" morph="none" start_char="11051" end_char="11064">Chernobyl…it’s</TOKEN>
<TOKEN id="token-106-7" pos="word" morph="none" start_char="11066" end_char="11073">probably</TOKEN>
<TOKEN id="token-106-8" pos="word" morph="none" start_char="11075" end_char="11079">going</TOKEN>
<TOKEN id="token-106-9" pos="word" morph="none" start_char="11081" end_char="11082">to</TOKEN>
<TOKEN id="token-106-10" pos="word" morph="none" start_char="11084" end_char="11091">reappear</TOKEN>
<TOKEN id="token-106-11" pos="word" morph="none" start_char="11093" end_char="11096">each</TOKEN>
<TOKEN id="token-106-12" pos="word" morph="none" start_char="11098" end_char="11101">year</TOKEN>
<TOKEN id="token-106-13" pos="word" morph="none" start_char="11103" end_char="11107">until</TOKEN>
<TOKEN id="token-106-14" pos="word" morph="none" start_char="11109" end_char="11112">more</TOKEN>
<TOKEN id="token-106-15" pos="word" morph="none" start_char="11114" end_char="11121">sensible</TOKEN>
<TOKEN id="token-106-16" pos="word" morph="none" start_char="11123" end_char="11132">scientists</TOKEN>
<TOKEN id="token-106-17" pos="word" morph="none" start_char="11134" end_char="11137">find</TOKEN>
<TOKEN id="token-106-18" pos="word" morph="none" start_char="11139" end_char="11139">a</TOKEN>
<TOKEN id="token-106-19" pos="word" morph="none" start_char="11141" end_char="11147">vaccine</TOKEN>
<TOKEN id="token-106-20" pos="punct" morph="none" start_char="11148" end_char="11148">.</TOKEN>
</SEG>
<SEG id="segment-107" start_char="11150" end_char="11489">
<ORIGINAL_TEXT>It is one thing to have it be an ‘accident’ it is also another having/allowing ‘children’ to play around with viruses when they are too incompetent to handle such tests securely (accident free)…but when they KNEW it was an issue they (Chinese government) then hoarded all the PPE shipments going out of their country…cause they… Read more »</ORIGINAL_TEXT>
<TOKEN id="token-107-0" pos="word" morph="none" start_char="11150" end_char="11151">It</TOKEN>
<TOKEN id="token-107-1" pos="word" morph="none" start_char="11153" end_char="11154">is</TOKEN>
<TOKEN id="token-107-2" pos="word" morph="none" start_char="11156" end_char="11158">one</TOKEN>
<TOKEN id="token-107-3" pos="word" morph="none" start_char="11160" end_char="11164">thing</TOKEN>
<TOKEN id="token-107-4" pos="word" morph="none" start_char="11166" end_char="11167">to</TOKEN>
<TOKEN id="token-107-5" pos="word" morph="none" start_char="11169" end_char="11172">have</TOKEN>
<TOKEN id="token-107-6" pos="word" morph="none" start_char="11174" end_char="11175">it</TOKEN>
<TOKEN id="token-107-7" pos="word" morph="none" start_char="11177" end_char="11178">be</TOKEN>
<TOKEN id="token-107-8" pos="word" morph="none" start_char="11180" end_char="11181">an</TOKEN>
<TOKEN id="token-107-9" pos="punct" morph="none" start_char="11183" end_char="11183">‘</TOKEN>
<TOKEN id="token-107-10" pos="word" morph="none" start_char="11184" end_char="11191">accident</TOKEN>
<TOKEN id="token-107-11" pos="punct" morph="none" start_char="11192" end_char="11192">’</TOKEN>
<TOKEN id="token-107-12" pos="word" morph="none" start_char="11194" end_char="11195">it</TOKEN>
<TOKEN id="token-107-13" pos="word" morph="none" start_char="11197" end_char="11198">is</TOKEN>
<TOKEN id="token-107-14" pos="word" morph="none" start_char="11200" end_char="11203">also</TOKEN>
<TOKEN id="token-107-15" pos="word" morph="none" start_char="11205" end_char="11211">another</TOKEN>
<TOKEN id="token-107-16" pos="unknown" morph="none" start_char="11213" end_char="11227">having/allowing</TOKEN>
<TOKEN id="token-107-17" pos="punct" morph="none" start_char="11229" end_char="11229">‘</TOKEN>
<TOKEN id="token-107-18" pos="word" morph="none" start_char="11230" end_char="11237">children</TOKEN>
<TOKEN id="token-107-19" pos="punct" morph="none" start_char="11238" end_char="11238">’</TOKEN>
<TOKEN id="token-107-20" pos="word" morph="none" start_char="11240" end_char="11241">to</TOKEN>
<TOKEN id="token-107-21" pos="word" morph="none" start_char="11243" end_char="11246">play</TOKEN>
<TOKEN id="token-107-22" pos="word" morph="none" start_char="11248" end_char="11253">around</TOKEN>
<TOKEN id="token-107-23" pos="word" morph="none" start_char="11255" end_char="11258">with</TOKEN>
<TOKEN id="token-107-24" pos="word" morph="none" start_char="11260" end_char="11266">viruses</TOKEN>
<TOKEN id="token-107-25" pos="word" morph="none" start_char="11268" end_char="11271">when</TOKEN>
<TOKEN id="token-107-26" pos="word" morph="none" start_char="11273" end_char="11276">they</TOKEN>
<TOKEN id="token-107-27" pos="word" morph="none" start_char="11278" end_char="11280">are</TOKEN>
<TOKEN id="token-107-28" pos="word" morph="none" start_char="11282" end_char="11284">too</TOKEN>
<TOKEN id="token-107-29" pos="word" morph="none" start_char="11286" end_char="11296">incompetent</TOKEN>
<TOKEN id="token-107-30" pos="word" morph="none" start_char="11298" end_char="11299">to</TOKEN>
<TOKEN id="token-107-31" pos="word" morph="none" start_char="11301" end_char="11306">handle</TOKEN>
<TOKEN id="token-107-32" pos="word" morph="none" start_char="11308" end_char="11311">such</TOKEN>
<TOKEN id="token-107-33" pos="word" morph="none" start_char="11313" end_char="11317">tests</TOKEN>
<TOKEN id="token-107-34" pos="word" morph="none" start_char="11319" end_char="11326">securely</TOKEN>
<TOKEN id="token-107-35" pos="punct" morph="none" start_char="11328" end_char="11328">(</TOKEN>
<TOKEN id="token-107-36" pos="word" morph="none" start_char="11329" end_char="11336">accident</TOKEN>
<TOKEN id="token-107-37" pos="unknown" morph="none" start_char="11338" end_char="11346">free)…but</TOKEN>
<TOKEN id="token-107-38" pos="word" morph="none" start_char="11348" end_char="11351">when</TOKEN>
<TOKEN id="token-107-39" pos="word" morph="none" start_char="11353" end_char="11356">they</TOKEN>
<TOKEN id="token-107-40" pos="word" morph="none" start_char="11358" end_char="11361">KNEW</TOKEN>
<TOKEN id="token-107-41" pos="word" morph="none" start_char="11363" end_char="11364">it</TOKEN>
<TOKEN id="token-107-42" pos="word" morph="none" start_char="11366" end_char="11368">was</TOKEN>
<TOKEN id="token-107-43" pos="word" morph="none" start_char="11370" end_char="11371">an</TOKEN>
<TOKEN id="token-107-44" pos="word" morph="none" start_char="11373" end_char="11377">issue</TOKEN>
<TOKEN id="token-107-45" pos="word" morph="none" start_char="11379" end_char="11382">they</TOKEN>
<TOKEN id="token-107-46" pos="punct" morph="none" start_char="11384" end_char="11384">(</TOKEN>
<TOKEN id="token-107-47" pos="word" morph="none" start_char="11385" end_char="11391">Chinese</TOKEN>
<TOKEN id="token-107-48" pos="word" morph="none" start_char="11393" end_char="11402">government</TOKEN>
<TOKEN id="token-107-49" pos="punct" morph="none" start_char="11403" end_char="11403">)</TOKEN>
<TOKEN id="token-107-50" pos="word" morph="none" start_char="11405" end_char="11408">then</TOKEN>
<TOKEN id="token-107-51" pos="word" morph="none" start_char="11410" end_char="11416">hoarded</TOKEN>
<TOKEN id="token-107-52" pos="word" morph="none" start_char="11418" end_char="11420">all</TOKEN>
<TOKEN id="token-107-53" pos="word" morph="none" start_char="11422" end_char="11424">the</TOKEN>
<TOKEN id="token-107-54" pos="word" morph="none" start_char="11426" end_char="11428">PPE</TOKEN>
<TOKEN id="token-107-55" pos="word" morph="none" start_char="11430" end_char="11438">shipments</TOKEN>
<TOKEN id="token-107-56" pos="word" morph="none" start_char="11440" end_char="11444">going</TOKEN>
<TOKEN id="token-107-57" pos="word" morph="none" start_char="11446" end_char="11448">out</TOKEN>
<TOKEN id="token-107-58" pos="word" morph="none" start_char="11450" end_char="11451">of</TOKEN>
<TOKEN id="token-107-59" pos="word" morph="none" start_char="11453" end_char="11457">their</TOKEN>
<TOKEN id="token-107-60" pos="unknown" morph="none" start_char="11459" end_char="11471">country…cause</TOKEN>
<TOKEN id="token-107-61" pos="word" morph="none" start_char="11473" end_char="11476">they</TOKEN>
<TOKEN id="token-107-62" pos="punct" morph="none" start_char="11477" end_char="11477">…</TOKEN>
<TOKEN id="token-107-63" pos="word" morph="none" start_char="11479" end_char="11482">Read</TOKEN>
<TOKEN id="token-107-64" pos="word" morph="none" start_char="11484" end_char="11487">more</TOKEN>
<TOKEN id="token-107-65" pos="punct" morph="none" start_char="11489" end_char="11489">»</TOKEN>
</SEG>
<SEG id="segment-108" start_char="11493" end_char="11560">
<ORIGINAL_TEXT>Wuhan was known to collect bats and study their SARS-family viruses.</ORIGINAL_TEXT>
<TOKEN id="token-108-0" pos="word" morph="none" start_char="11493" end_char="11497">Wuhan</TOKEN>
<TOKEN id="token-108-1" pos="word" morph="none" start_char="11499" end_char="11501">was</TOKEN>
<TOKEN id="token-108-2" pos="word" morph="none" start_char="11503" end_char="11507">known</TOKEN>
<TOKEN id="token-108-3" pos="word" morph="none" start_char="11509" end_char="11510">to</TOKEN>
<TOKEN id="token-108-4" pos="word" morph="none" start_char="11512" end_char="11518">collect</TOKEN>
<TOKEN id="token-108-5" pos="word" morph="none" start_char="11520" end_char="11523">bats</TOKEN>
<TOKEN id="token-108-6" pos="word" morph="none" start_char="11525" end_char="11527">and</TOKEN>
<TOKEN id="token-108-7" pos="word" morph="none" start_char="11529" end_char="11533">study</TOKEN>
<TOKEN id="token-108-8" pos="word" morph="none" start_char="11535" end_char="11539">their</TOKEN>
<TOKEN id="token-108-9" pos="unknown" morph="none" start_char="11541" end_char="11551">SARS-family</TOKEN>
<TOKEN id="token-108-10" pos="word" morph="none" start_char="11553" end_char="11559">viruses</TOKEN>
<TOKEN id="token-108-11" pos="punct" morph="none" start_char="11560" end_char="11560">.</TOKEN>
</SEG>
<SEG id="segment-109" start_char="11562" end_char="11689">
<ORIGINAL_TEXT>A virus the result of "serial passage" animal breeding would have no trace of genetic modification because it wouldn’t have any.</ORIGINAL_TEXT>
<TOKEN id="token-109-0" pos="word" morph="none" start_char="11562" end_char="11562">A</TOKEN>
<TOKEN id="token-109-1" pos="word" morph="none" start_char="11564" end_char="11568">virus</TOKEN>
<TOKEN id="token-109-2" pos="word" morph="none" start_char="11570" end_char="11572">the</TOKEN>
<TOKEN id="token-109-3" pos="word" morph="none" start_char="11574" end_char="11579">result</TOKEN>
<TOKEN id="token-109-4" pos="word" morph="none" start_char="11581" end_char="11582">of</TOKEN>
<TOKEN id="token-109-5" pos="punct" morph="none" start_char="11584" end_char="11584">"</TOKEN>
<TOKEN id="token-109-6" pos="word" morph="none" start_char="11585" end_char="11590">serial</TOKEN>
<TOKEN id="token-109-7" pos="word" morph="none" start_char="11592" end_char="11598">passage</TOKEN>
<TOKEN id="token-109-8" pos="punct" morph="none" start_char="11599" end_char="11599">"</TOKEN>
<TOKEN id="token-109-9" pos="word" morph="none" start_char="11601" end_char="11606">animal</TOKEN>
<TOKEN id="token-109-10" pos="word" morph="none" start_char="11608" end_char="11615">breeding</TOKEN>
<TOKEN id="token-109-11" pos="word" morph="none" start_char="11617" end_char="11621">would</TOKEN>
<TOKEN id="token-109-12" pos="word" morph="none" start_char="11623" end_char="11626">have</TOKEN>
<TOKEN id="token-109-13" pos="word" morph="none" start_char="11628" end_char="11629">no</TOKEN>
<TOKEN id="token-109-14" pos="word" morph="none" start_char="11631" end_char="11635">trace</TOKEN>
<TOKEN id="token-109-15" pos="word" morph="none" start_char="11637" end_char="11638">of</TOKEN>
<TOKEN id="token-109-16" pos="word" morph="none" start_char="11640" end_char="11646">genetic</TOKEN>
<TOKEN id="token-109-17" pos="word" morph="none" start_char="11648" end_char="11659">modification</TOKEN>
<TOKEN id="token-109-18" pos="word" morph="none" start_char="11661" end_char="11667">because</TOKEN>
<TOKEN id="token-109-19" pos="word" morph="none" start_char="11669" end_char="11670">it</TOKEN>
<TOKEN id="token-109-20" pos="word" morph="none" start_char="11672" end_char="11679">wouldn’t</TOKEN>
<TOKEN id="token-109-21" pos="word" morph="none" start_char="11681" end_char="11684">have</TOKEN>
<TOKEN id="token-109-22" pos="word" morph="none" start_char="11686" end_char="11688">any</TOKEN>
<TOKEN id="token-109-23" pos="punct" morph="none" start_char="11689" end_char="11689">.</TOKEN>
</SEG>
<SEG id="segment-110" start_char="11691" end_char="11822">
<ORIGINAL_TEXT>The most damning evidence in favor of lab origin is, ironically, China’s draconian "sanitizing" of the lab after the outbreak began.</ORIGINAL_TEXT>
<TOKEN id="token-110-0" pos="word" morph="none" start_char="11691" end_char="11693">The</TOKEN>
<TOKEN id="token-110-1" pos="word" morph="none" start_char="11695" end_char="11698">most</TOKEN>
<TOKEN id="token-110-2" pos="word" morph="none" start_char="11700" end_char="11706">damning</TOKEN>
<TOKEN id="token-110-3" pos="word" morph="none" start_char="11708" end_char="11715">evidence</TOKEN>
<TOKEN id="token-110-4" pos="word" morph="none" start_char="11717" end_char="11718">in</TOKEN>
<TOKEN id="token-110-5" pos="word" morph="none" start_char="11720" end_char="11724">favor</TOKEN>
<TOKEN id="token-110-6" pos="word" morph="none" start_char="11726" end_char="11727">of</TOKEN>
<TOKEN id="token-110-7" pos="word" morph="none" start_char="11729" end_char="11731">lab</TOKEN>
<TOKEN id="token-110-8" pos="word" morph="none" start_char="11733" end_char="11738">origin</TOKEN>
<TOKEN id="token-110-9" pos="word" morph="none" start_char="11740" end_char="11741">is</TOKEN>
<TOKEN id="token-110-10" pos="punct" morph="none" start_char="11742" end_char="11742">,</TOKEN>
<TOKEN id="token-110-11" pos="word" morph="none" start_char="11744" end_char="11753">ironically</TOKEN>
<TOKEN id="token-110-12" pos="punct" morph="none" start_char="11754" end_char="11754">,</TOKEN>
<TOKEN id="token-110-13" pos="word" morph="none" start_char="11756" end_char="11762">China’s</TOKEN>
<TOKEN id="token-110-14" pos="word" morph="none" start_char="11764" end_char="11772">draconian</TOKEN>
<TOKEN id="token-110-15" pos="punct" morph="none" start_char="11774" end_char="11774">"</TOKEN>
<TOKEN id="token-110-16" pos="word" morph="none" start_char="11775" end_char="11784">sanitizing</TOKEN>
<TOKEN id="token-110-17" pos="punct" morph="none" start_char="11785" end_char="11785">"</TOKEN>
<TOKEN id="token-110-18" pos="word" morph="none" start_char="11787" end_char="11788">of</TOKEN>
<TOKEN id="token-110-19" pos="word" morph="none" start_char="11790" end_char="11792">the</TOKEN>
<TOKEN id="token-110-20" pos="word" morph="none" start_char="11794" end_char="11796">lab</TOKEN>
<TOKEN id="token-110-21" pos="word" morph="none" start_char="11798" end_char="11802">after</TOKEN>
<TOKEN id="token-110-22" pos="word" morph="none" start_char="11804" end_char="11806">the</TOKEN>
<TOKEN id="token-110-23" pos="word" morph="none" start_char="11808" end_char="11815">outbreak</TOKEN>
<TOKEN id="token-110-24" pos="word" morph="none" start_char="11817" end_char="11821">began</TOKEN>
<TOKEN id="token-110-25" pos="punct" morph="none" start_char="11822" end_char="11822">.</TOKEN>
</SEG>
<SEG id="segment-111" start_char="11824" end_char="11898">
<ORIGINAL_TEXT>Circumstantial evidence points to the Wuhan lab, no gene sequencing needed.</ORIGINAL_TEXT>
<TOKEN id="token-111-0" pos="word" morph="none" start_char="11824" end_char="11837">Circumstantial</TOKEN>
<TOKEN id="token-111-1" pos="word" morph="none" start_char="11839" end_char="11846">evidence</TOKEN>
<TOKEN id="token-111-2" pos="word" morph="none" start_char="11848" end_char="11853">points</TOKEN>
<TOKEN id="token-111-3" pos="word" morph="none" start_char="11855" end_char="11856">to</TOKEN>
<TOKEN id="token-111-4" pos="word" morph="none" start_char="11858" end_char="11860">the</TOKEN>
<TOKEN id="token-111-5" pos="word" morph="none" start_char="11862" end_char="11866">Wuhan</TOKEN>
<TOKEN id="token-111-6" pos="word" morph="none" start_char="11868" end_char="11870">lab</TOKEN>
<TOKEN id="token-111-7" pos="punct" morph="none" start_char="11871" end_char="11871">,</TOKEN>
<TOKEN id="token-111-8" pos="word" morph="none" start_char="11873" end_char="11874">no</TOKEN>
<TOKEN id="token-111-9" pos="word" morph="none" start_char="11876" end_char="11879">gene</TOKEN>
<TOKEN id="token-111-10" pos="word" morph="none" start_char="11881" end_char="11890">sequencing</TOKEN>
<TOKEN id="token-111-11" pos="word" morph="none" start_char="11892" end_char="11897">needed</TOKEN>
<TOKEN id="token-111-12" pos="punct" morph="none" start_char="11898" end_char="11898">.</TOKEN>
</SEG>
<SEG id="segment-112" start_char="11900" end_char="12042">
<ORIGINAL_TEXT>I’m still looking for proof they were doing GoF research, preferably with a (hypothetical) SARS-CoV-2 ancestor, selecting for transmissibility.</ORIGINAL_TEXT>
<TOKEN id="token-112-0" pos="word" morph="none" start_char="11900" end_char="11902">I’m</TOKEN>
<TOKEN id="token-112-1" pos="word" morph="none" start_char="11904" end_char="11908">still</TOKEN>
<TOKEN id="token-112-2" pos="word" morph="none" start_char="11910" end_char="11916">looking</TOKEN>
<TOKEN id="token-112-3" pos="word" morph="none" start_char="11918" end_char="11920">for</TOKEN>
<TOKEN id="token-112-4" pos="word" morph="none" start_char="11922" end_char="11926">proof</TOKEN>
<TOKEN id="token-112-5" pos="word" morph="none" start_char="11928" end_char="11931">they</TOKEN>
<TOKEN id="token-112-6" pos="word" morph="none" start_char="11933" end_char="11936">were</TOKEN>
<TOKEN id="token-112-7" pos="word" morph="none" start_char="11938" end_char="11942">doing</TOKEN>
<TOKEN id="token-112-8" pos="word" morph="none" start_char="11944" end_char="11946">GoF</TOKEN>
<TOKEN id="token-112-9" pos="word" morph="none" start_char="11948" end_char="11955">research</TOKEN>
<TOKEN id="token-112-10" pos="punct" morph="none" start_char="11956" end_char="11956">,</TOKEN>
<TOKEN id="token-112-11" pos="word" morph="none" start_char="11958" end_char="11967">preferably</TOKEN>
<TOKEN id="token-112-12" pos="word" morph="none" start_char="11969" end_char="11972">with</TOKEN>
<TOKEN id="token-112-13" pos="word" morph="none" start_char="11974" end_char="11974">a</TOKEN>
<TOKEN id="token-112-14" pos="punct" morph="none" start_char="11976" end_char="11976">(</TOKEN>
<TOKEN id="token-112-15" pos="word" morph="none" start_char="11977" end_char="11988">hypothetical</TOKEN>
<TOKEN id="token-112-16" pos="punct" morph="none" start_char="11989" end_char="11989">)</TOKEN>
<TOKEN id="token-112-17" pos="unknown" morph="none" start_char="11991" end_char="12000">SARS-CoV-2</TOKEN>
<TOKEN id="token-112-18" pos="word" morph="none" start_char="12002" end_char="12009">ancestor</TOKEN>
<TOKEN id="token-112-19" pos="punct" morph="none" start_char="12010" end_char="12010">,</TOKEN>
<TOKEN id="token-112-20" pos="word" morph="none" start_char="12012" end_char="12020">selecting</TOKEN>
<TOKEN id="token-112-21" pos="word" morph="none" start_char="12022" end_char="12024">for</TOKEN>
<TOKEN id="token-112-22" pos="word" morph="none" start_char="12026" end_char="12041">transmissibility</TOKEN>
<TOKEN id="token-112-23" pos="punct" morph="none" start_char="12042" end_char="12042">.</TOKEN>
</SEG>
<SEG id="segment-113" start_char="12044" end_char="12122">
<ORIGINAL_TEXT>They could destroy evidence in the lab; published acadmeic papers, not so easy.</ORIGINAL_TEXT>
<TOKEN id="token-113-0" pos="word" morph="none" start_char="12044" end_char="12047">They</TOKEN>
<TOKEN id="token-113-1" pos="word" morph="none" start_char="12049" end_char="12053">could</TOKEN>
<TOKEN id="token-113-2" pos="word" morph="none" start_char="12055" end_char="12061">destroy</TOKEN>
<TOKEN id="token-113-3" pos="word" morph="none" start_char="12063" end_char="12070">evidence</TOKEN>
<TOKEN id="token-113-4" pos="word" morph="none" start_char="12072" end_char="12073">in</TOKEN>
<TOKEN id="token-113-5" pos="word" morph="none" start_char="12075" end_char="12077">the</TOKEN>
<TOKEN id="token-113-6" pos="word" morph="none" start_char="12079" end_char="12081">lab</TOKEN>
<TOKEN id="token-113-7" pos="punct" morph="none" start_char="12082" end_char="12082">;</TOKEN>
<TOKEN id="token-113-8" pos="word" morph="none" start_char="12084" end_char="12092">published</TOKEN>
<TOKEN id="token-113-9" pos="word" morph="none" start_char="12094" end_char="12101">acadmeic</TOKEN>
<TOKEN id="token-113-10" pos="word" morph="none" start_char="12103" end_char="12108">papers</TOKEN>
<TOKEN id="token-113-11" pos="punct" morph="none" start_char="12109" end_char="12109">,</TOKEN>
<TOKEN id="token-113-12" pos="word" morph="none" start_char="12111" end_char="12113">not</TOKEN>
<TOKEN id="token-113-13" pos="word" morph="none" start_char="12115" end_char="12116">so</TOKEN>
<TOKEN id="token-113-14" pos="word" morph="none" start_char="12118" end_char="12121">easy</TOKEN>
<TOKEN id="token-113-15" pos="punct" morph="none" start_char="12122" end_char="12122">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
